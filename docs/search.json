[
  {
    "objectID": "index.html#bachelor",
    "href": "index.html#bachelor",
    "title": "welcome",
    "section": "Bachelor",
    "text": "Bachelor\nnone"
  },
  {
    "objectID": "index.html#master",
    "href": "index.html#master",
    "title": "welcome",
    "section": "Master",
    "text": "Master\n\nDigital Innovation Management\n\nIntro to AI\nDI in Industry"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Me",
    "section": "",
    "text": "For more details please check my rofile at the HNU website"
  },
  {
    "objectID": "lectures/I2AI/0/index.html",
    "href": "lectures/I2AI/0/index.html",
    "title": "Administrivia üßê",
    "section": "",
    "text": "Top\nSlides\n\n\nGeneral remarks\nThis course will be taught using traditional synchronous lectures.\nThe focus of the live sessions is on the introduction of major concepts, the discussion of application scenarios and some exercises.\nIt is highly recommended to prepare for each session, details (see Table¬†1).\n\n\nContents\nAI is defined here as the study of agents that receive percepts from the environment and perform actions.\nIn this course, we will\n\n\ncover different types of agents (i.e., goal-based and utility-based agents)\nilluminate some concepts and major functions they implement,\ndiscuss, how to convert these agents into learning agents,\nidentify and discuss real life use cases for these agents, and\nhave a look at philosophical stances and ethical implications of AI.\n\n\n\n\nLearning objectives\nDuring this course, you should advance your skills in the following areas:\n\n\nUnderstanding of the origins, strengths and limitations of AI\nBasic knowledge of concepts, functions, and use cases of AI\nBasic knowledge of problem-solving algorithms, knowledge representation, probabilistic reasoning, machine learning, and natural language processing\nCapability to apply your knowledge and understanding of AI to different managerial and organizational contexts\nAbility to assess the potential of AI and use it in digital innovations\nAbility to evaluate new information and to question existing assumptions\nCapacity to assess social and ethical implications of AI applications\n\n\n\n\nSupporting literature\nThis course is essentially based on Russel and Norvig (2022)\n\n\n\n\nArtificial Intelligence: A Modern Approach, Global Edition, 4/E\nRussell / Norvig\nISBN-10: 1292401133 ‚Ä¢ ISBN-13: 9781292401133\nThe 3rd edition is available in the libary\n\n\n\n\nExam\nThere will we a written exam at the end of the semester.\nThe exam will\n\ntake place during the examination weeks,\nwill last 90 minutes,\ncover all contents discussed in lecture,\nfocus on the application of the knowledge gained in the course.\n\n\n\nSchedule\n\nIt is of importance that you reflect the contents of the session continually, do the exercises and ask questions.\n\n\n\n\nTable 1: Schedule summer term 2022 (may be subjected to changes)\n\n\n\n\n\n\n\nDate\nTopic\nPreparation\n\n\n\n\n03/21/2022\nAdministrivia, introduction & intelligent agents\n-\n\n\n04/04/2022\nProblem-solving by searching and games\nExercises\n\n\n04/11/2022\nKnowledge\nExercises\n\n\n05/02/2022\nUncertainty & probabilistic\nExercises\n\n\n05/16/2022\nMachine learning\nExercises\n\n\n05/30/2022\nGuest speech: reality of AI/ML in industry; Language & communication\nExercises\n\n\n06/27/2022\nPhilosophy, ethics & safety\nCase Study\n\n\n07/04/2022\nConclusion, wrap-up and Q&A\nQuestions\n\n\n\n\n\n\n\n\nReferences\n\nRussel, Stuart, and Peter Norvig. 2022. Artificial Intelligence: A Modern Approach. Harlow: Pearson Education."
  },
  {
    "objectID": "lectures/I2AI/1/index.html#sci-fi",
    "href": "lectures/I2AI/1/index.html#sci-fi",
    "title": "Introduction",
    "section": "Sci-Fi",
    "text": "Sci-Fi\n\nThe perception of AI changed over the decades. This is well reflected in Sci-Fi Movies, e.g.\n\nR2D2 in Star Wars\nThe Terminator\nAgent Smith in the Matrix\nSonny in I,Robot\nWall¬∑E\netc."
  },
  {
    "objectID": "lectures/I2AI/1/index.html#reality",
    "href": "lectures/I2AI/1/index.html#reality",
    "title": "Introduction",
    "section": "Reality",
    "text": "Reality\n\nWill the fictions become reality? We will se. In any case, the intellectual frontiers of AI are wide open. The subfields span from artificial general intelligence (learning, reasoning, perception, etc.) to specific fields (e.g., translating, playing go) (Russel and Norvig 2022).\n\n\n\n‚ÄúI believe it‚Äôs going to change the world more than anything in the history of mankind ‚Äî even more than electricity.‚Äù ‚ÄîKai-Fu Lee\n\n\n\n\n‚ÄúThe pace of progress in artificial intelligence is incredibly fast. Unless you have direct exposure to groups like Deepmind, you have no idea how fast‚Äîit is growing at a pace close to exponential. The risk of something seriously dangerous happening is in the five-year timeframe. 10 years at most.‚Äù ‚ÄîElon Musk\n\n\n\n\n‚ÄúForget artificial intelligence - in the brave new world of big data, it‚Äôs artificial idiocy we should be looking out for.‚Äù ‚ÄîTom Chatfield"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#towards-a-definition-of-ai",
    "href": "lectures/I2AI/1/index.html#towards-a-definition-of-ai",
    "title": "Introduction",
    "section": "Towards a definition of AI",
    "text": "Towards a definition of AI\nAI is the science of making machines to\n\n\nthink (though processes and reasoning)\n\nlike people\nrationally\n\nand to act (behavior)\n\nlike people\nrationally (Russel and Norvig 2022)\n\n\n\n\n\n\n\n\n\n\nWhat is rational?\n\n\n\nThe term rational is used here in a very specific, technical way:\n\nRational: maximally achieving pre-defined goals\nRationality only concerns what decisions are made (not the thought process behind them)\nGoals are expressed in terms of the utility of outcomes\nBeing rational means maximizing your expected utility"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#acting-humanly",
    "href": "lectures/I2AI/1/index.html#acting-humanly",
    "title": "Introduction",
    "section": "Acting humanly",
    "text": "Acting humanly\n\nThe Turing Test approach\nThe Turing Test (Turing 1950) tests if a computer has the ability to mimic peoples‚Äô behavior. To pass the test, it would need following capabilities:\n\n\nnatural language processing (communicate)\nknowledge representation (store information)\nautomated reasoning (answer questions, draw new conclusions)\nmachine learning (adapt to new circumstances)\n\n\n\nWant to do a Turing Test? Play ‚ÄúBot or Not‚Äù\n\n\n\n\n\n\n\n\nTuring Test\n\n\n\nThe Turing Test (Turing 1950) was designed as a tought experiment that would sidestep the philosophical vagueness of the question ‚ÄúCan a machine think?‚Äù A computer passes the test if a human interrogator, after posing some written questions, cannot tell wether the written responses come from a person or from a computer (Russel and Norvig 2022).\nThe Total Turing Test additionally requires interaction with objects and people in the real world. This also requires computer vision and robotic capabilities."
  },
  {
    "objectID": "lectures/I2AI/1/index.html#thinking-humanly",
    "href": "lectures/I2AI/1/index.html#thinking-humanly",
    "title": "Introduction",
    "section": "Thinking humanly",
    "text": "Thinking humanly\n\nThe cognitive modelling approach\nCognitive science is the study of the human brain and its processes ‚Äî it examines how the human brain may be functioning. Cognitive science requires analytical observation and experimentation. We can learn about human thought in three ways (Russel and Norvig 2022):\n\n\nintrospection (trying to catch our own thoughts)\nexperiments (observing a person in action)\nbrain imaging (observing the brain in action)\n\n\n\n\n\n\n\n\n\nDifferences between cognitive computing and AI\n\n\n\nCognitive science is about making computers solve complex problems similar to how humans solve problems. Cognitive computing tries to replicate how humans would solve problems, while AI is not bound to human cognitive processes."
  },
  {
    "objectID": "lectures/I2AI/1/index.html#thinking-rationally",
    "href": "lectures/I2AI/1/index.html#thinking-rationally",
    "title": "Introduction",
    "section": "Thinking rationally",
    "text": "Thinking rationally\n\nThe laws of thought approach\nThe ‚Äúlaws of thought‚Äù refer to fundamental axiomatic rules upon which rational discorse itself is often considered to be based.\n\n\nSocrates is a man and all men are mortal, thus, it can be concluded that Socrates is mortal ‚ÄîAristotle (384-322 BCE)\n\n\n\nIn principle, computers have been able to solve any solvable problem (i.e., make correct inferences), as long as\n\n\n\nthere are statements about any objects in the world,\nstatements about the relations among them, and\nthere is sufficient computing power available\n\n\n\n\n\n\n\n\n\nLogic, knowledge and probability\n\n\n\nThese laws of thought were supposed to govern the operation of the mind; the studies initiated the field called logic. However, human decisions are not always mathematically perfect or logical.\nLogic as conventionally understood requires knowledge of the world that is certain. As this condition is seldom achieved, the theory of probability fills this gap. Probability allows for rigorous reasoning with uncertain information. (Russel and Norvig 2022)"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#acting-rationally",
    "href": "lectures/I2AI/1/index.html#acting-rationally",
    "title": "Introduction",
    "section": "Acting rationally",
    "text": "Acting rationally\n\nThe rational agent approach\nAn agent is something that acts, an rational agent is one that acts so as to achieve the best coutcome (i.e., does the right thing), or, when there is uncertainty, the best expected outcome (i.e., does the appropriate thing) based on the objective that is provided to the agent (Russel and Norvig 2022).\n\nThe approach goes beyond the ‚Äúlaws of thought‚Äù approach as it involves actions based on\n\n\n\ninference (deducing that a given action is the best and then to act on this conclusion) and\nother mechanisms such as reflex (when speed is more successful than careful deliberation that takes some time)\n\n\n\n\n\n\n\n\n\nStandard model in AI\n\n\n\nRussel and Norvig (2022) call the approach where the primary definition of success is getting better and better at achieving rigid human-specified goals the standard model of AI research. This standard model prevails not only in AI, ‚Äúbut also in control theory, where a controller minimizes a cost funciton; in operations research, where a policy maximizes a sum of rewards; in statistics, where a decision rule minimizes a loss funciton; and in economics, where a decision maker maximizes utility or some measure of social welfare‚Äù (p.¬†22).\nThey also criticize the standard model because it is increasingly difficult to specify the goal completely and correctly (e.g., autonomous driving involves multiple goals such as reaching the goal safely, where a strict definition of safety requires staying in the garage because driving on the road has a risk of injury due to myriad factors; how should the trade-off between reaching the goal and taking a risk of injury be made? There are so many questions that are difficult to answer a priori). Mis-specified goals most likely do not reflect what human designers intend, e.g., by not taking into account human values that are not included in the goals."
  },
  {
    "objectID": "lectures/I2AI/1/index.html#the-thinking-machine",
    "href": "lectures/I2AI/1/index.html#the-thinking-machine",
    "title": "Introduction",
    "section": "The Thinking Machine",
    "text": "The Thinking Machine\nA series of interviews to some of the AI pioneers.\n\n\nThe full documentary is available here"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#a-brief-ai-timeline",
    "href": "lectures/I2AI/1/index.html#a-brief-ai-timeline",
    "title": "Introduction",
    "section": "A brief AI-timeline",
    "text": "A brief AI-timeline\n1943‚Äî1956 The inception of AI\n\n1943: McCulloch & Pitts: Boolean circuit model of brain (artificial neurons with on and off states; all logical connectives can be implemented with some network of these)\n1950: Turing‚Äôs ‚ÄúComputing Machinery and Intelligence‚Äù (Turing (1950) already introduced the Turing test, machine learning, genetic algorithms, and reinforcement learning)\n1950s: Early AI programs (e.g., Arthur Samuel‚Äôs influential checkers program that learned to play at a strong amateur level)"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#section",
    "href": "lectures/I2AI/1/index.html#section",
    "title": "Introduction",
    "section": "",
    "text": "1966‚Äî73 A dose of reality\n\nThe early AI programs failed on more difficult problems\n\nFocus on ‚Äúinformed introspection‚Äù as to how humans perform a task\nLack of appreciation of the intractability of many of the problems\n\nSignification reduction of government funding of AI research\n\n\n1970‚Äî90 Expert systems (knowledge-based approaches)\n\n1969‚Äî79: Early development of knowledge-based systems (rule-based heuristic algorithms)\n1980‚Äî88: Expert systems industry booms (nearly every major U.S. corporate had its own AI group)\nSoon after that came the ‚ÄúAI winter‚Äù (difficulties to build expert systems for complex domains due to uncertainty and a lack of learning)"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#section-1",
    "href": "lectures/I2AI/1/index.html#section-1",
    "title": "Introduction",
    "section": "",
    "text": "1990‚Äîpresent AI spring: (statistical approaches)\n\nFocus on probabilistic reasoning (rather than Boolean logic) and machine learning\nReunification of subfields such as computer vision, robotics, speech recognition, and natural language processing"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#section-2",
    "href": "lectures/I2AI/1/index.html#section-2",
    "title": "Introduction",
    "section": "",
    "text": "2012‚Äîpresent New excitement\n\nAdvances in computing power, WWW, and very large data sets\n(e.g., IBM Watson‚Äôs victory in Jeopardy!)\n\n\nImprovement in performance obtained from increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be obtained from tweaking the algorithm ‚ÄîBanko and Brill (2001)\n\n\nDeep learning systems offer significant performance gains\n(e.g., AlphaGo‚Äôs victories)\nSignificant focus on AI in academia and industry\nAI systems find incrasing application in the real world (e.g., robotic vehicles, machine translation, speech recognition, recommendations, autonomous planning, game playing, image understanding, medicine)"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#i2ai_1-e1",
    "href": "lectures/I2AI/1/index.html#i2ai_1-e1",
    "title": "Introduction",
    "section": "I2AI_1 E1",
    "text": "I2AI_1 E1\nDefine in your own words:\n\nintelligence\nartificial intelligence\nagent\nrationality\nlogical reasoning"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#i2ai_1-e2",
    "href": "lectures/I2AI/1/index.html#i2ai_1-e2",
    "title": "Introduction",
    "section": "I2AI_1 E2¬†",
    "text": "I2AI_1 E2¬†\nEvery year the Loebner Prize is awarded to the program that comes closest to passing a version of the Turing Test.\nResearch and report on the latest winner of the Loebner prize.\n\nWhat techniques does it use?\nHow does it advance the state of the art in AI?"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#i2ai_1-e3",
    "href": "lectures/I2AI/1/index.html#i2ai_1-e3",
    "title": "Introduction",
    "section": "I2AI_1 E3",
    "text": "I2AI_1 E3\nTo what extent are the following computer systems instances of artificial intelligence:\n\nSupermarket bar code scanners\nWeb search engines\nVoice-activated telephone menus\nInternet routing algorithms that respond dynamically to the state of the network"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#i2ai_1-e4",
    "href": "lectures/I2AI/1/index.html#i2ai_1-e4",
    "title": "Introduction",
    "section": "I2AI_1 E4",
    "text": "I2AI_1 E4\nVarious subfields of AI have held contests by defining a standard task and inviting researchers to do their best. Examples include the DARPA Grand Challenge for robotic cars, the International Planning Competition, the Robocup robotic soccer league, the TREC information retrieval event, and contests in machine translation and speech recognition.\nInvestigate one of these contests and describe the progress made over the years.\n\nTo what degree have the contests advanced the state of the art in AI?\nTo what degree do they hurt the field by drawing energy away from new ideas?"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#i2ai_1-e5",
    "href": "lectures/I2AI/1/index.html#i2ai_1-e5",
    "title": "Introduction",
    "section": "I2AI_1 E5",
    "text": "I2AI_1 E5\nRead the statements (one after the other) and discuss if the second sentence of each statement is true and if it does imply the first.\n\n‚ÄúSurely computers cannot be intelligent‚Äîthey can do only what their programmers tell them.‚Äù Is the latter statement true, and does it imply the former?\n\n\n‚ÄúSurely animals cannot be intelligent‚Äîthey can do only what their genes tell them.‚Äù Is the latter statement true, and does it imply the former?\n\n\n‚ÄúSurely animals, humans, and computers cannot be intelligent‚Äîthey can do only what their constituent atoms are told to do by the laws of physics.‚Äù Is the latter statement true, and does it imply the former?"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#sci-fi",
    "href": "lectures/I2AI/1/slides.html#sci-fi",
    "title": "Introduction",
    "section": "Sci-Fi",
    "text": "Sci-Fi\n\nThe perception of AI changed over the decades. This is well reflected in Sci-Fi Movies, e.g.\n\nR2D2 in Star Wars\nThe Terminator\nAgent Smith in the Matrix\nSonny in I,Robot\nWall¬∑E\netc."
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#reality",
    "href": "lectures/I2AI/1/slides.html#reality",
    "title": "Introduction",
    "section": "Reality",
    "text": "Reality\n\nWill the fictions become reality? We will se. In any case, the intellectual frontiers of AI are wide open. The subfields span from artificial general intelligence (learning, reasoning, perception, etc.) to specific fields (e.g., translating, playing go) (Russel and Norvig 2022).\n\n\n\n‚ÄúI believe it‚Äôs going to change the world more than anything in the history of mankind ‚Äî even more than electricity.‚Äù ‚ÄîKai-Fu Lee\n\n\n\n\n‚ÄúThe pace of progress in artificial intelligence is incredibly fast. Unless you have direct exposure to groups like Deepmind, you have no idea how fast‚Äîit is growing at a pace close to exponential. The risk of something seriously dangerous happening is in the five-year timeframe. 10 years at most.‚Äù ‚ÄîElon Musk\n\n\n\n\n‚ÄúForget artificial intelligence - in the brave new world of big data, it‚Äôs artificial idiocy we should be looking out for.‚Äù ‚ÄîTom Chatfield"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#towards-a-definition-of-ai",
    "href": "lectures/I2AI/1/slides.html#towards-a-definition-of-ai",
    "title": "Introduction",
    "section": "Towards a definition of AI",
    "text": "Towards a definition of AI\nAI is the science of making machines to\n\n\nthink (though processes and reasoning)\n\nlike people\nrationally\n\nand to act (behavior)\n\nlike people\nrationally (Russel and Norvig 2022)\n\n\n\n\n\n\n\n\n\n\nWhat is rational?\n\n\nThe term rational is used here in a very specific, technical way:\n\nRational: maximally achieving pre-defined goals\nRationality only concerns what decisions are made (not the thought process behind them)\nGoals are expressed in terms of the utility of outcomes\nBeing rational means maximizing your expected utility"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#acting-humanly",
    "href": "lectures/I2AI/1/slides.html#acting-humanly",
    "title": "Introduction",
    "section": "Acting humanly",
    "text": "Acting humanly\nThe Turing Test approach\nThe Turing Test (Turing 1950) tests if a computer has the ability to mimic peoples‚Äô behavior. To pass the test, it would need following capabilities:\n\n\nnatural language processing (communicate)\nknowledge representation (store information)\nautomated reasoning (answer questions, draw new conclusions)\nmachine learning (adapt to new circumstances)\n\n\n\nWant to do a Turing Test? Play ‚ÄúBot or Not‚Äù\n\n\n\n\n\n\n\n\nTuring Test\n\n\nThe Turing Test (Turing 1950) was designed as a tought experiment that would sidestep the philosophical vagueness of the question ‚ÄúCan a machine think?‚Äù A computer passes the test if a human interrogator, after posing some written questions, cannot tell wether the written responses come from a person or from a computer (Russel and Norvig 2022).\nThe Total Turing Test additionally requires interaction with objects and people in the real world. This also requires computer vision and robotic capabilities."
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#thinking-humanly",
    "href": "lectures/I2AI/1/slides.html#thinking-humanly",
    "title": "Introduction",
    "section": "Thinking humanly",
    "text": "Thinking humanly\nThe cognitive modelling approach\nCognitive science is the study of the human brain and its processes ‚Äî it examines how the human brain may be functioning. Cognitive science requires analytical observation and experimentation. We can learn about human thought in three ways (Russel and Norvig 2022):\n\n\nintrospection (trying to catch our own thoughts)\nexperiments (observing a person in action)\nbrain imaging (observing the brain in action)\n\n\n\n\n\n\n\n\n\nDifferences between cognitive computing and AI\n\n\nCognitive science is about making computers solve complex problems similar to how humans solve problems. Cognitive computing tries to replicate how humans would solve problems, while AI is not bound to human cognitive processes."
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#thinking-rationally",
    "href": "lectures/I2AI/1/slides.html#thinking-rationally",
    "title": "Introduction",
    "section": "Thinking rationally",
    "text": "Thinking rationally\nThe laws of thought approach\nThe ‚Äúlaws of thought‚Äù refer to fundamental axiomatic rules upon which rational discorse itself is often considered to be based.\n\n\nSocrates is a man and all men are mortal, thus, it can be concluded that Socrates is mortal ‚ÄîAristotle (384-322 BCE)\n\n\n\nIn principle, computers have been able to solve any solvable problem (i.e., make correct inferences), as long as\n\n\n\nthere are statements about any objects in the world,\nstatements about the relations among them, and\nthere is sufficient computing power available\n\n\n\n\n\n\n\n\n\nLogic, knowledge and probability\n\n\nThese laws of thought were supposed to govern the operation of the mind; the studies initiated the field called logic. However, human decisions are not always mathematically perfect or logical.\nLogic as conventionally understood requires knowledge of the world that is certain. As this condition is seldom achieved, the theory of probability fills this gap. Probability allows for rigorous reasoning with uncertain information. (Russel and Norvig 2022)"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#acting-rationally",
    "href": "lectures/I2AI/1/slides.html#acting-rationally",
    "title": "Introduction",
    "section": "Acting rationally",
    "text": "Acting rationally\nThe rational agent approach\nAn agent is something that acts, an rational agent is one that acts so as to achieve the best coutcome (i.e., does the right thing), or, when there is uncertainty, the best expected outcome (i.e., does the appropriate thing) based on the objective that is provided to the agent (Russel and Norvig 2022).\n\nThe approach goes beyond the ‚Äúlaws of thought‚Äù approach as it involves actions based on\n\n\n\ninference (deducing that a given action is the best and then to act on this conclusion) and\nother mechanisms such as reflex (when speed is more successful than careful deliberation that takes some time)\n\n\n\n\n\n\n\n\n\nStandard model in AI\n\n\nRussel and Norvig (2022) call the approach where the primary definition of success is getting better and better at achieving rigid human-specified goals the standard model of AI research. This standard model prevails not only in AI, ‚Äúbut also in control theory, where a controller minimizes a cost funciton; in operations research, where a policy maximizes a sum of rewards; in statistics, where a decision rule minimizes a loss funciton; and in economics, where a decision maker maximizes utility or some measure of social welfare‚Äù (p.¬†22).\nThey also criticize the standard model because it is increasingly difficult to specify the goal completely and correctly (e.g., autonomous driving involves multiple goals such as reaching the goal safely, where a strict definition of safety requires staying in the garage because driving on the road has a risk of injury due to myriad factors; how should the trade-off between reaching the goal and taking a risk of injury be made? There are so many questions that are difficult to answer a priori). Mis-specified goals most likely do not reflect what human designers intend, e.g., by not taking into account human values that are not included in the goals."
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#the-thinking-machine",
    "href": "lectures/I2AI/1/slides.html#the-thinking-machine",
    "title": "Introduction",
    "section": "The Thinking Machine",
    "text": "The Thinking Machine\nA series of interviews to some of the AI pioneers.\n\n\nThe full documentary is available here"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#a-brief-ai-timeline",
    "href": "lectures/I2AI/1/slides.html#a-brief-ai-timeline",
    "title": "Introduction",
    "section": "A brief AI-timeline",
    "text": "A brief AI-timeline\n1943‚Äî1956 The inception of AI\n\n1943: McCulloch & Pitts: Boolean circuit model of brain (artificial neurons with on and off states; all logical connectives can be implemented with some network of these)\n1950: Turing‚Äôs ‚ÄúComputing Machinery and Intelligence‚Äù (Turing (1950) already introduced the Turing test, machine learning, genetic algorithms, and reinforcement learning)\n1950s: Early AI programs (e.g., Arthur Samuel‚Äôs influential checkers program that learned to play at a strong amateur level)"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#section",
    "href": "lectures/I2AI/1/slides.html#section",
    "title": "Introduction",
    "section": "",
    "text": "1966‚Äî73 A dose of reality\n\nThe early AI programs failed on more difficult problems\n\nFocus on ‚Äúinformed introspection‚Äù as to how humans perform a task\nLack of appreciation of the intractability of many of the problems\n\nSignification reduction of government funding of AI research\n\n\n1970‚Äî90 Expert systems (knowledge-based approaches)\n\n1969‚Äî79: Early development of knowledge-based systems (rule-based heuristic algorithms)\n1980‚Äî88: Expert systems industry booms (nearly every major U.S. corporate had its own AI group)\nSoon after that came the ‚ÄúAI winter‚Äù (difficulties to build expert systems for complex domains due to uncertainty and a lack of learning)"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#section-1",
    "href": "lectures/I2AI/1/slides.html#section-1",
    "title": "Introduction",
    "section": "",
    "text": "1990‚Äîpresent AI spring: (statistical approaches)\n\nFocus on probabilistic reasoning (rather than Boolean logic) and machine learning\nReunification of subfields such as computer vision, robotics, speech recognition, and natural language processing"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#section-2",
    "href": "lectures/I2AI/1/slides.html#section-2",
    "title": "Introduction",
    "section": "",
    "text": "2012‚Äîpresent New excitement\n\nAdvances in computing power, WWW, and very large data sets\n(e.g., IBM Watson‚Äôs victory in Jeopardy!)\n\n\nImprovement in performance obtained from increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be obtained from tweaking the algorithm ‚ÄîBanko and Brill (2001)\n\n\nDeep learning systems offer significant performance gains\n(e.g., AlphaGo‚Äôs victories)\nSignificant focus on AI in academia and industry\nAI systems find incrasing application in the real world (e.g., robotic vehicles, machine translation, speech recognition, recommendations, autonomous planning, game playing, image understanding, medicine)"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#i2ai_1-e1",
    "href": "lectures/I2AI/1/slides.html#i2ai_1-e1",
    "title": "Introduction",
    "section": "I2AI_1 E1",
    "text": "I2AI_1 E1\nDefine in your own words:\n\nintelligence\nartificial intelligence\nagent\nrationality\nlogical reasoning"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#i2ai_1-e2",
    "href": "lectures/I2AI/1/slides.html#i2ai_1-e2",
    "title": "Introduction",
    "section": "I2AI_1 E2¬†",
    "text": "I2AI_1 E2¬†\nEvery year the Loebner Prize is awarded to the program that comes closest to passing a version of the Turing Test.\nResearch and report on the latest winner of the Loebner prize.\n\nWhat techniques does it use?\nHow does it advance the state of the art in AI?"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#i2ai_1-e3",
    "href": "lectures/I2AI/1/slides.html#i2ai_1-e3",
    "title": "Introduction",
    "section": "I2AI_1 E3",
    "text": "I2AI_1 E3\nTo what extent are the following computer systems instances of artificial intelligence:\n\nSupermarket bar code scanners\nWeb search engines\nVoice-activated telephone menus\nInternet routing algorithms that respond dynamically to the state of the network"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#i2ai_1-e4",
    "href": "lectures/I2AI/1/slides.html#i2ai_1-e4",
    "title": "Introduction",
    "section": "I2AI_1 E4",
    "text": "I2AI_1 E4\nVarious subfields of AI have held contests by defining a standard task and inviting researchers to do their best. Examples include the DARPA Grand Challenge for robotic cars, the International Planning Competition, the Robocup robotic soccer league, the TREC information retrieval event, and contests in machine translation and speech recognition.\nInvestigate one of these contests and describe the progress made over the years.\n\nTo what degree have the contests advanced the state of the art in AI?\nTo what degree do they hurt the field by drawing energy away from new ideas?"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#i2ai_1-e5",
    "href": "lectures/I2AI/1/slides.html#i2ai_1-e5",
    "title": "Introduction",
    "section": "I2AI_1 E5",
    "text": "I2AI_1 E5\nRead the statements (one after the other) and discuss if the second sentence of each statement is true and if it does imply the first.\n\n‚ÄúSurely computers cannot be intelligent‚Äîthey can do only what their programmers tell them.‚Äù Is the latter statement true, and does it imply the former?\n\n\n‚ÄúSurely animals cannot be intelligent‚Äîthey can do only what their genes tell them.‚Äù Is the latter statement true, and does it imply the former?\n\n\n‚ÄúSurely animals, humans, and computers cannot be intelligent‚Äîthey can do only what their constituent atoms are told to do by the laws of physics.‚Äù Is the latter statement true, and does it imply the former?"
  },
  {
    "objectID": "lectures/I2AI/3/index.html#agent",
    "href": "lectures/I2AI/3/index.html#agent",
    "title": "Problem solving (searching)",
    "section": "Agent",
    "text": "Agent\nAgents that plan ahead by considering a sequence of actions that form a path to a goal state are called problem-solving agents (Russel and Norvig 2022, 81)\n\n\nThe computational process it undertakes is search\nThe representations the agents use are atomic representations\nThere are search algorithms for several environments.\n\n\n\nHere only simple environments are considered (episodic, single agent, fully observable, deterministic, static, discrete, and known).\n‚Äì> We assume that information about the environment are given (e.g., a map).\n\n\nThere are also search algorithms for problems in partially observable, nondeterministic, unknown, and continuous environments (i.e., complex environments) like local search methods (e.g., hill-climbing search, local beam search, evolutionary algorithms). For details please see Russel and Norvig (2022)."
  },
  {
    "objectID": "lectures/I2AI/3/index.html#problem-solving-process",
    "href": "lectures/I2AI/3/index.html#problem-solving-process",
    "title": "Problem solving (searching)",
    "section": "Problem-solving process",
    "text": "Problem-solving process\nIn simple environments, agents can follow a four-phase-problem-solving process (Russel and Norvig 2022, 81‚Äì82):\n\n\nGoal formulation: goals organize behavior by limiting the objectives and hence the actions to be considered\nProblem formulation: the agents devices a description of the states and actions necessary to reach the goal‚Äîan abstract model of the relevant part of the environment\nSearch: the agent simulates sequences of actions in its model, searching until it finds a sequence that reaches the goal (i.e., the solution)\nExecution: the agent executes the actions in the solution, one at a time"
  },
  {
    "objectID": "lectures/I2AI/3/index.html#search-problem",
    "href": "lectures/I2AI/3/index.html#search-problem",
    "title": "Problem solving (searching)",
    "section": "Search problem",
    "text": "Search problem\nA search problem can be defined formally as (Russel and Norvig 2022, 83):\n\n\nThe state space: a set of possible states the environment can be in\nThe initial state: the state that the agent starts\nGoal states: a singe goal state, a small set of alternative goal states, or a property that applies to many states (e.g, no dirt in any location)\nThe actions available to the agent ACTIONS(s) where s is the current state\nA transition model: describes what each action does. RESULT(s,a)returens the state that results from doing action ain state s.\nAn action cost function: gives the numeric cost of applying action a in state s to reach state s' (ACTION-COST(s,a,s')\nA path: a sequence of actions\nA solution: a path from the initial state to the goal state"
  },
  {
    "objectID": "lectures/I2AI/3/index.html#search-problems-are-models",
    "href": "lectures/I2AI/3/index.html#search-problems-are-models",
    "title": "Problem solving (searching)",
    "section": "Search problems are models",
    "text": "Search problems are models\n\n\n\nFigure 1: Example: A simplified map of Romania, with road distances in miles; based on Russel and Norvig (2022, 82)\n\n\n\nFigure¬†1 depicts the search problem as model, the state space graph (Russel and Norvig 2022, 82‚Äì84):\n\nState space: cities (vertices, each state occurs only once)\nInitial state: Arad\nGoal state: Bucharest (goal test: Is state == Bucharest?)\nActions: directed edges between the vertices (paths)\nAction costs: numbers on the paths\n\n\n\n\n\n\n\nDefinition of search problems and abstraction\n\n\n\nThe model is an abstract mathematical description, here a simple atomic state description. The model is an abstraction as it ignores many details of the reality (e.g., weather and scenery).\nA good problem formulation has the right level of detail (i.e., an appropriate level of abstraction).\nThe choice of a good abstraction involves removing as much detail as possible while retaining validity and ensuring that the abstract actions are easy to carry out. An abstraction is valid if any abstract solution can be elaborated into a solution in the more detailed world.\n\n\n\nReal world problems\nExamples for search problems are (Russel and Norvig 2022, 87‚Äì88):\n\nRoute-finding problems (e.g., car navigation, airline travel-planning)\nTouring problems (e.g., the traveling salesperson problem)\nVLSI layout problems (positioning millions of components and connections on a chip)\nRobot navigation (e.g., vacuum robots)\nAutomatic assembly sequencing of complex objects (e.g., protein design)"
  },
  {
    "objectID": "lectures/I2AI/3/index.html#real-world-problems",
    "href": "lectures/I2AI/3/index.html#real-world-problems",
    "title": "Problem solving (searching)",
    "section": "Real world problems",
    "text": "Real world problems\nExamples for search problems are (Russel and Norvig 2022, 87‚Äì88):\n\nRoute-finding problems (e.g., car navigation, airline travel-planning)\nTouring problems (e.g., the traveling salesperson problem)\nVLSI layout problems (positioning millions of components and connections on a chip)\nRobot navigation (e.g., vacuum robots)\nAutomatic assembly sequencing of complex objects (e.g., protein design)"
  },
  {
    "objectID": "lectures/I2AI/3/index.html#search-trees",
    "href": "lectures/I2AI/3/index.html#search-trees",
    "title": "Problem solving (searching)",
    "section": "Search trees",
    "text": "Search trees\n\n\n\nFigure 2: Example: A partial search tree for finding a route from Arad to Bucharest; based on Russel and Norvig (2022, 89)\n\n\n\nIn Figure¬†2, nodes that have been expanded are white with bold letters; nodes on the frontier that have been generated but not yet expanded are in white and regular letter; the set of states corresponding to these two types of nodes are said to have been reached. Nodes that could be generated next are shown in faint dashed lines.\n\n\n\n\n\n\nDefinition of search trees\n\n\n\nA search tree is a ‚Äúwhat if‚Äù tree of plans and their outcomes.\n\nThe start state is the root node,\nchildren correspond to successors,\nnodes show states, but correspond to PLANS that achieve those states\n\nThere are lots of repeated structure in the search tree. Thus, for most problems, the whole tree can never be actually built. In practice, both state space graphs and search trees are constructed on demand and as little as possible."
  },
  {
    "objectID": "lectures/I2AI/3/index.html#uniformed-search",
    "href": "lectures/I2AI/3/index.html#uniformed-search",
    "title": "Problem solving (searching)",
    "section": "Uniformed search",
    "text": "Uniformed search"
  },
  {
    "objectID": "lectures/I2AI/3/index.html#informed-search",
    "href": "lectures/I2AI/3/index.html#informed-search",
    "title": "Problem solving (searching)",
    "section": "Informed search",
    "text": "Informed search"
  },
  {
    "objectID": "lectures/I2AI/3/index.html#i2ai_3-e1",
    "href": "lectures/I2AI/3/index.html#i2ai_3-e1",
    "title": "Problem solving (searching)",
    "section": "I2AI_3 E1",
    "text": "I2AI_3 E1\nDefine in your own words the following terms:\n\n‚Ä¶"
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#agent",
    "href": "lectures/I2AI/3/slides.html#agent",
    "title": "Problem solving (searching)",
    "section": "Agent",
    "text": "Agent\nAgents that plan ahead by considering a sequence of actions that form a path to a goal state are called problem-solving agents (Russel and Norvig 2022, 81)\n\n\nThe computational process it undertakes is search\nThe representations the agents use are atomic representations\nThere are search algorithms for several environments.\n\n\n\nHere only simple environments are considered (episodic, single agent, fully observable, deterministic, static, discrete, and known).\n‚Äì> We assume that information about the environment are given (e.g., a map).\n\n\nThere are also search algorithms for problems in partially observable, nondeterministic, unknown, and continuous environments (i.e., complex environments) like local search methods (e.g., hill-climbing search, local beam search, evolutionary algorithms). For details please see Russel and Norvig (2022)."
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#problem-solving-process",
    "href": "lectures/I2AI/3/slides.html#problem-solving-process",
    "title": "Problem solving (searching)",
    "section": "Problem-solving process",
    "text": "Problem-solving process\nIn simple environments, agents can follow a four-phase-problem-solving process (Russel and Norvig 2022, 81‚Äì82):\n\n\nGoal formulation: goals organize behavior by limiting the objectives and hence the actions to be considered\nProblem formulation: the agents devices a description of the states and actions necessary to reach the goal‚Äîan abstract model of the relevant part of the environment\nSearch: the agent simulates sequences of actions in its model, searching until it finds a sequence that reaches the goal (i.e., the solution)\nExecution: the agent executes the actions in the solution, one at a time"
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#search-problem",
    "href": "lectures/I2AI/3/slides.html#search-problem",
    "title": "Problem solving (searching)",
    "section": "Search problem",
    "text": "Search problem\nA search problem can be defined formally as (Russel and Norvig 2022, 83):\n\n\nThe state space: a set of possible states the environment can be in\nThe initial state: the state that the agent starts\nGoal states: a singe goal state, a small set of alternative goal states, or a property that applies to many states (e.g, no dirt in any location)\nThe actions available to the agent ACTIONS(s) where s is the current state\nA transition model: describes what each action does. RESULT(s,a)returens the state that results from doing action ain state s.\nAn action cost function: gives the numeric cost of applying action a in state s to reach state s' (ACTION-COST(s,a,s')\nA path: a sequence of actions\nA solution: a path from the initial state to the goal state"
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#search-problems-are-models",
    "href": "lectures/I2AI/3/slides.html#search-problems-are-models",
    "title": "Problem solving (searching)",
    "section": "Search problems are models",
    "text": "Search problems are models\n\n\n\nFigure 1: Example: A simplified map of Romania, with road distances in miles; based on Russel and Norvig (2022, 82)\n\n\n\nFigure¬†1 depicts the search problem as model, the state space graph (Russel and Norvig 2022, 82‚Äì84):\n\nState space: cities (vertices, each state occurs only once)\nInitial state: Arad\nGoal state: Bucharest (goal test: Is state == Bucharest?)\nActions: directed edges between the vertices (paths)\nAction costs: numbers on the paths\n\n\n\n\n\n\n\nDefinition of search problems and abstraction\n\n\nThe model is an abstract mathematical description, here a simple atomic state description. The model is an abstraction as it ignores many details of the reality (e.g., weather and scenery).\nA good problem formulation has the right level of detail (i.e., an appropriate level of abstraction).\nThe choice of a good abstraction involves removing as much detail as possible while retaining validity and ensuring that the abstract actions are easy to carry out. An abstraction is valid if any abstract solution can be elaborated into a solution in the more detailed world.\n\n\n\n\nReal world problems\nExamples for search problems are (Russel and Norvig 2022, 87‚Äì88):\n\nRoute-finding problems (e.g., car navigation, airline travel-planning)\nTouring problems (e.g., the traveling salesperson problem)\nVLSI layout problems (positioning millions of components and connections on a chip)\nRobot navigation (e.g., vacuum robots)\nAutomatic assembly sequencing of complex objects (e.g., protein design)"
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#real-world-problems",
    "href": "lectures/I2AI/3/slides.html#real-world-problems",
    "title": "Problem solving (searching)",
    "section": "Real world problems",
    "text": "Real world problems\nExamples for search problems are (Russel and Norvig 2022, 87‚Äì88):\n\nRoute-finding problems (e.g., car navigation, airline travel-planning)\nTouring problems (e.g., the traveling salesperson problem)\nVLSI layout problems (positioning millions of components and connections on a chip)\nRobot navigation (e.g., vacuum robots)\nAutomatic assembly sequencing of complex objects (e.g., protein design)"
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#search-trees",
    "href": "lectures/I2AI/3/slides.html#search-trees",
    "title": "Problem solving (searching)",
    "section": "Search trees",
    "text": "Search trees\n\n\n\nFigure 2: Example: A partial search tree for finding a route from Arad to Bucharest; based on Russel and Norvig (2022, 89)\n\n\n\nIn Figure¬†2, nodes that have been expanded are white with bold letters; nodes on the frontier that have been generated but not yet expanded are in white and regular letter; the set of states corresponding to these two types of nodes are said to have been reached. Nodes that could be generated next are shown in faint dashed lines.\n\n\n\n\n\n\nDefinition of search trees\n\n\nA search tree is a ‚Äúwhat if‚Äù tree of plans and their outcomes.\n\nThe start state is the root node,\nchildren correspond to successors,\nnodes show states, but correspond to PLANS that achieve those states\n\nThere are lots of repeated structure in the search tree. Thus, for most problems, the whole tree can never be actually built. In practice, both state space graphs and search trees are constructed on demand and as little as possible."
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#uniformed-search",
    "href": "lectures/I2AI/3/slides.html#uniformed-search",
    "title": "Problem solving (searching)",
    "section": "Uniformed search",
    "text": "Uniformed search"
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#informed-search",
    "href": "lectures/I2AI/3/slides.html#informed-search",
    "title": "Problem solving (searching)",
    "section": "Informed search",
    "text": "Informed search"
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#i2ai_3-e1",
    "href": "lectures/I2AI/3/slides.html#i2ai_3-e1",
    "title": "Problem solving (searching)",
    "section": "I2AI_3 E1",
    "text": "I2AI_3 E1\nDefine in your own words the following terms:\n\n‚Ä¶"
  },
  {
    "objectID": "lectures/I2AI/2/index.html#agent",
    "href": "lectures/I2AI/2/index.html#agent",
    "title": "Intelligent agents",
    "section": "Agent",
    "text": "Agent\n\n\n\nFigure 1: Agents interact with environments through sensors and actuators\n\n\n\nAn agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\n\nThe term percept refers to the content an agent‚Äôs sensors are perceiving\nAn agent‚Äôs percept sequence is the complete history of everything the agent has ever perceived\nThe agent function maps any given percept sequence to an action (an abstract mathematical description)\nThe agent function for an AI agent will be implemented by an agent program (a concrete implementation, running within some physical system)\n\n\n\n\n\n\n\nExample\n\n\n\nTo illustrate these ideas Russel and Norvig (2022, 55) use a simple example‚Äîthe vacuum-cleaner world, which consists of a robotic vacuum-cleaning agent in a world consisting of squares that can be either dirty or clean. The vacuum agent perceives which square it is in and whether there is dirt in the square. The agent starts in square A. The available actions are to move to the right, move to the left, suck up the dirt, or do nothing. One very simple agent function is the following: if the current square is dirty, then suck; otherwise, move to the other square.\n\n\nAccording to Russel and Norvig (2022, 56), all areas of engineering can be seen as designing artifacts that interact with the world. AI operates at the most interesting end to the spectrum, where the artifacts have significant computational resources and the task environment requires nontrivial decision making."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#rational-agent",
    "href": "lectures/I2AI/2/index.html#rational-agent",
    "title": "Intelligent agents",
    "section": "Rational agent",
    "text": "Rational agent\nA rational agent is one that does the right thing.\n\n\nFor each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has (Russel and Norvig 2022, 58).\n\n\n\nWhat is rational at any given time depends on four things:\n\nThe performance measure that defines the criterion of success\nThe agent‚Äôs prior knowledge of the environment\nThe actions that the agent can performance\nThe agent‚Äôs percept sequence to date\n\n\n\n\n\n\n\nExample\n\n\n\nUnder following circumstances, the vacuum cleaning agent is rational:\n\nThe performance measure of the vacuum cleaner might award one point for each clean square at each time step, over a ‚Äúlifetime‚Äù of 1,000 time steps (to prevent the cleaner to oscillate needlessly back and forth)\nThe ‚Äúgeography‚Äù of the environment is known a priori but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking cleans the current square. The Right and Left actions move the agent one square except when this would take the agent outside the environment in which case the agent remains where it is\nThe only available action is Right, Left, and Suck\nThe agent correctly perceives its location and wether that location contains dirt\n\n(For details such as tabulated agent functions please see Russel and Norvig (2022))\n\n\n\n\nIt can be quite hard to formulate a performance measure correctly, however:\n\nIf we use, to achieve our purposes, a mechanical agency with those operation we cannot interfere once we have started it [‚Ä¶] we had better be quite sure that the purpose built into the machine is the purpose which we really desire (Wiener 1960, 1358)"
  },
  {
    "objectID": "lectures/I2AI/2/index.html#rationality",
    "href": "lectures/I2AI/2/index.html#rationality",
    "title": "Intelligent agents",
    "section": "Rationality",
    "text": "Rationality\nRationality is not the same as perfection.\n\n\nRationality maximizes expected performance\nPerfection maximizes actual performance\nPerfection requires omniscience\nRational choice depends only on the percept sequence to date\n\n\n\nAs the environment is usually not completely known a priori and completely predictable (or stable), information gathering and learning are important parts of rationality (Russel and Norvig 2022, 59).\n\n\n\n\n\n\nExample\n\n\n\nThe vacuum cleaner needs to explore an initially unknown environment (i.e., exploration) to maximize its expected performance. In addition, a vacuum cleaner that learns to predict where and when additional dirt will appear will do better than one that does not."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#components",
    "href": "lectures/I2AI/2/index.html#components",
    "title": "Intelligent agents",
    "section": "Components",
    "text": "Components\nBefore designing an agent (the solution), the task environment (the problem) must be specified as fully as possible, including\n\n\nthe performance measure (P),\nthe environment (E),\nthe actuators (A), and\nthe sensors (S)\n\n\n\nRussel and Norvig (2022) call the task environment PEAS.\n\n\n\n\n\n\n\n\nExample of an PEAS description\n\n\n\nTask environment of a taxi driver agent\n\nP: Safe, fast, legal, comfortable, maximize profits, minimize impact on other road users\nE: Roads, other road users, police, pedestrians, customers, weather\nA: Steering, accelerator, brake, signal horn, display, speech\nS: Cameras, radar, speedometer, GPS, engine, sensors, accelerometer, microphones, touchscreen\n\nSource: Russel and Norvig (2022, 61)"
  },
  {
    "objectID": "lectures/I2AI/2/index.html#properties",
    "href": "lectures/I2AI/2/index.html#properties",
    "title": "Intelligent agents",
    "section": "Properties",
    "text": "Properties\nTask environments can be categorized along following dimensions (Russel and Norvig 2022, 62‚Äì64):\n\n\nFully observable vs. partially observable\nSingle agent vs. multi-agent\nDeterministic vs. nondeterministic\nEpisodic vs. sequential\nStatic vs. dynamic\nDiscrete vs. continuous\nKnown vs. unknown\n\n\n\nIf an agent‚Äôs sensors give it access to the full state of the environment at any point in time, then we say that the task environment is fully observable (e.g., image analysis).\nWhen multiple agents intend to maximize a performance measure that depends on the behavior of other agents, we say the environment is multi-agent (e.g., chess).\nWhen the environment is completely determined by the current state and the actions performed by the agent(s), it is called a deterministic environment (e.g., crossword puzzle). When a model of the environment explicitly uses probabilities, it is called a stochastic environment (e.g., poker).\nIf an agent‚Äôs experience is divided into atomic episodes in which the agent receives a perception and then performs a single action, and if the next episode does not depend on the actions performed in the previous episodes, then we say that the task environment is episodic (e.g., image analysis).\nIf the environment changes while an agent is deliberating, then the environment is dynamic (e.g., taxi driving).\nIf the environment has a finite number of different states, we speak of discrete environments (e.g., chess).\nIf the outcomes (or outcome probabilities) for all actions are given, then the environment is known (e.g., solitaire card game).\nSource: Russel and Norvig (2022), p.62-64\n\n\nThe hardest case is partially observable, multi-agent, nondeterministic, sequential, dynamic, and continuous."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#simple-reflex-agents",
    "href": "lectures/I2AI/2/index.html#simple-reflex-agents",
    "title": "Intelligent agents",
    "section": "Simple reflex agents",
    "text": "Simple reflex agents\n\n\n\nFigure 2: A simple reflex agent\n\n\n\n\nRectangles are used to denote the current internal state of the agent‚Äôs decision process, rectangles with rounded corners to represent the background information used in the process.\n\nSimple reflex agents select actions on the basis of the current percept, ignoring the rest of the percept history. Thus, these agents work only if the correct decision can be made on the basis of just the current percept. The environment needs to be fully observable (Russel and Norvig 2022, 68)."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#model-based-reflex-agents",
    "href": "lectures/I2AI/2/index.html#model-based-reflex-agents",
    "title": "Intelligent agents",
    "section": "Model-based reflex agents",
    "text": "Model-based reflex agents\n\n\n\nFigure 3: A model-based reflex agent\n\n\n\nModel-based reflex agents use transition models and sensor models to keep track of the state of the world as perceived by the sensors (i.e., internal state). (Russel and Norvig 2022, 70).\nThe transition model reflects ‚Äúhow the world works,‚Äù i.e., how the world evolves (a) independently of the agent and (b) depending on the agent‚Äôs actions.\nThe sensor model reflects how the state of the world is reflected in the agent‚Äôs percepts (i.e., by its sensors).\n\n\n\n\n\n\nTypes of representation of states and the transitions between them\n\n\n\nThe representations of states can be placed along an axis of increasing complexity and expressive power (Russel and Norvig 2022, 76‚Äì77):\n\nAtomic representation: a state is a blackbox with no internal structure (A ‚Äì> B)\nFactored representation: a state consists of a vector of attribute values; values can be Boolean, real-valued, or one of a fixed set of symbols; factored states can share some attributes and not others, which makes it easier to identify transitions between states\nStructured representation: a state includes objects, each of which may have attributes of its own as well as relationships to other objects; structured representations underlie relational databases and first-order logic, first-oder probability models, and much of natural language understanding.\n\nThe more expressive language is much more concise, but makes reasoning and learning more complex."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#goal-based-agents",
    "href": "lectures/I2AI/2/index.html#goal-based-agents",
    "title": "Intelligent agents",
    "section": "Goal-based agents",
    "text": "Goal-based agents\n\n\n\nFigure 4: A model-based, goal-based agent\n\n\n\nA model-based, goal-based agent keeps track of the world state as well as a set of goals it is trying to achieve. Such an agent chooses an action that will (eventually) lead to the achievement of its goals (Russel and Norvig 2022, 72)."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#utility-based-agents",
    "href": "lectures/I2AI/2/index.html#utility-based-agents",
    "title": "Intelligent agents",
    "section": "Utility-based agents",
    "text": "Utility-based agents\n\n\n\nFigure 5: A model-based, utility-based agent\n\n\n\nA model-based, utility-based agent uses a model of the world, along with a utility function that measures its preferences among states of the world. It chooses the action that leads to the best expected utility, where expected utility is computed by averaging over all possible states, weighted by the probability of the outcome (Russel and Norvig 2022, 73).\nThe utility function is essentially an internalization of the performance measure.\nA utility-based agent has many advantages in terms of flexibility and learning, which are particularly helpful in environments characterized by partial observability and nondeterminism.\nIn addition, there are cases where the goals are insufficient but a utility-based agent can still make rational decisions based on the probabilities and the utilities of the outcomes:\n\nWhen there are conflicting goals, the utility function specifies the appropriate tradeoff.\nLikelihood of success (i.e., goal achievement) can be weighed against the importance of the goals\n\nMode- and utility-based agents are difficult to implement. They need to model and keep track of the task environment, which requires ingenious sensors, sophisticated algorithms, and a high computational complexity.\nThere are also utility-based agents that are not model-based. These agents just learn what action is best in a particular situation without any ‚Äúunderstanding‚Äù of its impact on the environment (e.g., based on reinforcement learning)."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#learning-agents",
    "href": "lectures/I2AI/2/index.html#learning-agents",
    "title": "Intelligent agents",
    "section": "Learning agents",
    "text": "Learning agents\n\n\n\nFigure 6: A learning agent\n\n\n\nEach type of agent can be either hand-programmed or created as a learning agent. The behavior of learning agents is (also) determined by their own experience, while the behavior of hand-programmed agents is solely determined by their initial programming. Thus, learning agents have greater autonony.\nA learning agent consists of four conceptual components (Russel and Norvig 2022, p- 74-75), as shown in Figure¬†6:\n\nThe performance element is responsible for taking percepts and selecting actions (i.e., what has previously been considered the entire agent program).\nThe learning element is responsible for improvements. It uses feedback from the critic on how the agent is doing and determines how the performance element should be modified to do better in the future. It can make changes to any of the ‚Äúknowledge components‚Äù shown in the agent diagrams (i.e., condition-action-rules, transition model, sensor model)\nThe performance standard is responsible to inform the agent about the meaning of percepts ‚Äî are they good nor not (e.g., the meaning of receiving no tips from passengers is a negative contribution to an automated taxis‚Äôs overall performance). The standard is fixed and cannot be influenced by the agent.\nThe problem generator is responsible for suggesting actions that lead to new and informative experiences. The problem generator suggests exploratory actions that may be suboptimal in the short term, but can lead to the discovery of better actions in the long term."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e1",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e1",
    "title": "Intelligent agents",
    "section": "I2AI_2 E1",
    "text": "I2AI_2 E1\nDefine in your own words the following terms:\n\nAgent\nEnvironment\nSensor\nActuator\nPercept\nAgent function\nAgent program"
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e2",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e2",
    "title": "Intelligent agents",
    "section": "I2AI_2 E2",
    "text": "I2AI_2 E2\nFor each of the following agents, specify the sensors, actuators, and environment:\n\nMicrowave oven\nChess program\nAutonomous supply delivery"
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e3",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e3",
    "title": "Intelligent agents",
    "section": "I2AI_2 E3",
    "text": "I2AI_2 E3\nDescribe a task environments in which the performance measure is easy to specify completely and correctly, and a in which it is not."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e4",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e4",
    "title": "Intelligent agents",
    "section": "I2AI_2 E4",
    "text": "I2AI_2 E4\nFor each of the following assertions, say whether it is true or false and support your answer with examples or counterexamples where appropriate.\n\nAn agent that senses only partial information about the state cannot be perfectly rational.\nThere exist task environments in which no pure reflex agent can behave rationally.\nThere exists a task environment in which every agent is rational.\nThe input to an agent program is the same as the input to the agent function.\nEvery agent is rational in an unobservable environment.\nThere is a model-based reflex agent that can remember all of its percepts.\nSuppose agent A1 is rational and agent A2 is irrational. There exists a task environment where A2‚Äôs actual score will be greater than A1‚Äôs actual score."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e5",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e5",
    "title": "Intelligent agents",
    "section": "I2AI_2 E5",
    "text": "I2AI_2 E5\nFor each of the following activities, give a PEAS description of the task environment and characterize it in terms of the properties discussed in class.\n\nPlaying soccer.\nExploring the subsurface oceans of Titan.\nShopping for used AI books on the Internet.\nPlaying a tennis match."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e6",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e6",
    "title": "Intelligent agents",
    "section": "I2AI_2 E6",
    "text": "I2AI_2 E6\nFor each of the following task environment properties, rank the example task environments from most to least according to how well the environment satisfies the property.\nLay out any assumptions you make to reach your conclusions.\n\nFully observable: driving; document classification; tutoring a student in calculus; skin cancer diagnosis from images\nContinuous: driving; spoken conversation; written conversation; climate engineering by stratospheric aerosol injection\nStochastic: driving; sudoku; poker; soccer\nStatic: chat room; checkers; tax planning; tennis"
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e7",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e7",
    "title": "Intelligent agents",
    "section": "I2AI_2 E7",
    "text": "I2AI_2 E7\nDefine in your own words the following terms\n\nRationality\nAutonomy\nReflex agent,\nModel-based agent\nGoal-based agent\nUtility-based agent\nLearning agent"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#agent",
    "href": "lectures/I2AI/2/slides.html#agent",
    "title": "Intelligent agents",
    "section": "Agent",
    "text": "Agent\n\n\n\nFigure 1: Agents interact with environments through sensors and actuators\n\n\n\nAn agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\n\nThe term percept refers to the content an agent‚Äôs sensors are perceiving\nAn agent‚Äôs percept sequence is the complete history of everything the agent has ever perceived\nThe agent function maps any given percept sequence to an action (an abstract mathematical description)\nThe agent function for an AI agent will be implemented by an agent program (a concrete implementation, running within some physical system)\n\n\n\n\n\n\n\nExample\n\n\nTo illustrate these ideas Russel and Norvig (2022, 55) use a simple example‚Äîthe vacuum-cleaner world, which consists of a robotic vacuum-cleaning agent in a world consisting of squares that can be either dirty or clean. The vacuum agent perceives which square it is in and whether there is dirt in the square. The agent starts in square A. The available actions are to move to the right, move to the left, suck up the dirt, or do nothing. One very simple agent function is the following: if the current square is dirty, then suck; otherwise, move to the other square.\n\n\n\nAccording to Russel and Norvig (2022, 56), all areas of engineering can be seen as designing artifacts that interact with the world. AI operates at the most interesting end to the spectrum, where the artifacts have significant computational resources and the task environment requires nontrivial decision making."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#rational-agent",
    "href": "lectures/I2AI/2/slides.html#rational-agent",
    "title": "Intelligent agents",
    "section": "Rational agent",
    "text": "Rational agent\nA rational agent is one that does the right thing.\n\n\nFor each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has (Russel and Norvig 2022, 58).\n\n\n\nWhat is rational at any given time depends on four things:\n\nThe performance measure that defines the criterion of success\nThe agent‚Äôs prior knowledge of the environment\nThe actions that the agent can performance\nThe agent‚Äôs percept sequence to date\n\n\n\n\n\n\n\nExample\n\n\nUnder following circumstances, the vacuum cleaning agent is rational:\n\nThe performance measure of the vacuum cleaner might award one point for each clean square at each time step, over a ‚Äúlifetime‚Äù of 1,000 time steps (to prevent the cleaner to oscillate needlessly back and forth)\nThe ‚Äúgeography‚Äù of the environment is known a priori but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking cleans the current square. The Right and Left actions move the agent one square except when this would take the agent outside the environment in which case the agent remains where it is\nThe only available action is Right, Left, and Suck\nThe agent correctly perceives its location and wether that location contains dirt\n\n(For details such as tabulated agent functions please see Russel and Norvig (2022))\n\n\n\n\n\nIt can be quite hard to formulate a performance measure correctly, however:\n\nIf we use, to achieve our purposes, a mechanical agency with those operation we cannot interfere once we have started it [‚Ä¶] we had better be quite sure that the purpose built into the machine is the purpose which we really desire (Wiener 1960, 1358)"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#rationality",
    "href": "lectures/I2AI/2/slides.html#rationality",
    "title": "Intelligent agents",
    "section": "Rationality",
    "text": "Rationality\nRationality is not the same as perfection.\n\n\nRationality maximizes expected performance\nPerfection maximizes actual performance\nPerfection requires omniscience\nRational choice depends only on the percept sequence to date\n\n\n\nAs the environment is usually not completely known a priori and completely predictable (or stable), information gathering and learning are important parts of rationality (Russel and Norvig 2022, 59).\n\n\n\n\n\n\nExample\n\n\nThe vacuum cleaner needs to explore an initially unknown environment (i.e., exploration) to maximize its expected performance. In addition, a vacuum cleaner that learns to predict where and when additional dirt will appear will do better than one that does not."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#components",
    "href": "lectures/I2AI/2/slides.html#components",
    "title": "Intelligent agents",
    "section": "Components",
    "text": "Components\nBefore designing an agent (the solution), the task environment (the problem) must be specified as fully as possible, including\n\n\nthe performance measure (P),\nthe environment (E),\nthe actuators (A), and\nthe sensors (S)\n\n\n\nRussel and Norvig (2022) call the task environment PEAS.\n\n\n\n\n\n\n\n\nExample of an PEAS description\n\n\nTask environment of a taxi driver agent\n\nP: Safe, fast, legal, comfortable, maximize profits, minimize impact on other road users\nE: Roads, other road users, police, pedestrians, customers, weather\nA: Steering, accelerator, brake, signal horn, display, speech\nS: Cameras, radar, speedometer, GPS, engine, sensors, accelerometer, microphones, touchscreen\n\nSource: Russel and Norvig (2022, 61)"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#properties",
    "href": "lectures/I2AI/2/slides.html#properties",
    "title": "Intelligent agents",
    "section": "Properties",
    "text": "Properties\nTask environments can be categorized along following dimensions (Russel and Norvig 2022, 62‚Äì64):\n\n\nFully observable vs. partially observable\nSingle agent vs. multi-agent\nDeterministic vs. nondeterministic\nEpisodic vs. sequential\nStatic vs. dynamic\nDiscrete vs. continuous\nKnown vs. unknown\n\n\n\nIf an agent‚Äôs sensors give it access to the full state of the environment at any point in time, then we say that the task environment is fully observable (e.g., image analysis).\nWhen multiple agents intend to maximize a performance measure that depends on the behavior of other agents, we say the environment is multi-agent (e.g., chess).\nWhen the environment is completely determined by the current state and the actions performed by the agent(s), it is called a deterministic environment (e.g., crossword puzzle). When a model of the environment explicitly uses probabilities, it is called a stochastic environment (e.g., poker).\nIf an agent‚Äôs experience is divided into atomic episodes in which the agent receives a perception and then performs a single action, and if the next episode does not depend on the actions performed in the previous episodes, then we say that the task environment is episodic (e.g., image analysis).\nIf the environment changes while an agent is deliberating, then the environment is dynamic (e.g., taxi driving).\nIf the environment has a finite number of different states, we speak of discrete environments (e.g., chess).\nIf the outcomes (or outcome probabilities) for all actions are given, then the environment is known (e.g., solitaire card game).\nSource: Russel and Norvig (2022), p.62-64\n\n\nThe hardest case is partially observable, multi-agent, nondeterministic, sequential, dynamic, and continuous."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#simple-reflex-agents",
    "href": "lectures/I2AI/2/slides.html#simple-reflex-agents",
    "title": "Intelligent agents",
    "section": "Simple reflex agents",
    "text": "Simple reflex agents\n\n\n\nFigure 2: A simple reflex agent\n\n\n\n\nRectangles are used to denote the current internal state of the agent‚Äôs decision process, rectangles with rounded corners to represent the background information used in the process.\n\nSimple reflex agents select actions on the basis of the current percept, ignoring the rest of the percept history. Thus, these agents work only if the correct decision can be made on the basis of just the current percept. The environment needs to be fully observable (Russel and Norvig 2022, 68)."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#model-based-reflex-agents",
    "href": "lectures/I2AI/2/slides.html#model-based-reflex-agents",
    "title": "Intelligent agents",
    "section": "Model-based reflex agents",
    "text": "Model-based reflex agents\n\n\n\nFigure 3: A model-based reflex agent\n\n\n\nModel-based reflex agents use transition models and sensor models to keep track of the state of the world as perceived by the sensors (i.e., internal state). (Russel and Norvig 2022, 70).\nThe transition model reflects ‚Äúhow the world works,‚Äù i.e., how the world evolves (a) independently of the agent and (b) depending on the agent‚Äôs actions.\nThe sensor model reflects how the state of the world is reflected in the agent‚Äôs percepts (i.e., by its sensors).\n\n\n\n\n\n\nTypes of representation of states and the transitions between them\n\n\nThe representations of states can be placed along an axis of increasing complexity and expressive power (Russel and Norvig 2022, 76‚Äì77):\n\nAtomic representation: a state is a blackbox with no internal structure (A ‚Äì> B)\nFactored representation: a state consists of a vector of attribute values; values can be Boolean, real-valued, or one of a fixed set of symbols; factored states can share some attributes and not others, which makes it easier to identify transitions between states\nStructured representation: a state includes objects, each of which may have attributes of its own as well as relationships to other objects; structured representations underlie relational databases and first-order logic, first-oder probability models, and much of natural language understanding.\n\nThe more expressive language is much more concise, but makes reasoning and learning more complex."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#goal-based-agents",
    "href": "lectures/I2AI/2/slides.html#goal-based-agents",
    "title": "Intelligent agents",
    "section": "Goal-based agents",
    "text": "Goal-based agents\n\n\n\nFigure 4: A model-based, goal-based agent\n\n\n\nA model-based, goal-based agent keeps track of the world state as well as a set of goals it is trying to achieve. Such an agent chooses an action that will (eventually) lead to the achievement of its goals (Russel and Norvig 2022, 72)."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#utility-based-agents",
    "href": "lectures/I2AI/2/slides.html#utility-based-agents",
    "title": "Intelligent agents",
    "section": "Utility-based agents",
    "text": "Utility-based agents\n\n\n\nFigure 5: A model-based, utility-based agent\n\n\n\nA model-based, utility-based agent uses a model of the world, along with a utility function that measures its preferences among states of the world. It chooses the action that leads to the best expected utility, where expected utility is computed by averaging over all possible states, weighted by the probability of the outcome (Russel and Norvig 2022, 73).\nThe utility function is essentially an internalization of the performance measure.\nA utility-based agent has many advantages in terms of flexibility and learning, which are particularly helpful in environments characterized by partial observability and nondeterminism.\nIn addition, there are cases where the goals are insufficient but a utility-based agent can still make rational decisions based on the probabilities and the utilities of the outcomes:\n\nWhen there are conflicting goals, the utility function specifies the appropriate tradeoff.\nLikelihood of success (i.e., goal achievement) can be weighed against the importance of the goals\n\nMode- and utility-based agents are difficult to implement. They need to model and keep track of the task environment, which requires ingenious sensors, sophisticated algorithms, and a high computational complexity.\nThere are also utility-based agents that are not model-based. These agents just learn what action is best in a particular situation without any ‚Äúunderstanding‚Äù of its impact on the environment (e.g., based on reinforcement learning)."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#learning-agents",
    "href": "lectures/I2AI/2/slides.html#learning-agents",
    "title": "Intelligent agents",
    "section": "Learning agents",
    "text": "Learning agents\n\n\n\nFigure 6: A learning agent\n\n\n\nEach type of agent can be either hand-programmed or created as a learning agent. The behavior of learning agents is (also) determined by their own experience, while the behavior of hand-programmed agents is solely determined by their initial programming. Thus, learning agents have greater autonony.\nA learning agent consists of four conceptual components (Russel and Norvig 2022, p- 74-75), as shown in Figure¬†6:\n\nThe performance element is responsible for taking percepts and selecting actions (i.e., what has previously been considered the entire agent program).\nThe learning element is responsible for improvements. It uses feedback from the critic on how the agent is doing and determines how the performance element should be modified to do better in the future. It can make changes to any of the ‚Äúknowledge components‚Äù shown in the agent diagrams (i.e., condition-action-rules, transition model, sensor model)\nThe performance standard is responsible to inform the agent about the meaning of percepts ‚Äî are they good nor not (e.g., the meaning of receiving no tips from passengers is a negative contribution to an automated taxis‚Äôs overall performance). The standard is fixed and cannot be influenced by the agent.\nThe problem generator is responsible for suggesting actions that lead to new and informative experiences. The problem generator suggests exploratory actions that may be suboptimal in the short term, but can lead to the discovery of better actions in the long term."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e1",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e1",
    "title": "Intelligent agents",
    "section": "I2AI_2 E1",
    "text": "I2AI_2 E1\nDefine in your own words the following terms:\n\nAgent\nEnvironment\nSensor\nActuator\nPercept\nAgent function\nAgent program"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e2",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e2",
    "title": "Intelligent agents",
    "section": "I2AI_2 E2",
    "text": "I2AI_2 E2\nFor each of the following agents, specify the sensors, actuators, and environment:\n\nMicrowave oven\nChess program\nAutonomous supply delivery"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e3",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e3",
    "title": "Intelligent agents",
    "section": "I2AI_2 E3",
    "text": "I2AI_2 E3\nDescribe a task environments in which the performance measure is easy to specify completely and correctly, and a in which it is not."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e4",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e4",
    "title": "Intelligent agents",
    "section": "I2AI_2 E4",
    "text": "I2AI_2 E4\nFor each of the following assertions, say whether it is true or false and support your answer with examples or counterexamples where appropriate.\n\nAn agent that senses only partial information about the state cannot be perfectly rational.\nThere exist task environments in which no pure reflex agent can behave rationally.\nThere exists a task environment in which every agent is rational.\nThe input to an agent program is the same as the input to the agent function.\nEvery agent is rational in an unobservable environment.\nThere is a model-based reflex agent that can remember all of its percepts.\nSuppose agent A1 is rational and agent A2 is irrational. There exists a task environment where A2‚Äôs actual score will be greater than A1‚Äôs actual score."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e5",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e5",
    "title": "Intelligent agents",
    "section": "I2AI_2 E5",
    "text": "I2AI_2 E5\nFor each of the following activities, give a PEAS description of the task environment and characterize it in terms of the properties discussed in class.\n\nPlaying soccer.\nExploring the subsurface oceans of Titan.\nShopping for used AI books on the Internet.\nPlaying a tennis match."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e6",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e6",
    "title": "Intelligent agents",
    "section": "I2AI_2 E6",
    "text": "I2AI_2 E6\nFor each of the following task environment properties, rank the example task environments from most to least according to how well the environment satisfies the property.\nLay out any assumptions you make to reach your conclusions.\n\nFully observable: driving; document classification; tutoring a student in calculus; skin cancer diagnosis from images\nContinuous: driving; spoken conversation; written conversation; climate engineering by stratospheric aerosol injection\nStochastic: driving; sudoku; poker; soccer\nStatic: chat room; checkers; tax planning; tennis"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e7",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e7",
    "title": "Intelligent agents",
    "section": "I2AI_2 E7",
    "text": "I2AI_2 E7\nDefine in your own words the following terms\n\nRationality\nAutonomy\nReflex agent,\nModel-based agent\nGoal-based agent\nUtility-based agent\nLearning agent"
  },
  {
    "objectID": "lectures/DIiI/0/index.html",
    "href": "lectures/DIiI/0/index.html",
    "title": "Administrivia üßê",
    "section": "",
    "text": "Top\nSlides\n\n\nGeneral remarks\nThis course will be taught using traditional synchronous lectures. The focus of the live sessions is on discussing real world cases and developing theoretically-based explanations. In addition, you will do an original research on a digital innovation case on your own (see assignment).\nIt is of really important that you prepare yourself for every session. You either need to work through teaching materials in advance or prepare to present the deliverables of your own case (see Table¬†1).\n\n\nContents\nThis course offers you the possibility to learn about industry-specific challenges and how this influences the way of organizing, managing and communicating digital innovations, in particular:\n\n\nSpecifics of industries (context, products, markets, obstacles, enablers)\nIndustry-specific applicability and adaptations of innovation strategies\nMethods for discovering need finding, ideation and prototyping of digital innovations\nCase studies of digital innovation in industries\nTheories that explain the mechanisms of digital innovations\n\n\n\n\nLearning objectives\nDuring this course, you should advance your skills in the following areas:\n\n\nEnhanced knowledge of various industries and their special characteristics\nUnderstanding of specifics of managing digital innovations in these industries\nKnowledge of theoretical models that help to explain and manage these innovations\nAbility to analyze specificities and complexities of different industries\nCapacity to adapt digital innovation management methods and tools accordingly\nAbility to clearly communicate conclusions and the underlying rationale\nAbility to evaluate new information and to question existing assumptions\nExpertise to develop independent contributions to practical and theoretical discourse\n\n\n\n\nEffort\nYou will gain 5 ECTS for this course. This equals approx. 150 hours workload, of which you need most to pass the course.\n\n\nLive sessions (approx. 40 hours)\nPreparation of the live sessions (approx. 20 hours)\nCase research and writing (approx. 90 hours)\n\n\n\nPlease prepare your schedule accordingly.\n\n\n\nGrading\nThe grade is based on the evaluation of the written case study (see assignment).\nThe presentations are mandatory but will not be graded individually.\n\n\nSchedule\n\nIt is of importance that you prepare yourself for the sessions and work on the case study continually. Please prepare your schedule accordingly.\n\n\n\n\nTable 1: Schedule summer term 2022 (may be subjected to changes)\n\n\n\n\n\n\n\nDate\nTopic\nPreparation\n\n\n\n\n03/15/2022\nAdministrivia & new patterns of innovation\n-\n\n\n03/16/2022\nWashington Post ‚Äî digital transformation in media (SWAT & BM Canvas)\nReading (case)\n\n\n03/17/2022\nThreadless.com ‚Äî creating value through online communities (crowdsourcing & innovation)\nReading (case)\n\n\n03/24/2022\nInterim presentation & coaching: ideas\nPresentation\n\n\n03/31/2022\nWieland ‚Äî virtualizing product development; guest speech (digital twin)\n-\n\n\n04/07/2022\nBVB ‚Äî innovation in retail; guest speech (signaling theory) ZOOM\n-\n\n\n04/14/2022\nInterim presentation & coaching: problem & solution\nPresentation\n\n\n04/21/2022\nSCHOTT ‚Äî a two sided approach for DI (path constitution theory)\nReading (case)\n\n\n04/28/2022\nInterim presentation & coaching: industry & company\nPresentation\n\n\n05/05/2022\nDeutsche Bank - innovation in finance, a safe bank? guest speech (ambidexterity)\n-\n\n\n05/12/2022\nInterim presentation & coaching: approach\nPresentation\n\n\n05/19/2022\nSpringest ‚Äî IT-enabled holacratic organizations (self-managing organizations)\nReading (case)\n\n\n06/02/2022\nInterim presentation & coaching: discussion\nPresentation\n\n\n06/23/2022\nFinal presentations\nPresentation\n\n\n07/07/2022\nDeadline case study\nSeminar paper"
  },
  {
    "objectID": "lectures/DIiI/assignment/index.html#paper",
    "href": "lectures/DIiI/assignment/index.html#paper",
    "title": "Assignment",
    "section": "Paper",
    "text": "Paper\nThe seminar paper is composed of following elements\n\n\nCase summary: A brief description of the case, the digital innovation and its impact on the organization and industry.\nThe Hook: the beginning of the case that grabs the reader and generates interest in reading further. It should stimulate interest or curiosity in the reader for what is to come by suggesting the main challenge that is addressed by the digital innovation.\nIndustry: here you characterize the industry and outline any information that is required or helpful for understanding the context of the innovation and the innovation process, e.g., relevant services or products, relevant frameworks like regulations, the primary factor that produces profit and key challenges addressed by the innovation described in this case study.\nCompany: provides a decent overview of the company (organization, respectively) including the most salient features about the business and challenges faced that are related to the innovation.\nProblem: you outline and reflect the challenges which the digital innovation addresses (should address) and synthesize them into a concise problem statement.\nApproach: covers how the organization has developed and implemented the digital innovation and, if data is available, what challenges needed to be tackled.\nSolution: provides a rich description of the digital innovation, how it approaches the initially outlined problem statement and which effects it has generated on the organization (e.g., revenues) and the industry.\nDiscussion: you comment, provide analysis and identify theories, or connect concepts or learning that help explain the mechanisms and effects of the innovation.\nLessons learned: you should make at least one firm recommendation for practitioners dealing with innovation management in the industry and explain why you have made this recommendation.\nAppendix: covers at least comprehensible information on how you did collect the research or this case study. You may also add any information that is helpful to gain a broader understanding of the case but is not necessary to follow your line of though in the main body of the text.\n\n\n\n\n\n\nSummary\nHook (intro & motivation)\nIndustry (characteristics, challenges)\nCompany (overview, specifics)\nProblem\n\n\n\n\n\nApproach (innovation process)\nSolution\nDiscussion (explanation)\nLessons Learned (recommendations)\nAppendix (methods)"
  },
  {
    "objectID": "lectures/DIiI/assignment/index.html#industries",
    "href": "lectures/DIiI/assignment/index.html#industries",
    "title": "Assignment",
    "section": "Industries",
    "text": "Industries\nThe case should focus on a digital innovation in one of the following industries\n\nHealthcare\nManufacturing\nLogistics/transportation\nRetail\nProfessional services"
  },
  {
    "objectID": "lectures/DIiI/assignment/index.html#presentations",
    "href": "lectures/DIiI/assignment/index.html#presentations",
    "title": "Assignment",
    "section": "Presentations",
    "text": "Presentations\nYou will present your interim results repeatedly throughout the semester\n\n\nIdeas (brief intro to two to three digital innovations)\nGATE ‚Äî assignment of the topics\nIndustry & company (characteristics & challenges)\nProblem and solution\nApproach (development & implementation)\nDiscussion (theoretically grounded explanations)\nSummary & lessons learned\n\nFor dates please see the schedule"
  },
  {
    "objectID": "lectures/DIiI/assignment/slides.html#paper",
    "href": "lectures/DIiI/assignment/slides.html#paper",
    "title": "Assignment",
    "section": "Paper",
    "text": "Paper\nThe seminar paper is composed of following elements\n\n\nCase summary: A brief description of the case, the digital innovation and its impact on the organization and industry.\nThe Hook: the beginning of the case that grabs the reader and generates interest in reading further. It should stimulate interest or curiosity in the reader for what is to come by suggesting the main challenge that is addressed by the digital innovation.\nIndustry: here you characterize the industry and outline any information that is required or helpful for understanding the context of the innovation and the innovation process, e.g., relevant services or products, relevant frameworks like regulations, the primary factor that produces profit and key challenges addressed by the innovation described in this case study.\nCompany: provides a decent overview of the company (organization, respectively) including the most salient features about the business and challenges faced that are related to the innovation.\nProblem: you outline and reflect the challenges which the digital innovation addresses (should address) and synthesize them into a concise problem statement.\nApproach: covers how the organization has developed and implemented the digital innovation and, if data is available, what challenges needed to be tackled.\nSolution: provides a rich description of the digital innovation, how it approaches the initially outlined problem statement and which effects it has generated on the organization (e.g., revenues) and the industry.\nDiscussion: you comment, provide analysis and identify theories, or connect concepts or learning that help explain the mechanisms and effects of the innovation.\nLessons learned: you should make at least one firm recommendation for practitioners dealing with innovation management in the industry and explain why you have made this recommendation.\nAppendix: covers at least comprehensible information on how you did collect the research or this case study. You may also add any information that is helpful to gain a broader understanding of the case but is not necessary to follow your line of though in the main body of the text.\n\n\n\n\n\n\nSummary\nHook (intro & motivation)\nIndustry (characteristics, challenges)\nCompany (overview, specifics)\nProblem\n\n\n\n\n\nApproach (innovation process)\nSolution\nDiscussion (explanation)\nLessons Learned (recommendations)\nAppendix (methods)"
  },
  {
    "objectID": "lectures/DIiI/assignment/slides.html#industries",
    "href": "lectures/DIiI/assignment/slides.html#industries",
    "title": "Assignment",
    "section": "Industries",
    "text": "Industries\nThe case should focus on a digital innovation in one of the following industries\n\nHealthcare\nManufacturing\nLogistics/transportation\nRetail\nProfessional services"
  },
  {
    "objectID": "lectures/DIiI/assignment/slides.html#presentations",
    "href": "lectures/DIiI/assignment/slides.html#presentations",
    "title": "Assignment",
    "section": "Presentations",
    "text": "Presentations\nYou will present your interim results repeatedly throughout the semester\n\n\nIdeas (brief intro to two to three digital innovations)\nGATE ‚Äî assignment of the topics\nIndustry & company (characteristics & challenges)\nProblem and solution\nApproach (development & implementation)\nDiscussion (theoretically grounded explanations)\nSummary & lessons learned\n\nFor dates please see the schedule"
  }
]