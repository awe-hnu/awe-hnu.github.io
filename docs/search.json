[
  {
    "objectID": "index.html#bachelor",
    "href": "index.html#bachelor",
    "title": "welcome",
    "section": "Bachelor",
    "text": "Bachelor\n\nIMUK\n\nGrundzÃ¼ge Informationsmanagement\nGrundzÃ¼ge der Wirtschaftsinformatik"
  },
  {
    "objectID": "index.html#master",
    "href": "index.html#master",
    "title": "welcome",
    "section": "Master",
    "text": "Master\n\nDigital Innovation Management\n\nIntro to AI\nDI in Industry"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Me",
    "section": "",
    "text": "For more details please check my rofile at the HNU website"
  },
  {
    "objectID": "lectures/GWI/0/index.html#einheiten",
    "href": "lectures/GWI/0/index.html#einheiten",
    "title": "Administratives ðŸ§",
    "section": "Einheiten",
    "text": "Einheiten\nDie Vorlesung ist in 10 Kapitel (K) gegliedert\n\n\nK1 EinfÃ¼hrung: Was ist das und weshalb ist es wichtig?\nK2 Das digitale Zeitalter: Was macht unsere Zeit besonders?\nK3 IT: Was ist die Basis von Informationssystemen?\nK4 Datenbanken #1: Wie kÃ¶nnen Daten strukturiert werden?\nK5 Datenbanken #2: Wie kÃ¶nnen Datenstrukturen abgebildet werden?\nK6 Anwendungssysteme: Welche gibt es und wozu?\nK7 GeschÃ¤ftsprozessmanagement: Wie kÃ¶nnen AblÃ¤ufe gesteuert werden?\nK8 Prozess-Modellierung: Wie kÃ¶nnen Prozesse verstÃ¤ndlich abbilden?\nK9 IT-Management: Wie steure ich IT im Unternehmen?\nK10 Digitale GeschÃ¤ftsmodelle: Was ist das nÃ¤chste groÃŸe Ding?"
  },
  {
    "objectID": "lectures/GWI/0/index.html#lernmaterialien",
    "href": "lectures/GWI/0/index.html#lernmaterialien",
    "title": "Administratives ðŸ§",
    "section": "Lernmaterialien",
    "text": "Lernmaterialien\nJedes Kapitel enthÃ¤lt folgende Lernmaterialien:\n\n\nVorlesungsskript\nPodcast (Kommentare zu den Inhalten des Kapitels und ErgÃ¤nzungen; keine 1:1 Besprechung der Folien)\nÃœbungen (Wiederholung, Reflektion und Vertiefung)\nReferenz zu weiterfÃ¼hrender Literatur (insbesondere Krcmar (2015), Fank (2018), Mertens u.Â a. (2016), Lemke, Brenner, und Kirchner (2017) und BÃ¤chle, Daurer, und Kolb (2021))\nMoodle-Quiz\n\n\n\nAlle Inhalte sind prÃ¼fungsrelevant."
  },
  {
    "objectID": "lectures/GWI/0/index.html#ablauf-und-aufwÃ¤nde",
    "href": "lectures/GWI/0/index.html#ablauf-und-aufwÃ¤nde",
    "title": "Administratives ðŸ§",
    "section": "Ablauf und AufwÃ¤nde",
    "text": "Ablauf und AufwÃ¤nde\nDer Kurs basiert auf einem Blended Learning Konzept (â€žFlipped Classroomâ€œ)\n\n\nSie erarbeiten sich die Inhalte im Selbststudium (10 x ca. 3,5 h)\nWir wiederholen und vertiefen in der Veranstaltung (13 x ca. 2,5 h)\n\nZusammenfassung der Inhalte (Big Picture)\nAntwort auf Ihre Fragen (Q&A)\nÃœbungen (oft in Kleingruppen)\n\nIn der PrÃ¼fung zeigen Sie Ihre Kompetenzen durch Anwendung des Gelernten\n\n\n\nDer Kurs ist mit 4 ECTS gewichtet. Die ECTS entsprechen einem Arbeitsaufwand von ca. 120 h.\nDie ungefÃ¤hre Verteilung des Arbeitsaufwands entnehmen Sie bitte obiger Auflistung. Die restliche Zeit ist fÃ¼r das Selbststudium, insbesondere die Vorbereitung auf die AbschlussprÃ¼fung vorgesehen.\nDie Erfahrung aus vorangegangen Kursen lehrt, dass ohne die Teilnahme an den Live-Sessions und die Bearbeitung der Ãœbungen das Bestehen des Kurses mit einer ordentlichen Note kaum mÃ¶glich ist.\nDieses Lern-Set-Up soll Sie dazu befÃ¤higen\n\neigenverantwortlich mit Lernangeboten umzugehen und wirksame Lernstrategien zu entwickeln,\nden Einsatz von digitalen Werkzeugen in Unternehmen gesellschaftskritisch zu reflektieren,\nZusammenhÃ¤nge zu erfassen, auf den Punkt zu bringen und fÃ¼r andere verstÃ¤ndlich zu erklÃ¤ren"
  },
  {
    "objectID": "lectures/GWI/0/index.html#prÃ¼fungsleistung",
    "href": "lectures/GWI/0/index.html#prÃ¼fungsleistung",
    "title": "Administratives ðŸ§",
    "section": "PrÃ¼fungsleistung",
    "text": "PrÃ¼fungsleistung\n\n\nDie Vorlesung wird anhand einer PrÃ¼fung bewertet\n\nDie PrÃ¼fung umfasst 90 Punkte und dauert 90 Minuten\nSie benÃ¶tpgen 45 Punkte zum Bestehen\nAlle Themen sind prÃ¼fungsrelevant, insbesondere die Ãœbungen\n\nVoraussetzung zur Teilnahme an der PrÃ¼fung ist eine Studienleistung (Referat)\nSie kÃ¶nnen einen Notenbonus von 0,3 Notenpunkten erhalten, wenn Sie alle Quizze in Moodle rechtzeitig bestehen 2"
  },
  {
    "objectID": "lectures/GWI/0/slides.html#einheiten",
    "href": "lectures/GWI/0/slides.html#einheiten",
    "title": "Administratives ðŸ§",
    "section": "Einheiten",
    "text": "Einheiten\nDie Vorlesung ist in 10 Kapitel (K) gegliedert\n\n\nK1 EinfÃ¼hrung: Was ist das und weshalb ist es wichtig?\nK2 Das digitale Zeitalter: Was macht unsere Zeit besonders?\nK3 IT: Was ist die Basis von Informationssystemen?\nK4 Datenbanken #1: Wie kÃ¶nnen Daten strukturiert werden?\nK5 Datenbanken #2: Wie kÃ¶nnen Datenstrukturen abgebildet werden?\nK6 Anwendungssysteme: Welche gibt es und wozu?\nK7 GeschÃ¤ftsprozessmanagement: Wie kÃ¶nnen AblÃ¤ufe gesteuert werden?\nK8 Prozess-Modellierung: Wie kÃ¶nnen Prozesse verstÃ¤ndlich abbilden?\nK9 IT-Management: Wie steure ich IT im Unternehmen?\nK10 Digitale GeschÃ¤ftsmodelle: Was ist das nÃ¤chste groÃŸe Ding?"
  },
  {
    "objectID": "lectures/GWI/0/slides.html#lernmaterialien",
    "href": "lectures/GWI/0/slides.html#lernmaterialien",
    "title": "Administratives ðŸ§",
    "section": "Lernmaterialien",
    "text": "Lernmaterialien\nJedes Kapitel enthÃ¤lt folgende Lernmaterialien:\n\n\nVorlesungsskript\nPodcast (Kommentare zu den Inhalten des Kapitels und ErgÃ¤nzungen; keine 1:1 Besprechung der Folien)\nÃœbungen (Wiederholung, Reflektion und Vertiefung)\nReferenz zu weiterfÃ¼hrender Literatur (insbesondere Krcmar (2015), Fank (2018), Mertens u.Â a. (2016), Lemke, Brenner, und Kirchner (2017) und BÃ¤chle, Daurer, und Kolb (2021))\nMoodle-Quiz\n\n\n\nAlle Inhalte sind prÃ¼fungsrelevant."
  },
  {
    "objectID": "lectures/GWI/0/slides.html#ablauf-und-aufwÃ¤nde",
    "href": "lectures/GWI/0/slides.html#ablauf-und-aufwÃ¤nde",
    "title": "Administratives ðŸ§",
    "section": "Ablauf und AufwÃ¤nde",
    "text": "Ablauf und AufwÃ¤nde\nDer Kurs basiert auf einem Blended Learning Konzept (â€žFlipped Classroomâ€œ)\n\n\nSie erarbeiten sich die Inhalte im Selbststudium (10 x ca. 3,5 h)\nWir wiederholen und vertiefen in der Veranstaltung (13 x ca. 2,5 h)\n\nZusammenfassung der Inhalte (Big Picture)\nAntwort auf Ihre Fragen (Q&A)\nÃœbungen (oft in Kleingruppen)\n\nIn der PrÃ¼fung zeigen Sie Ihre Kompetenzen durch Anwendung des Gelernten\n\n\n\nDer Kurs ist mit 4 ECTS gewichtet. Die ECTS entsprechen einem Arbeitsaufwand von ca. 120 h.\nDie ungefÃ¤hre Verteilung des Arbeitsaufwands entnehmen Sie bitte obiger Auflistung. Die restliche Zeit ist fÃ¼r das Selbststudium, insbesondere die Vorbereitung auf die AbschlussprÃ¼fung vorgesehen.\nDie Erfahrung aus vorangegangen Kursen lehrt, dass ohne die Teilnahme an den Live-Sessions und die Bearbeitung der Ãœbungen das Bestehen des Kurses mit einer ordentlichen Note kaum mÃ¶glich ist.\nDieses Lern-Set-Up soll Sie dazu befÃ¤higen\n\neigenverantwortlich mit Lernangeboten umzugehen und wirksame Lernstrategien zu entwickeln,\nden Einsatz von digitalen Werkzeugen in Unternehmen gesellschaftskritisch zu reflektieren,\nZusammenhÃ¤nge zu erfassen, auf den Punkt zu bringen und fÃ¼r andere verstÃ¤ndlich zu erklÃ¤ren"
  },
  {
    "objectID": "lectures/GWI/0/slides.html#prÃ¼fungsleistung",
    "href": "lectures/GWI/0/slides.html#prÃ¼fungsleistung",
    "title": "Administratives ðŸ§",
    "section": "PrÃ¼fungsleistung",
    "text": "PrÃ¼fungsleistung\n\n\nDie Vorlesung wird anhand einer PrÃ¼fung bewertet\n\nDie PrÃ¼fung umfasst 90 Punkte und dauert 90 Minuten\nSie benÃ¶tpgen 45 Punkte zum Bestehen\nAlle Themen sind prÃ¼fungsrelevant, insbesondere die Ãœbungen\n\nVoraussetzung zur Teilnahme an der PrÃ¼fung ist eine Studienleistung (Referat)\nSie kÃ¶nnen einen Notenbonus von 0,3 Notenpunkten erhalten, wenn Sie alle Quizze in Moodle rechtzeitig bestehen 2"
  },
  {
    "objectID": "lectures/GWI/semesterleistung/index.html#setting",
    "href": "lectures/GWI/semesterleistung/index.html#setting",
    "title": "Semesterleistung",
    "section": "Setting",
    "text": "Setting\n\nStellen Sie sich vor, Sie arbeiten in einem grÃ¶ÃŸeren Unternehmen Ihrer Wahl und treffen Ihren CEO im Fahrstuhl. Sie nutzen die Gelegenheit, â€žIhr Themaâ€œ dem CEO zu prÃ¤sentieren und ihn davon zu Ã¼berzeugen, dass sein Unternehmen in Ihr Thema investieren sollte. Bis der Fahrstuhl in der Vorstandsetage angekommen ist, haben Sie nur 300 Sekunden Zeit. In dieser Zeit mÃ¼ssen Sie den CEO von der Relevanz des Themas Ã¼berzeugen. Das machen Sie am besten, indem Sie kurz in Ihr Thema einfÃ¼hren und dann mÃ¶glichst konkret zeigen, wie Ihr Thema dazu beitragen kÃ¶nnte, dass das Unternehmen erfolreicher wird."
  },
  {
    "objectID": "lectures/GWI/semesterleistung/index.html#rahmenbedingungen",
    "href": "lectures/GWI/semesterleistung/index.html#rahmenbedingungen",
    "title": "Semesterleistung",
    "section": "Rahmenbedingungen",
    "text": "Rahmenbedingungen\n\n\nLÃ¤nge mindestens 4 Minuten, maximal 5 Minuten\nÃœberzeugende Vorstellung des Themas\n\nallgemeinverstÃ¤ndliche EinfÃ¼hrung in das Thema\n(â€œwas ist es und weshalb ist es wichtig?â€)\nVorschlag eines Anwendungsfalls zur Thematik\n(wie kÃ¶nnen wir damit erfolgreich sein?)\n\nEine Folie zur Illustration (Ausruck im Fahrstuhl, Einreichung vorab)\nDie Termine entnehmen Sie dem Zeitplan\n\n\n\nMit Ablegen der PrÃ¤sentationsprÃ¼fung versichern Sie, dass Sie die eingereichte PrÃ¼fungsleistung ohne fremde Hilfe verfasst haben. Bereits der Versuch, das Ergebnis der Arbeit durch TÃ¤uschung (z.B. durch Mithilfe Dritter, auch Kommilitonen) zu beeinflussen kann zum Nicht-Bestehen der Leistung fÃ¼hren."
  },
  {
    "objectID": "lectures/GWI/semesterleistung/index.html#themenwahl",
    "href": "lectures/GWI/semesterleistung/index.html#themenwahl",
    "title": "Semesterleistung",
    "section": "Themenwahl",
    "text": "Themenwahl\nSie suchen sich ein Thema aus einer Liste an mÃ¶glichen Themen aus (siehe Moodle)\n\n\nJedes Thema kann maximal zweimal vergeben werden\nDie Vergabe erfolgt nach dem Prinzip â€žFirst come, first serveâ€œ\nDie Wahl ist fix, kann nicht geÃ¤ndert werden\n\n\n\nThemenliste\n\nThemen\n5G\nAugmented reality\nCDN (content distribution network)\nConversational interfaces\nCrowdsourcing\nCyber physical system\nData lake\nDeep-learning\nDesktop virtualization\nDigital twin\nDistributed ledger\nEdge computing\nGreen computing\nIn-memory databases\nIntrusion detection systems\nLiDAR\nLoRaWan\nLow code platforms\nNFC (near field communications)\nPlatform as a Service (PaaS)\nProcess mining\nRed teaming\nRobotic process automation (RPA)\nScaled agile framework (SAFe)\nServerless computing\nSmart contracts\nSocial commerce\nSocial engineering penetration tests\nVirtual reality\nWorkplace as a service"
  },
  {
    "objectID": "lectures/GWI/semesterleistung/index.html#prÃ¤sentation",
    "href": "lectures/GWI/semesterleistung/index.html#prÃ¤sentation",
    "title": "Semesterleistung",
    "section": "PrÃ¤sentation",
    "text": "PrÃ¤sentation\nDen Termin zur PrÃ¤sentationsprÃ¼fung entnehmen Sie dem Zeitplan.\n\n\nWÃ¤hrend der PrÃ¤sentationsprÃ¼fung sprechen Sie frei\nSie haben nur eine Folie zur Illustration. Bringen Sie diese ausgedruckt mit\nAndere Hilfsmittel wie bspw. Notizen sind nicht erlaub\nDie Zeit ist fix, die PrÃ¤sentation dauert min 4, maximal 5 Minuten.\nVerschwenden Sie keine Zeit. Ãœben sie vorher."
  },
  {
    "objectID": "lectures/GWI/semesterleistung/slides.html#rahmenbedingungen",
    "href": "lectures/GWI/semesterleistung/slides.html#rahmenbedingungen",
    "title": "Semesterleistung",
    "section": "Rahmenbedingungen",
    "text": "Rahmenbedingungen\n\n\nLÃ¤nge mindestens 4 Minuten, maximal 5 Minuten\nÃœberzeugende Vorstellung des Themas\n\nallgemeinverstÃ¤ndliche EinfÃ¼hrung in das Thema\n(â€œwas ist es und weshalb ist es wichtig?â€)\nVorschlag eines Anwendungsfalls zur Thematik\n(wie kÃ¶nnen wir damit erfolgreich sein?)\n\nEine Folie zur Illustration (Ausruck im Fahrstuhl, Einreichung vorab)\nDie Termine entnehmen Sie dem Zeitplan\n\n\n\nMit Ablegen der PrÃ¤sentationsprÃ¼fung versichern Sie, dass Sie die eingereichte PrÃ¼fungsleistung ohne fremde Hilfe verfasst haben. Bereits der Versuch, das Ergebnis der Arbeit durch TÃ¤uschung (z.B. durch Mithilfe Dritter, auch Kommilitonen) zu beeinflussen kann zum Nicht-Bestehen der Leistung fÃ¼hren."
  },
  {
    "objectID": "lectures/GWI/semesterleistung/slides.html#themenwahl",
    "href": "lectures/GWI/semesterleistung/slides.html#themenwahl",
    "title": "Semesterleistung",
    "section": "Themenwahl",
    "text": "Themenwahl\nSie suchen sich ein Thema aus einer Liste an mÃ¶glichen Themen aus (siehe Moodle)\n\n\nJedes Thema kann maximal zweimal vergeben werden\nDie Vergabe erfolgt nach dem Prinzip â€žFirst come, first serveâ€œ\nDie Wahl ist fix, kann nicht geÃ¤ndert werden\n\n\n\nThemenliste\n\nThemen\n5G\nAugmented reality\nCDN (content distribution network)\nConversational interfaces\nCrowdsourcing\nCyber physical system\nData lake\nDeep-learning\nDesktop virtualization\nDigital twin\nDistributed ledger\nEdge computing\nGreen computing\nIn-memory databases\nIntrusion detection systems\nLiDAR\nLoRaWan\nLow code platforms\nNFC (near field communications)\nPlatform as a Service (PaaS)\nProcess mining\nRed teaming\nRobotic process automation (RPA)\nScaled agile framework (SAFe)\nServerless computing\nSmart contracts\nSocial commerce\nSocial engineering penetration tests\nVirtual reality\nWorkplace as a service"
  },
  {
    "objectID": "lectures/GWI/semesterleistung/slides.html#prÃ¤sentation",
    "href": "lectures/GWI/semesterleistung/slides.html#prÃ¤sentation",
    "title": "Semesterleistung",
    "section": "PrÃ¤sentation",
    "text": "PrÃ¤sentation\nDen Termin zur PrÃ¤sentationsprÃ¼fung entnehmen Sie dem Zeitplan.\n\n\nWÃ¤hrend der PrÃ¤sentationsprÃ¼fung sprechen Sie frei\nSie haben nur eine Folie zur Illustration. Bringen Sie diese ausgedruckt mit\nAndere Hilfsmittel wie bspw. Notizen sind nicht erlaub\nDie Zeit ist fix, die PrÃ¤sentation dauert min 4, maximal 5 Minuten.\nVerschwenden Sie keine Zeit. Ãœben sie vorher."
  },
  {
    "objectID": "lectures/I2AI/0/index.html",
    "href": "lectures/I2AI/0/index.html",
    "title": "Administrivia ðŸ§",
    "section": "",
    "text": "Top\nSlides\n\n\nGeneral remarks\nThis course will be taught using traditional synchronous lectures.\nThe focus of the live sessions is on the introduction of major concepts, the discussion of application scenarios and some exercises.\nIt is highly recommended to prepare for each session, details (see TableÂ 1).\n\n\nContents\nAI is defined here as the study of agents that receive percepts from the environment and perform actions.\nIn this course, we will\n\n\ncover different types of agents (i.e., goal-based and utility-based agents)\nilluminate some concepts and major functions they implement,\ndiscuss, how to convert these agents into learning agents,\nidentify and discuss real life use cases for these agents, and\nhave a look at philosophical stances and ethical implications of AI.\n\n\n\n\nLearning objectives\nDuring this course, you should advance your skills in the following areas:\n\n\nUnderstanding of the origins, strengths and limitations of AI\nBasic knowledge of concepts, functions, and use cases of AI\nBasic knowledge of problem-solving algorithms, knowledge representation, probabilistic reasoning, machine learning, and natural language processing\nCapability to apply your knowledge and understanding of AI to different managerial and organizational contexts\nAbility to assess the potential of AI and use it in digital innovations\nAbility to evaluate new information and to question existing assumptions\nCapacity to assess social and ethical implications of AI applications\n\n\n\n\nSupporting literature\nThis course is essentially based on Russel and Norvig (2022)\n\n\n\n\nArtificial Intelligence: A Modern Approach, Global Edition, 4/E\nRussell / Norvig\nISBN-10: 1292401133 â€¢ ISBN-13: 9781292401133\nThe 3rd edition is available in the libary\n\n\n\n\nExam\nThere will we a written exam at the end of the semester.\nThe exam will\n\ntake place during the examination weeks,\nwill last 90 minutes,\ncover all contents discussed in lecture,\nfocus on the application of the knowledge gained in the course.\n\n\n\nSchedule\n\nIt is of importance that you reflect the contents of the session continually, do the exercises and ask questions.\n\n\n\n\nTable 1: Schedule summer term 2022 (may be subjected to changes)\n\n\n\n\n\n\n\nDate\nTopic\nPreparation\n\n\n\n\n03/21/2022\nAdministrivia, introduction & intelligent agents\n-\n\n\n04/04/2022\nSearch & games\nExercises\n\n\n04/11/2022\nKnowledge\nExercises\n\n\n05/02/2022\nUncertainty & probabilistic\nExercises\n\n\n05/16/2022\nMachine learning\nExercises\n\n\n05/30/2022\nGuest speech: reality of AI/ML in industry; language & communication\nExercises\n\n\n06/27/2022\nPhilosophy, ethics & safety\nCase Study\n\n\n07/04/2022\nConclusion, wrap-up and Q&A\nQuestions\n\n\n\n\n\n\n\n\nReferences\n\nRussel, Stuart, and Peter Norvig. 2022. Artificial Intelligence: A Modern Approach. Harlow: Pearson Education."
  },
  {
    "objectID": "lectures/I2AI/1/index.html#sci-fi",
    "href": "lectures/I2AI/1/index.html#sci-fi",
    "title": "Introduction",
    "section": "Sci-Fi",
    "text": "Sci-Fi\n\nThe perception of AI changed over the decades. This is well reflected in Sci-Fi Movies, e.g.\n\nR2D2 in Star Wars\nThe Terminator\nAgent Smith in the Matrix\nSonny in I,Robot\nWallÂ·E\netc."
  },
  {
    "objectID": "lectures/I2AI/1/index.html#reality",
    "href": "lectures/I2AI/1/index.html#reality",
    "title": "Introduction",
    "section": "Reality",
    "text": "Reality\n\nWill the fictions become reality? We will se. In any case, the intellectual frontiers of AI are wide open. The subfields span from artificial general intelligence (learning, reasoning, perception, etc.) to specific fields (e.g., translating, playing go) (Russel and Norvig 2022).\n\n\n\nâ€œI believe itâ€™s going to change the world more than anything in the history of mankind â€” even more than electricity.â€ â€”Kai-Fu Lee\n\n\n\n\nâ€œThe pace of progress in artificial intelligence is incredibly fast. Unless you have direct exposure to groups like Deepmind, you have no idea how fastâ€”it is growing at a pace close to exponential. The risk of something seriously dangerous happening is in the five-year timeframe. 10 years at most.â€ â€”Elon Musk\n\n\n\n\nâ€œForget artificial intelligence - in the brave new world of big data, itâ€™s artificial idiocy we should be looking out for.â€ â€”Tom Chatfield"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#towards-a-definition-of-ai",
    "href": "lectures/I2AI/1/index.html#towards-a-definition-of-ai",
    "title": "Introduction",
    "section": "Towards a definition of AI",
    "text": "Towards a definition of AI\nAI is the science of making machines to\n\n\nthink (though processes and reasoning)\n\nlike people\nrationally\n\nand to act (behavior)\n\nlike people\nrationally (Russel and Norvig 2022)\n\n\n\n\n\n\n\n\n\n\nWhat is rational?\n\n\n\nThe term rational is used here in a very specific, technical way:\n\nRational: maximally achieving pre-defined goals\nRationality only concerns what decisions are made (not the thought process behind them)\nGoals are expressed in terms of the utility of outcomes\nBeing rational means maximizing your expected utility"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#acting-humanly",
    "href": "lectures/I2AI/1/index.html#acting-humanly",
    "title": "Introduction",
    "section": "Acting humanly",
    "text": "Acting humanly\n\nThe Turing Test approach\nThe Turing Test (Turing 1950) tests if a computer has the ability to mimic peoplesâ€™ behavior. To pass the test, it would need following capabilities:\n\n\nnatural language processing (communicate)\nknowledge representation (store information)\nautomated reasoning (answer questions, draw new conclusions)\nmachine learning (adapt to new circumstances)\n\n\n\nWant to do a Turing Test? Play â€œBot or Notâ€\n\n\n\n\n\n\n\n\nTuring Test\n\n\n\nThe Turing Test (Turing 1950) was designed as a thought experiment that would sidestep the philosophical vagueness of the question â€œCan a machine think?â€ A computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer (Russel and Norvig 2022).\nThe Total Turing Test additionally requires interaction with objects and people in the real world. This also requires computer vision and robotic capabilities."
  },
  {
    "objectID": "lectures/I2AI/1/index.html#thinking-humanly",
    "href": "lectures/I2AI/1/index.html#thinking-humanly",
    "title": "Introduction",
    "section": "Thinking humanly",
    "text": "Thinking humanly\n\nThe cognitive modelling approach\nCognitive science is the study of the human brain and its processes â€” it examines how the human brain may be functioning. Cognitive science requires analytical observation and experimentation. We can learn about human thought in three ways (Russel and Norvig 2022):\n\n\nintrospection (trying to catch our own thoughts)\nexperiments (observing a person in action)\nbrain imaging (observing the brain in action)\n\n\n\n\n\n\n\n\n\nDifferences between cognitive computing and AI\n\n\n\nCognitive science is about making computers solve complex problems similar to how humans solve problems. Cognitive computing tries to replicate how humans would solve problems, while AI is not bound to human cognitive processes."
  },
  {
    "objectID": "lectures/I2AI/1/index.html#thinking-rationally",
    "href": "lectures/I2AI/1/index.html#thinking-rationally",
    "title": "Introduction",
    "section": "Thinking rationally",
    "text": "Thinking rationally\n\nThe laws of thought approach\nThe â€œlaws of thoughtâ€ refer to fundamental axiomatic rules upon which rational discorse itself is often considered to be based.\n\n\nSocrates is a man and all men are mortal, thus, it can be concluded that Socrates is mortal â€”Aristotle (384-322 BCE)\n\n\n\nIn principle, computers have been able to solve any solvable problem (i.e., make correct inferences), as long as\n\n\n\nthere are statements about any objects in the world,\nstatements about the relations among them, and\nthere is sufficient computing power available\n\n\n\n\n\n\n\n\n\nLogic, knowledge and probability\n\n\n\nThese laws of thought were supposed to govern the operation of the mind; the studies initiated the field called logic. However, human decisions are not always mathematically perfect or logical.\nLogic as conventionally understood requires knowledge of the world that is certain. As this condition is seldom achieved, the theory of probability fills this gap. Probability allows for rigorous reasoning with uncertain information. (Russel and Norvig 2022)"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#acting-rationally",
    "href": "lectures/I2AI/1/index.html#acting-rationally",
    "title": "Introduction",
    "section": "Acting rationally",
    "text": "Acting rationally\n\nThe rational agent approach\nAn agent is something that acts, an rational agent is one that acts so as to achieve the best coutcome (i.e., does the right thing), or, when there is uncertainty, the best expected outcome (i.e., does the appropriate thing) based on the objective that is provided to the agent (Russel and Norvig 2022).\n\nThe approach goes beyond the â€œlaws of thoughtâ€ approach as it involves actions based on\n\n\n\ninference (deducing that a given action is the best and then to act on this conclusion) and\nother mechanisms such as reflex (when speed is more successful than careful deliberation that takes some time)\n\n\n\n\n\n\n\n\n\nStandard model in AI\n\n\n\nRussel and Norvig (2022) call the approach where the primary definition of success is getting better and better at achieving rigid human-specified goals the standard model of AI research. This standard model prevails not only in AI, â€œbut also in control theory, where a controller minimizes a cost function; in operations research, where a policy maximizes a sum of rewards; in statistics, where a decision rule minimizes a loss function; and in economics, where a decision maker maximizes utility or some measure of social welfareâ€ (p.Â 22).\nThey also criticize the standard model because it is increasingly difficult to specify the goal completely and correctly (e.g., autonomous driving involves multiple goals such as reaching the goal safely, where a strict definition of safety requires staying in the garage because driving on the road has a risk of injury due to myriad factors; how should the trade-off between reaching the goal and taking a risk of injury be made? There are so many questions that are difficult to answer a priori). Mis-specified goals most likely do not reflect what human designers intend, e.g., by not taking into account human values that are not included in the goals."
  },
  {
    "objectID": "lectures/I2AI/1/index.html#the-thinking-machine",
    "href": "lectures/I2AI/1/index.html#the-thinking-machine",
    "title": "Introduction",
    "section": "The Thinking Machine",
    "text": "The Thinking Machine\nA series of interviews to some of the AI pioneers.\n\n\nThe full documentary is available here"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#a-brief-ai-timeline",
    "href": "lectures/I2AI/1/index.html#a-brief-ai-timeline",
    "title": "Introduction",
    "section": "A brief AI-timeline",
    "text": "A brief AI-timeline\n1943â€”1956 The inception of AI\n\n1943: McCulloch & Pitts: Boolean circuit model of brain (artificial neurons with on and off states; all logical connectives can be implemented with some network of these)\n1950: Turingâ€™s â€œComputing Machinery and Intelligenceâ€ (Turing (1950) already introduced the Turing test, machine learning, genetic algorithms, and reinforcement learning)\n1950s: Early AI programs (e.g., Arthur Samuelâ€™s influential checkers program that learned to play at a strong amateur level)"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#section",
    "href": "lectures/I2AI/1/index.html#section",
    "title": "Introduction",
    "section": "",
    "text": "1966â€”73 A dose of reality\n\nThe early AI programs failed on more difficult problems\n\nFocus on â€œinformed introspectionâ€ as to how humans perform a task\nLack of appreciation of the intractability of many of the problems\n\nSignification reduction of government funding of AI research\n\n\n1970â€”90 Expert systems (knowledge-based approaches)\n\n1969â€”79: Early development of knowledge-based systems (rule-based heuristic algorithms)\n1980â€”88: Expert systems industry booms (nearly every major U.S. corporate had its own AI group)\nSoon after that came the â€œAI winterâ€ (difficulties to build expert systems for complex domains due to uncertainty and a lack of learning)"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#section-1",
    "href": "lectures/I2AI/1/index.html#section-1",
    "title": "Introduction",
    "section": "",
    "text": "1990â€”present AI spring: (statistical approaches)\n\nFocus on probabilistic reasoning (rather than Boolean logic) and machine learning\nReunification of subfields such as computer vision, robotics, speech recognition, and natural language processing"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#section-2",
    "href": "lectures/I2AI/1/index.html#section-2",
    "title": "Introduction",
    "section": "",
    "text": "2012â€”present New excitement\n\nAdvances in computing power, WWW, and very large data sets\n(e.g., IBM Watsonâ€™s victory in Jeopardy!)\n\n\nImprovement in performance obtained from increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be obtained from tweaking the algorithm â€”Banko and Brill (2001)\n\n\nDeep learning systems offer significant performance gains\n(e.g., AlphaGoâ€™s victories)\nSignificant focus on AI in academia and industry\nAI systems find increasing application in the real world (e.g., robotic vehicles, machine translation, speech recognition, recommendations, autonomous planning, game playing, image understanding, medicine)"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#i2ai_1-e1",
    "href": "lectures/I2AI/1/index.html#i2ai_1-e1",
    "title": "Introduction",
    "section": "I2AI_1 E1",
    "text": "I2AI_1 E1\nDefine in your own words:\n\nintelligence\nartificial intelligence\nagent\nrationality\nlogical reasoning"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#i2ai_1-e2",
    "href": "lectures/I2AI/1/index.html#i2ai_1-e2",
    "title": "Introduction",
    "section": "I2AI_1 E2Â ",
    "text": "I2AI_1 E2Â \nEvery year the Loebner Prize is awarded to the program that comes closest to passing a version of the Turing Test.\nResearch and report on the latest winner of the Loebner prize.\n\nWhat techniques does it use?\nHow does it advance the state of the art in AI?"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#i2ai_1-e3",
    "href": "lectures/I2AI/1/index.html#i2ai_1-e3",
    "title": "Introduction",
    "section": "I2AI_1 E3",
    "text": "I2AI_1 E3\nTo what extent are the following computer systems instances of artificial intelligence:\n\nSupermarket bar code scanners\nWeb search engines\nVoice-activated telephone menus\nInternet routing algorithms that respond dynamically to the state of the network"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#i2ai_1-e4",
    "href": "lectures/I2AI/1/index.html#i2ai_1-e4",
    "title": "Introduction",
    "section": "I2AI_1 E4",
    "text": "I2AI_1 E4\nVarious subfields of AI have held contests by defining a standard task and inviting researchers to do their best. Examples include the DARPA Grand Challenge for robotic cars, the International Planning Competition, the Robocup robotic soccer league, the TREC information retrieval event, and contests in machine translation and speech recognition.\nInvestigate one of these contests and describe the progress made over the years.\n\nTo what degree have the contests advanced the state of the art in AI?\nTo what degree do they hurt the field by drawing energy away from new ideas?"
  },
  {
    "objectID": "lectures/I2AI/1/index.html#i2ai_1-e5",
    "href": "lectures/I2AI/1/index.html#i2ai_1-e5",
    "title": "Introduction",
    "section": "I2AI_1 E5",
    "text": "I2AI_1 E5\nRead the statements (one after the other) and discuss if the second sentence of each statement is true and if it does imply the first.\n\nâ€œSurely computers cannot be intelligentâ€”they can do only what their programmers tell them.â€ Is the latter statement true, and does it imply the former?\n\n\nâ€œSurely animals cannot be intelligentâ€”they can do only what their genes tell them.â€ Is the latter statement true, and does it imply the former?\n\n\nâ€œSurely animals, humans, and computers cannot be intelligentâ€”they can do only what their constituent atoms are told to do by the laws of physics.â€ Is the latter statement true, and does it imply the former?"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#sci-fi",
    "href": "lectures/I2AI/1/slides.html#sci-fi",
    "title": "Introduction",
    "section": "Sci-Fi",
    "text": "Sci-Fi\n\nThe perception of AI changed over the decades. This is well reflected in Sci-Fi Movies, e.g.\n\nR2D2 in Star Wars\nThe Terminator\nAgent Smith in the Matrix\nSonny in I,Robot\nWallÂ·E\netc."
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#reality",
    "href": "lectures/I2AI/1/slides.html#reality",
    "title": "Introduction",
    "section": "Reality",
    "text": "Reality\n\nWill the fictions become reality? We will se. In any case, the intellectual frontiers of AI are wide open. The subfields span from artificial general intelligence (learning, reasoning, perception, etc.) to specific fields (e.g., translating, playing go) (Russel and Norvig 2022).\n\n\n\nâ€œI believe itâ€™s going to change the world more than anything in the history of mankind â€” even more than electricity.â€ â€”Kai-Fu Lee\n\n\n\n\nâ€œThe pace of progress in artificial intelligence is incredibly fast. Unless you have direct exposure to groups like Deepmind, you have no idea how fastâ€”it is growing at a pace close to exponential. The risk of something seriously dangerous happening is in the five-year timeframe. 10 years at most.â€ â€”Elon Musk\n\n\n\n\nâ€œForget artificial intelligence - in the brave new world of big data, itâ€™s artificial idiocy we should be looking out for.â€ â€”Tom Chatfield"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#towards-a-definition-of-ai",
    "href": "lectures/I2AI/1/slides.html#towards-a-definition-of-ai",
    "title": "Introduction",
    "section": "Towards a definition of AI",
    "text": "Towards a definition of AI\nAI is the science of making machines to\n\n\nthink (though processes and reasoning)\n\nlike people\nrationally\n\nand to act (behavior)\n\nlike people\nrationally (Russel and Norvig 2022)\n\n\n\n\n\n\n\n\n\n\nWhat is rational?\n\n\nThe term rational is used here in a very specific, technical way:\n\nRational: maximally achieving pre-defined goals\nRationality only concerns what decisions are made (not the thought process behind them)\nGoals are expressed in terms of the utility of outcomes\nBeing rational means maximizing your expected utility"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#acting-humanly",
    "href": "lectures/I2AI/1/slides.html#acting-humanly",
    "title": "Introduction",
    "section": "Acting humanly",
    "text": "Acting humanly\nThe Turing Test approach\nThe Turing Test (Turing 1950) tests if a computer has the ability to mimic peoplesâ€™ behavior. To pass the test, it would need following capabilities:\n\n\nnatural language processing (communicate)\nknowledge representation (store information)\nautomated reasoning (answer questions, draw new conclusions)\nmachine learning (adapt to new circumstances)\n\n\n\nWant to do a Turing Test? Play â€œBot or Notâ€\n\n\n\n\n\n\n\n\nTuring Test\n\n\nThe Turing Test (Turing 1950) was designed as a thought experiment that would sidestep the philosophical vagueness of the question â€œCan a machine think?â€ A computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer (Russel and Norvig 2022).\nThe Total Turing Test additionally requires interaction with objects and people in the real world. This also requires computer vision and robotic capabilities."
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#thinking-humanly",
    "href": "lectures/I2AI/1/slides.html#thinking-humanly",
    "title": "Introduction",
    "section": "Thinking humanly",
    "text": "Thinking humanly\nThe cognitive modelling approach\nCognitive science is the study of the human brain and its processes â€” it examines how the human brain may be functioning. Cognitive science requires analytical observation and experimentation. We can learn about human thought in three ways (Russel and Norvig 2022):\n\n\nintrospection (trying to catch our own thoughts)\nexperiments (observing a person in action)\nbrain imaging (observing the brain in action)\n\n\n\n\n\n\n\n\n\nDifferences between cognitive computing and AI\n\n\nCognitive science is about making computers solve complex problems similar to how humans solve problems. Cognitive computing tries to replicate how humans would solve problems, while AI is not bound to human cognitive processes."
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#thinking-rationally",
    "href": "lectures/I2AI/1/slides.html#thinking-rationally",
    "title": "Introduction",
    "section": "Thinking rationally",
    "text": "Thinking rationally\nThe laws of thought approach\nThe â€œlaws of thoughtâ€ refer to fundamental axiomatic rules upon which rational discorse itself is often considered to be based.\n\n\nSocrates is a man and all men are mortal, thus, it can be concluded that Socrates is mortal â€”Aristotle (384-322 BCE)\n\n\n\nIn principle, computers have been able to solve any solvable problem (i.e., make correct inferences), as long as\n\n\n\nthere are statements about any objects in the world,\nstatements about the relations among them, and\nthere is sufficient computing power available\n\n\n\n\n\n\n\n\n\nLogic, knowledge and probability\n\n\nThese laws of thought were supposed to govern the operation of the mind; the studies initiated the field called logic. However, human decisions are not always mathematically perfect or logical.\nLogic as conventionally understood requires knowledge of the world that is certain. As this condition is seldom achieved, the theory of probability fills this gap. Probability allows for rigorous reasoning with uncertain information. (Russel and Norvig 2022)"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#acting-rationally",
    "href": "lectures/I2AI/1/slides.html#acting-rationally",
    "title": "Introduction",
    "section": "Acting rationally",
    "text": "Acting rationally\nThe rational agent approach\nAn agent is something that acts, an rational agent is one that acts so as to achieve the best coutcome (i.e., does the right thing), or, when there is uncertainty, the best expected outcome (i.e., does the appropriate thing) based on the objective that is provided to the agent (Russel and Norvig 2022).\n\nThe approach goes beyond the â€œlaws of thoughtâ€ approach as it involves actions based on\n\n\n\ninference (deducing that a given action is the best and then to act on this conclusion) and\nother mechanisms such as reflex (when speed is more successful than careful deliberation that takes some time)\n\n\n\n\n\n\n\n\n\nStandard model in AI\n\n\nRussel and Norvig (2022) call the approach where the primary definition of success is getting better and better at achieving rigid human-specified goals the standard model of AI research. This standard model prevails not only in AI, â€œbut also in control theory, where a controller minimizes a cost function; in operations research, where a policy maximizes a sum of rewards; in statistics, where a decision rule minimizes a loss function; and in economics, where a decision maker maximizes utility or some measure of social welfareâ€ (p.Â 22).\nThey also criticize the standard model because it is increasingly difficult to specify the goal completely and correctly (e.g., autonomous driving involves multiple goals such as reaching the goal safely, where a strict definition of safety requires staying in the garage because driving on the road has a risk of injury due to myriad factors; how should the trade-off between reaching the goal and taking a risk of injury be made? There are so many questions that are difficult to answer a priori). Mis-specified goals most likely do not reflect what human designers intend, e.g., by not taking into account human values that are not included in the goals."
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#the-thinking-machine",
    "href": "lectures/I2AI/1/slides.html#the-thinking-machine",
    "title": "Introduction",
    "section": "The Thinking Machine",
    "text": "The Thinking Machine\nA series of interviews to some of the AI pioneers.\n\n\nThe full documentary is available here"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#a-brief-ai-timeline",
    "href": "lectures/I2AI/1/slides.html#a-brief-ai-timeline",
    "title": "Introduction",
    "section": "A brief AI-timeline",
    "text": "A brief AI-timeline\n1943â€”1956 The inception of AI\n\n1943: McCulloch & Pitts: Boolean circuit model of brain (artificial neurons with on and off states; all logical connectives can be implemented with some network of these)\n1950: Turingâ€™s â€œComputing Machinery and Intelligenceâ€ (Turing (1950) already introduced the Turing test, machine learning, genetic algorithms, and reinforcement learning)\n1950s: Early AI programs (e.g., Arthur Samuelâ€™s influential checkers program that learned to play at a strong amateur level)"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#section",
    "href": "lectures/I2AI/1/slides.html#section",
    "title": "Introduction",
    "section": "",
    "text": "1966â€”73 A dose of reality\n\nThe early AI programs failed on more difficult problems\n\nFocus on â€œinformed introspectionâ€ as to how humans perform a task\nLack of appreciation of the intractability of many of the problems\n\nSignification reduction of government funding of AI research\n\n\n1970â€”90 Expert systems (knowledge-based approaches)\n\n1969â€”79: Early development of knowledge-based systems (rule-based heuristic algorithms)\n1980â€”88: Expert systems industry booms (nearly every major U.S. corporate had its own AI group)\nSoon after that came the â€œAI winterâ€ (difficulties to build expert systems for complex domains due to uncertainty and a lack of learning)"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#section-1",
    "href": "lectures/I2AI/1/slides.html#section-1",
    "title": "Introduction",
    "section": "",
    "text": "1990â€”present AI spring: (statistical approaches)\n\nFocus on probabilistic reasoning (rather than Boolean logic) and machine learning\nReunification of subfields such as computer vision, robotics, speech recognition, and natural language processing"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#section-2",
    "href": "lectures/I2AI/1/slides.html#section-2",
    "title": "Introduction",
    "section": "",
    "text": "2012â€”present New excitement\n\nAdvances in computing power, WWW, and very large data sets\n(e.g., IBM Watsonâ€™s victory in Jeopardy!)\n\n\nImprovement in performance obtained from increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be obtained from tweaking the algorithm â€”Banko and Brill (2001)\n\n\nDeep learning systems offer significant performance gains\n(e.g., AlphaGoâ€™s victories)\nSignificant focus on AI in academia and industry\nAI systems find increasing application in the real world (e.g., robotic vehicles, machine translation, speech recognition, recommendations, autonomous planning, game playing, image understanding, medicine)"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#i2ai_1-e1",
    "href": "lectures/I2AI/1/slides.html#i2ai_1-e1",
    "title": "Introduction",
    "section": "I2AI_1 E1",
    "text": "I2AI_1 E1\nDefine in your own words:\n\nintelligence\nartificial intelligence\nagent\nrationality\nlogical reasoning"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#i2ai_1-e2",
    "href": "lectures/I2AI/1/slides.html#i2ai_1-e2",
    "title": "Introduction",
    "section": "I2AI_1 E2Â ",
    "text": "I2AI_1 E2Â \nEvery year the Loebner Prize is awarded to the program that comes closest to passing a version of the Turing Test.\nResearch and report on the latest winner of the Loebner prize.\n\nWhat techniques does it use?\nHow does it advance the state of the art in AI?"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#i2ai_1-e3",
    "href": "lectures/I2AI/1/slides.html#i2ai_1-e3",
    "title": "Introduction",
    "section": "I2AI_1 E3",
    "text": "I2AI_1 E3\nTo what extent are the following computer systems instances of artificial intelligence:\n\nSupermarket bar code scanners\nWeb search engines\nVoice-activated telephone menus\nInternet routing algorithms that respond dynamically to the state of the network"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#i2ai_1-e4",
    "href": "lectures/I2AI/1/slides.html#i2ai_1-e4",
    "title": "Introduction",
    "section": "I2AI_1 E4",
    "text": "I2AI_1 E4\nVarious subfields of AI have held contests by defining a standard task and inviting researchers to do their best. Examples include the DARPA Grand Challenge for robotic cars, the International Planning Competition, the Robocup robotic soccer league, the TREC information retrieval event, and contests in machine translation and speech recognition.\nInvestigate one of these contests and describe the progress made over the years.\n\nTo what degree have the contests advanced the state of the art in AI?\nTo what degree do they hurt the field by drawing energy away from new ideas?"
  },
  {
    "objectID": "lectures/I2AI/1/slides.html#i2ai_1-e5",
    "href": "lectures/I2AI/1/slides.html#i2ai_1-e5",
    "title": "Introduction",
    "section": "I2AI_1 E5",
    "text": "I2AI_1 E5\nRead the statements (one after the other) and discuss if the second sentence of each statement is true and if it does imply the first.\n\nâ€œSurely computers cannot be intelligentâ€”they can do only what their programmers tell them.â€ Is the latter statement true, and does it imply the former?\n\n\nâ€œSurely animals cannot be intelligentâ€”they can do only what their genes tell them.â€ Is the latter statement true, and does it imply the former?\n\n\nâ€œSurely animals, humans, and computers cannot be intelligentâ€”they can do only what their constituent atoms are told to do by the laws of physics.â€ Is the latter statement true, and does it imply the former?"
  },
  {
    "objectID": "lectures/I2AI/4/index.html#playful-superiority",
    "href": "lectures/I2AI/4/index.html#playful-superiority",
    "title": "Games",
    "section": "Playful superiority",
    "text": "Playful superiority"
  },
  {
    "objectID": "lectures/I2AI/4/index.html#adversarial-search",
    "href": "lectures/I2AI/4/index.html#adversarial-search",
    "title": "Games",
    "section": "Adversarial search",
    "text": "Adversarial search\nIn competitive environments, two or more agents have conflicting goals.\n\nThis gives rise to adversarial search problems.\n\n\nThe AI community is particularly interested in games of a simplified nature (e.g., chess, go, and poker).\n\n\n\nState of a game is easy to represent\nAgents are restricted to a few actions\nEffects of actions are defined by precise rules"
  },
  {
    "objectID": "lectures/I2AI/4/index.html#deterministic-games",
    "href": "lectures/I2AI/4/index.html#deterministic-games",
    "title": "Games",
    "section": "Deterministic games",
    "text": "Deterministic games\nThe games most commonly studied within AI are deterministic (two-player, turn-taking, fully observable, zero-sum1) (Russel and Norvig 2022, 192).\n\nPossible formalization\n\n\nStates: S (start at Sâ‚€)\nPlayer: TO-MOVE(s) (defines which player has the move in state s)\nActions: ACTIONS(s) (the set of legal moves in state s)\nTransition model: RESULT(s,a) (defines the state resulting from action a in state s)\nTerminal test: IS-TERMINAL(s) (is true when the game is over2)\nUtility function: UTILITY(s,p) (defines the final numeric value to player p when the game ends in terminal state s)\n\n\n\n\nThe initial state, ACTIONS function, and RESULT function define the state space graph, where the vertices are states, the edges are moves and the state might be reached by multiple paths."
  },
  {
    "objectID": "lectures/I2AI/4/index.html#optimal-decisions",
    "href": "lectures/I2AI/4/index.html#optimal-decisions",
    "title": "Games",
    "section": "Optimal decisions",
    "text": "Optimal decisions\nPlayers (here MIN and MAX, alternate turns) need to have a conditional planâ€”a contingent strategy specifying a response to each move of the opponent.\n\n\nFor games with binary outcomes (win or lose), AND-OR search can be used\n(see Russel and Norvig 2022, 143)\nFor games with multiple outcome scores, the minimax search algorithm is used"
  },
  {
    "objectID": "lectures/I2AI/4/index.html#minimax-value",
    "href": "lectures/I2AI/4/index.html#minimax-value",
    "title": "Games",
    "section": "Minimax value",
    "text": "Minimax value\nGiven a state-space search tree, each nodeâ€™s minimax value is calculated.\n\nThe minimax value is the best achievable utility of being in a given sate (against a rational adversary).\n\n\nMAX prefers to move to a state of maximum value\n\nMIN prefers to move to a state of minimum value"
  },
  {
    "objectID": "lectures/I2AI/4/index.html#adversarial-game-tree",
    "href": "lectures/I2AI/4/index.html#adversarial-game-tree",
    "title": "Games",
    "section": "Adversarial game tree",
    "text": "Adversarial game tree\n\n\n\nFigure 1: Example: A adversarial game tree; based on Russel and Norvig (2022, 195)\n\n\n\nThe â–³ nodes are â€œMAX nodesâ€, in which it is MAXâ€™s turn to move; the â–½ nodes are â€œMIN nodesâ€. MAXâ€™s best move at the root is Î±â‚ (highest minimax value), MINâ€™s best move is Î²â‚ (lowest minimax value)."
  },
  {
    "objectID": "lectures/I2AI/4/index.html#minimax-search-algorithm",
    "href": "lectures/I2AI/4/index.html#minimax-search-algorithm",
    "title": "Games",
    "section": "Minimax search algorithm",
    "text": "Minimax search algorithm\nThe minimax algorithm performs a complete depth-first exploration of the game tree (Russel and Norvig 2022, 196â€“96).\n\n\nAssumes that the adversary plays optimal\nReturns action whose terminal state has the optimal MINIMAX value\n\nIf the state is a terminal state (IS-TERMINAL(s) = true):\nreturn the stateâ€™s utility (UTILITY(s,p))\nIf the next agent is MAX (TO-MOVE(s) = MAX):\nreturn MAX-VALUE(s)\nIf the next agent is MIN (TO-MOVE(s) = MIN):\nreturn MIN-VALUE(s)\n\n\n\n\nThe exponential complexity makes the miminmax algorithm impractical for complex games (even with alpha-beta pruning applied; chess game tree size > atoms in the universe).\n\n\n\n\n\n\n\n\nPruning to reduce computing complexity (Russel and Norvig 2022, 198â€“99).\n\n\n\nPruning stops the search at the moment when it is determined that the value of a subtree is worse than the best solution already identified.\nThe general principle is as follows: consider a node n somewhere in the tree, such that Player has a choice of moving to n. If Player has a better choice either at the same level or at any point higher up in the tree, then Player will never move to n. So enough about n is found out (by examining some of its descendants) to reach this conclusion, it can be pruned (Russel and Norvig 2022, 198).\nAlpha-beta pruning gets its name from the two extra parameters in MAX-VALUE(state,Î±,Î²) that describe the bounds on the backed-up values that appear anywhere along the path:\n\nÎ± = the value of the best choice for MAX found so far (â€œat leastâ€)\nÎ² = the value of the bast choice for MIN found so far (â€œat mostâ€)\n\nAlpha-beta search updates the values of Î± and Î² as it goes along and prunes the remaining branches at a node as soon as the value of the current node is known to be worse than the current Î± and Î² for MAX or MIN, respectively."
  },
  {
    "objectID": "lectures/I2AI/4/index.html#selection",
    "href": "lectures/I2AI/4/index.html#selection",
    "title": "Games",
    "section": "Selection",
    "text": "Selection\n\n\n\n\n\nFigure 2: Example: MCTS â€” selection; based on Russel and Norvig (2022, 208)\n\n\n\n\nFigureÂ 2 shows a tree with the root representing a state where P-A has won 37/100 playouts done\n\n\nP-B selects a move to a node where it has won 60/79 playouts\nThis is the best win percentage among the available moves\nP-A will select a move to a node where it has won 27/35 playouts (assuming it plays optimally)\n\n\n\n\n\n\nIt would also have been reasonable to select the 2/11 node for the sake of explorationâ€”with only 11 playouts, the node still has high uncertainty in its valuation, and might end up being the best option if more information about it is gained."
  },
  {
    "objectID": "lectures/I2AI/4/index.html#selection-policy-example",
    "href": "lectures/I2AI/4/index.html#selection-policy-example",
    "title": "Games",
    "section": "Selection policy example",
    "text": "Selection policy example\nUpper confidence bounds applied to trees (UCT) is a very effective selection policy ranking possible moves based on an upper confidence bound formula (UCB1)\n\n\n\n\n is the total utility of all playouts that went through node \n is the number of playouts through node \n is the parent node of  in the tree\n is the average utility of  (exploitation term, â€œhow good are the stats?â€)\n higher for  only explored a few times\n(exploration term, â€œhow much has the child be â€˜ignoredâ€™?â€)\n is a constant that balance exploitation and exploration (theoretically )\n\n\n\n\nWith , the 60/79 node in FigureÂ 2 has the highest UCB1 score, but with , it would be the 2/11 node.\n\n\n\n\n\n\nUtiliy of MCTS\n\n\n\nThe conventional wisdom has been that Monte Carlo search has an advantage over Heuristic Alpha-Beta Tree Search (not discussed here, see (Russel and Norvig 2022, 202ff)) for games where the branching factor is very high (and thus alpha-beta canâ€™t search deep enough), or when it is difficult to define a good evaluation function 5."
  },
  {
    "objectID": "lectures/I2AI/4/index.html#expansion-and-simulation",
    "href": "lectures/I2AI/4/index.html#expansion-and-simulation",
    "title": "Games",
    "section": "Expansion and simulation",
    "text": "Expansion and simulation\n\n\n\n\n\nFigure 3: Example: MCTS â€” expansion and simulation; based on Russel and Norvig (2022, 208)\n\n\n\n\nFigureÂ 3 shows a tree where a new child of the selected node is generated and marked with 0/0 (expansion).\n\n\nA playout for the newly generated child node is performed (simulation).\n\n\nMoves for both players according the playout policy6 are chosen\nThe moves are not recorded in the search tree\nIn FigureÂ 3, the simulation results in a win for P-B"
  },
  {
    "objectID": "lectures/I2AI/4/index.html#back-propagation",
    "href": "lectures/I2AI/4/index.html#back-propagation",
    "title": "Games",
    "section": "Back-propagation",
    "text": "Back-propagation\n\n\n\n\n\nFigure 4: Example: MCTS â€” selection; based on Russel and Norvig (2022, 208)\n\n\n\n\nThe result of the simulation is used to update all the search tree nodes going up to the root.\n\n\nP-B's nodes are incremented in both the number of wins and the number of playouts\nP-A's nodes are incremented in the number of playouts only"
  },
  {
    "objectID": "lectures/I2AI/4/index.html#summary-1",
    "href": "lectures/I2AI/4/index.html#summary-1",
    "title": "Games",
    "section": "Summary",
    "text": "Summary\n\n\nIn two-player, discrete, deterministic, turn-taking zero-sum games with perfect information, the minimax algorithm can select optimal moves by a depth-first search in the game tree\nEfficiency can be improved by using the alpha-beta search algorithm, which eliminates subtrees that are shown to be irrelevant.\nMonte Carlo tree search evaluates states by playing out the game all the way to the end to see who won. This playout simulation is repeated multiple times. The evaluation is an average of the results."
  },
  {
    "objectID": "lectures/I2AI/4/index.html#concluding-remarks-xkcd",
    "href": "lectures/I2AI/4/index.html#concluding-remarks-xkcd",
    "title": "Games",
    "section": "Concluding remarks (XKCD)",
    "text": "Concluding remarks (XKCD)\n\n\n\nFigure 5: 1263 XKCD Comic: Reassuring"
  },
  {
    "objectID": "lectures/I2AI/4/index.html#i2ai_4-e1",
    "href": "lectures/I2AI/4/index.html#i2ai_4-e1",
    "title": "Games",
    "section": "I2AI_4 E1",
    "text": "I2AI_4 E1\nExplain in your own words the following terms:\n\nZero-sum\nTerminal test\nMinimax value\nSelection policy\nPlayout policy\nMonte Carlo tree\nBack-propagation"
  },
  {
    "objectID": "lectures/I2AI/4/index.html#i2ai_4-e2",
    "href": "lectures/I2AI/4/index.html#i2ai_4-e2",
    "title": "Games",
    "section": "I2AI_4 E2",
    "text": "I2AI_4 E2\nExplain if the MINIMAX algorithm is complete and optimal.\nCan it be beaten by an opponent playing suboptimally? Why (not)?\nCome up with a game tree in which MAX can do still better using a suboptimal strategy against a suboptimal MIN."
  },
  {
    "objectID": "lectures/I2AI/4/index.html#i2ai_4-e3",
    "href": "lectures/I2AI/4/index.html#i2ai_4-e3",
    "title": "Games",
    "section": "I2AI_4 E3",
    "text": "I2AI_4 E3\nRead the note about pruning (and consult Russel and Norvig (2022) if necessary).\nExplain in your own words, under what conditions a subtree is skipped using Alpha-beta pruning.\nDraw an example (game search tree, 3 levels depth)."
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#playful-superiority",
    "href": "lectures/I2AI/4/slides.html#playful-superiority",
    "title": "Games",
    "section": "Playful superiority",
    "text": "Playful superiority"
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#adversarial-search",
    "href": "lectures/I2AI/4/slides.html#adversarial-search",
    "title": "Games",
    "section": "Adversarial search",
    "text": "Adversarial search\nIn competitive environments, two or more agents have conflicting goals.\n\nThis gives rise to adversarial search problems.\n\n\nThe AI community is particularly interested in games of a simplified nature (e.g., chess, go, and poker).\n\n\n\nState of a game is easy to represent\nAgents are restricted to a few actions\nEffects of actions are defined by precise rules"
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#deterministic-games",
    "href": "lectures/I2AI/4/slides.html#deterministic-games",
    "title": "Games",
    "section": "Deterministic games",
    "text": "Deterministic games\nThe games most commonly studied within AI are deterministic (two-player, turn-taking, fully observable, zero-sum1) (Russel and Norvig 2022, 192).\n\nPossible formalization\n\n\nStates: S (start at Sâ‚€)\nPlayer: TO-MOVE(s) (defines which player has the move in state s)\nActions: ACTIONS(s) (the set of legal moves in state s)\nTransition model: RESULT(s,a) (defines the state resulting from action a in state s)\nTerminal test: IS-TERMINAL(s) (is true when the game is over2)\nUtility function: UTILITY(s,p) (defines the final numeric value to player p when the game ends in terminal state s)\n\n\n\n\nThe initial state, ACTIONS function, and RESULT function define the state space graph, where the vertices are states, the edges are moves and the state might be reached by multiple paths."
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#optimal-decisions",
    "href": "lectures/I2AI/4/slides.html#optimal-decisions",
    "title": "Games",
    "section": "Optimal decisions",
    "text": "Optimal decisions\nPlayers (here MIN and MAX, alternate turns) need to have a conditional planâ€”a contingent strategy specifying a response to each move of the opponent.\n\n\nFor games with binary outcomes (win or lose), AND-OR search can be used\n(see Russel and Norvig 2022, 143)\nFor games with multiple outcome scores, the minimax search algorithm is used"
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#minimax-value",
    "href": "lectures/I2AI/4/slides.html#minimax-value",
    "title": "Games",
    "section": "Minimax value",
    "text": "Minimax value\nGiven a state-space search tree, each nodeâ€™s minimax value is calculated.\n\nThe minimax value is the best achievable utility of being in a given sate (against a rational adversary).\n\n\nMAX prefers to move to a state of maximum value\n\nMIN prefers to move to a state of minimum value"
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#adversarial-game-tree",
    "href": "lectures/I2AI/4/slides.html#adversarial-game-tree",
    "title": "Games",
    "section": "Adversarial game tree",
    "text": "Adversarial game tree\n\n\n\nFigure 1: Example: A adversarial game tree; based on Russel and Norvig (2022, 195)\n\n\n\nThe â–³ nodes are â€œMAX nodesâ€, in which it is MAXâ€™s turn to move; the â–½ nodes are â€œMIN nodesâ€. MAXâ€™s best move at the root is Î±â‚ (highest minimax value), MINâ€™s best move is Î²â‚ (lowest minimax value)."
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#minimax-search-algorithm",
    "href": "lectures/I2AI/4/slides.html#minimax-search-algorithm",
    "title": "Games",
    "section": "Minimax search algorithm",
    "text": "Minimax search algorithm\nThe minimax algorithm performs a complete depth-first exploration of the game tree (Russel and Norvig 2022, 196â€“96).\n\n\nAssumes that the adversary plays optimal\nReturns action whose terminal state has the optimal MINIMAX value\n\nIf the state is a terminal state (IS-TERMINAL(s) = true):\nreturn the stateâ€™s utility (UTILITY(s,p))\nIf the next agent is MAX (TO-MOVE(s) = MAX):\nreturn MAX-VALUE(s)\nIf the next agent is MIN (TO-MOVE(s) = MIN):\nreturn MIN-VALUE(s)\n\n\n\n\nThe exponential complexity makes the miminmax algorithm impractical for complex games (even with alpha-beta pruning applied; chess game tree size > atoms in the universe).\n\n\n\n\n\n\n\n\nPruning to reduce computing complexity (Russel and Norvig 2022, 198â€“99).\n\n\nPruning stops the search at the moment when it is determined that the value of a subtree is worse than the best solution already identified.\nThe general principle is as follows: consider a node n somewhere in the tree, such that Player has a choice of moving to n. If Player has a better choice either at the same level or at any point higher up in the tree, then Player will never move to n. So enough about n is found out (by examining some of its descendants) to reach this conclusion, it can be pruned (Russel and Norvig 2022, 198).\nAlpha-beta pruning gets its name from the two extra parameters in MAX-VALUE(state,Î±,Î²) that describe the bounds on the backed-up values that appear anywhere along the path:\n\nÎ± = the value of the best choice for MAX found so far (â€œat leastâ€)\nÎ² = the value of the bast choice for MIN found so far (â€œat mostâ€)\n\nAlpha-beta search updates the values of Î± and Î² as it goes along and prunes the remaining branches at a node as soon as the value of the current node is known to be worse than the current Î± and Î² for MAX or MIN, respectively."
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#selection",
    "href": "lectures/I2AI/4/slides.html#selection",
    "title": "Games",
    "section": "Selection",
    "text": "Selection\n\n\n\n\n\nFigure 2: Example: MCTS â€” selection; based on Russel and Norvig (2022, 208)\n\n\n\n\nFigureÂ 2 shows a tree with the root representing a state where P-A has won 37/100 playouts done\n\n\nP-B selects a move to a node where it has won 60/79 playouts\nThis is the best win percentage among the available moves\nP-A will select a move to a node where it has won 27/35 playouts (assuming it plays optimally)\n\n\n\n\n\n\nIt would also have been reasonable to select the 2/11 node for the sake of explorationâ€”with only 11 playouts, the node still has high uncertainty in its valuation, and might end up being the best option if more information about it is gained."
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#selection-policy-example",
    "href": "lectures/I2AI/4/slides.html#selection-policy-example",
    "title": "Games",
    "section": "Selection policy example",
    "text": "Selection policy example\nUpper confidence bounds applied to trees (UCT) is a very effective selection policy ranking possible moves based on an upper confidence bound formula (UCB1)\n\n\n\n\n is the total utility of all playouts that went through node \n is the number of playouts through node \n is the parent node of  in the tree\n is the average utility of  (exploitation term, â€œhow good are the stats?â€)\n higher for  only explored a few times\n(exploration term, â€œhow much has the child be â€˜ignoredâ€™?â€)\n is a constant that balance exploitation and exploration (theoretically )\n\n\n\n\nWith , the 60/79 node in FigureÂ 2 has the highest UCB1 score, but with , it would be the 2/11 node.\n\n\n\n\n\n\nUtiliy of MCTS\n\n\nThe conventional wisdom has been that Monte Carlo search has an advantage over Heuristic Alpha-Beta Tree Search (not discussed here, see (Russel and Norvig 2022, 202ff)) for games where the branching factor is very high (and thus alpha-beta canâ€™t search deep enough), or when it is difficult to define a good evaluation function 5."
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#expansion-and-simulation",
    "href": "lectures/I2AI/4/slides.html#expansion-and-simulation",
    "title": "Games",
    "section": "Expansion and simulation",
    "text": "Expansion and simulation\n\n\n\n\n\nFigure 3: Example: MCTS â€” expansion and simulation; based on Russel and Norvig (2022, 208)\n\n\n\n\nFigureÂ 3 shows a tree where a new child of the selected node is generated and marked with 0/0 (expansion).\n\n\nA playout for the newly generated child node is performed (simulation).\n\n\nMoves for both players according the playout policy6 are chosen\nThe moves are not recorded in the search tree\nIn FigureÂ 3, the simulation results in a win for P-B"
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#back-propagation",
    "href": "lectures/I2AI/4/slides.html#back-propagation",
    "title": "Games",
    "section": "Back-propagation",
    "text": "Back-propagation\n\n\n\n\n\nFigure 4: Example: MCTS â€” selection; based on Russel and Norvig (2022, 208)\n\n\n\n\nThe result of the simulation is used to update all the search tree nodes going up to the root.\n\n\nP-B's nodes are incremented in both the number of wins and the number of playouts\nP-A's nodes are incremented in the number of playouts only"
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#summary-1",
    "href": "lectures/I2AI/4/slides.html#summary-1",
    "title": "Games",
    "section": "Summary",
    "text": "Summary\n\n\nIn two-player, discrete, deterministic, turn-taking zero-sum games with perfect information, the minimax algorithm can select optimal moves by a depth-first search in the game tree\nEfficiency can be improved by using the alpha-beta search algorithm, which eliminates subtrees that are shown to be irrelevant.\nMonte Carlo tree search evaluates states by playing out the game all the way to the end to see who won. This playout simulation is repeated multiple times. The evaluation is an average of the results."
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#concluding-remarks-xkcd",
    "href": "lectures/I2AI/4/slides.html#concluding-remarks-xkcd",
    "title": "Games",
    "section": "Concluding remarks (XKCD)",
    "text": "Concluding remarks (XKCD)\n\n\n\nFigure 5: 1263 XKCD Comic: Reassuring"
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#i2ai_4-e1",
    "href": "lectures/I2AI/4/slides.html#i2ai_4-e1",
    "title": "Games",
    "section": "I2AI_4 E1",
    "text": "I2AI_4 E1\nExplain in your own words the following terms:\n\nZero-sum\nTerminal test\nMinimax value\nSelection policy\nPlayout policy\nMonte Carlo tree\nBack-propagation"
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#i2ai_4-e2",
    "href": "lectures/I2AI/4/slides.html#i2ai_4-e2",
    "title": "Games",
    "section": "I2AI_4 E2",
    "text": "I2AI_4 E2\nExplain if the MINIMAX algorithm is complete and optimal.\nCan it be beaten by an opponent playing suboptimally? Why (not)?\nCome up with a game tree in which MAX can do still better using a suboptimal strategy against a suboptimal MIN."
  },
  {
    "objectID": "lectures/I2AI/4/slides.html#i2ai_4-e3",
    "href": "lectures/I2AI/4/slides.html#i2ai_4-e3",
    "title": "Games",
    "section": "I2AI_4 E3",
    "text": "I2AI_4 E3\nRead the note about pruning (and consult Russel and Norvig (2022) if necessary).\nExplain in your own words, under what conditions a subtree is skipped using Alpha-beta pruning.\nDraw an example (game search tree, 3 levels depth)."
  },
  {
    "objectID": "lectures/I2AI/3/index.html#applications",
    "href": "lectures/I2AI/3/index.html#applications",
    "title": "Search",
    "section": "Applications",
    "text": "Applications\n\n\n\n\n\n\n\nExamples for search problems are (Russel and Norvig 2022, 87â€“88):\n\nRoute-finding problems (e.g., car navigation, airline travel-planning)\nTouring problems (e.g., the traveling salesperson problem)\nVLSI layout problems (positioning millions of components and connections on a chip)\nRobot navigation (e.g., vacuum robots)\nAutomatic assembly sequencing of complex objects (e.g., protein design)"
  },
  {
    "objectID": "lectures/I2AI/3/index.html#agents",
    "href": "lectures/I2AI/3/index.html#agents",
    "title": "Search",
    "section": "Agents",
    "text": "Agents\nAgents that plan ahead by considering a sequence of actions that form a path to a goal state are called problem-solving agents (Russel and Norvig 2022, 81)\n\n\nThe computational process it undertakes is search\nThe representations the agents use are atomic representations\nThere are search algorithms for several environments\n\n\n\nHere only simple environments are considered (episodic, single agent, fully observable, deterministic, static, discrete, and known).\nâ€“> We assume that information about the environment are given (e.g., a map)\n\n\nThere are also search algorithms for problems in partially observable, nondeterministic, unknown, and continuous environments (i.e., complex environments) like local search methods (e.g., hill-climbing search, local beam search, evolutionary algorithms). For details please see Russel and Norvig (2022)."
  },
  {
    "objectID": "lectures/I2AI/3/index.html#process",
    "href": "lectures/I2AI/3/index.html#process",
    "title": "Search",
    "section": "Process",
    "text": "Process\nIn simple environments, agents can follow a four-phase-problem-solving process (Russel and Norvig 2022, 81â€“82):\n\n\nGoal formulation: goals organize behavior by limiting the objectives and hence the actions to be considered\nProblem formulation: the agents devices a description of the states and actions necessary to reach the goalâ€”an abstract model of the relevant part of the environment\nSearch: the agent simulates sequences of actions in its model, searching until it finds a sequence that reaches the goal (i.e., the solution)\nExecution: the agent executes the actions in the solution, one at a time\n\n\n\n\n\n\n\n\n\nProblem formulation must follow goal formulation\n\n\n\nIn goal formulation, we decide which aspects of the world we are interested in, and which can be ignored or abstracted away. Then in problem formulation we decide how to manipulate the important aspects (and ignore the others). If we did problem formulation first we would not know what to include and what to leave out.\nIt can happen that there is a cycle of iterations between goal formulation, problem formulation, and problem-solving until one arrives at a sufficiently useful and efficient solution."
  },
  {
    "objectID": "lectures/I2AI/3/index.html#definition",
    "href": "lectures/I2AI/3/index.html#definition",
    "title": "Search",
    "section": "Definition",
    "text": "Definition\nA search problem can be defined formally as (Russel and Norvig 2022, 83):\n\n\nThe state space: a set of possible states the environment can be in\nThe initial state: the state that the agent starts\nGoal states: a singe goal state, a small set of alternative goal states, or a property that applies to many states (e.g, no dirt in any location)\nThe actions available to the agent ACTIONS(s) where s is the current state\nA transition model: describes what each action does. RESULT(s,a) returns the state that results from doing action a in state s.\nAn action cost function: gives the numeric cost of applying action a in state s to reach state s' (ACTION-COST(s,a,s')\nA path: a sequence of actions\nA solution: a path from the initial state to the goal state"
  },
  {
    "objectID": "lectures/I2AI/3/index.html#modelling",
    "href": "lectures/I2AI/3/index.html#modelling",
    "title": "Search",
    "section": "Modelling",
    "text": "Modelling\n\n\n\nFigure 1: Example: A simplified map of Romania, with road distances in miles; based on Russel and Norvig (2022, 82)\n\n\n\nFigureÂ 1 depicts the search problem as model, the state space graph (Russel and Norvig 2022, 82â€“84):\n\nState space: cities (vertices, each state occurs only once)\nInitial state: Arad\nGoal state: Bucharest (goal test: Is state == Bucharest?)\nActions: directed edges between the vertices (paths)\nAction costs: numbers on the paths\n\n\n\n\n\n\n\nAbstraction\n\n\n\nThe model is an abstract mathematical description, here a simple atomic state description. The model is an abstraction as it ignores many details of the reality (e.g., weather and scenery).\nA good problem formulation has the right level of detail (i.e., an appropriate level of abstraction).\nThe choice of a good abstraction involves removing as much detail as possible while retaining validity and ensuring that the abstract actions are easy to carry out. An abstraction is valid if any abstract solution can be elaborated into a solution in the more detailed world."
  },
  {
    "objectID": "lectures/I2AI/3/index.html#structure",
    "href": "lectures/I2AI/3/index.html#structure",
    "title": "Search",
    "section": "Structure",
    "text": "Structure\n\n\n\nFigure 2: Example: A partial search tree for finding a route from Arad to Bucharest; based on Russel and Norvig (2022, 89)\n\n\n\nFigureÂ 2 visualizes the first few steps in finding a path from Arad (initial state) to Bucharest (goal).\n\nThe root node of the search tree is the initial state s (Arad)\nThe available ACTIONS(s), using the RESULTS(s,a) function are considered\nNew nodes, called child nodes or successor nodes are generated for each resulting states (i.e., the root node is expanded)\nEach child node has Arad as its parent node\nThe child node to be considered next needs to be selected (strategies see below)\n\nIn FigureÂ 2, nodes that have been expanded are white with bold letters; nodes on the frontier that have been generated but not yet expanded are in white and regular letters; the set of states corresponding to these two types of nodes are said to have been reached. Nodes that could be generated next are shown in faint dashed lines.\n\n\n\n\n\n\nDefinition of search trees\n\n\n\nA search tree is a â€œwhat ifâ€ tree of plans and their outcomes.\n\nThe start state is the root node,\nchildren correspond to successors,\nnodes show states, but correspond to PLANS that achieve those states\n\nThere are lots of repeated structure in the search tree. Thus, for most problems, the whole tree can never be actually built. In practice, both state space graphs and search trees are constructed on demand and as little as possible (Russel and Norvig 2022)."
  },
  {
    "objectID": "lectures/I2AI/3/index.html#algorithms",
    "href": "lectures/I2AI/3/index.html#algorithms",
    "title": "Search",
    "section": "Algorithms",
    "text": "Algorithms\nSearch algorithms take a search problem as input and return a solution, or an indication of failure (Russel and Norvig 2022, 89).\n\n\nThey superimpose a search tree over the state-space graph,\nform various paths from the initial state, and\ntry to find a path that reaches a goal state.\n\n\n\nThey can implement\n\n\n\nuninformed search methods, which only have access to the problem definition\ninformed search methods, which have access to a heuristic function that estimates the costs of a solution (e.g., straight-line distance in route-finding problems)"
  },
  {
    "objectID": "lectures/I2AI/3/index.html#uniformed-search",
    "href": "lectures/I2AI/3/index.html#uniformed-search",
    "title": "Search",
    "section": "Uniformed search",
    "text": "Uniformed search\n\n\nBreadth-first search: expands the shallowest nodes first\n\ncomplete, optimal for unit action costs\nexponential space complexity\nFIFO queue\n\nUniform-cost search: expands the node with the lowest path costs\n\ncomplete, optimal for general action costs\npriority queue using cumulative cost\n\nDepth-first search: expands the deepest unexpanded node first\n\nneither complete nor optimal\nlinear space complexity, a depth bound can be added\nLIFO queue\n\n\n\n\n\n\n\n\n\n\nQues used in search algorithms\n\n\n\nThree types of ques are used in search algorithms (Russel and Norvig 2022, 92):\n\nA priority queue first pops the node with the minimum costs according to some evaluation function, f.\nA LIFO queue or first-in-first-out queue first pops the node that was added to the queue first.\nA LIFO queue or last-in-first-out queue (also known as a stack) pops first the most recently added node.\n\n\n\n\n\n\n\n\n\nMeasuring problem-solving performance\n\n\n\nThe performance of problem-solving algorithms can be evaluated in four ways (Russel and Norvig 2022, 93):\n\nCompleteness: Is the algorithm guaranteed to find a solution when there is one, and to correctly report failure when there is not?\nCost optimality: Does it find a solution with the lowest path cost of all solutions?\nTime complexity: How long does it take to find a solution (e.g., measured in seconds or by the number of states and actions considered)?\nSpace complexity: How much memory is needed to perform the search?"
  },
  {
    "objectID": "lectures/I2AI/3/index.html#online-search",
    "href": "lectures/I2AI/3/index.html#online-search",
    "title": "Search",
    "section": "Online search",
    "text": "Online search\nThe agents considered so far use offline search algorithm. They compute a complete solution before taking their first action.\nOnline search agents interleaves computation and action:\n\nTakes action,\nobserves the environment, and\ncomputes the next action\n\nThese agents can discover successor only for a state that is occupied or that is learned (i.e., contained in a map created online)\nOnline search is a good idea in dynamic or semi-dynamic environments."
  },
  {
    "objectID": "lectures/I2AI/3/index.html#i2ai_3-e1",
    "href": "lectures/I2AI/3/index.html#i2ai_3-e1",
    "title": "Search",
    "section": "I2AI_3 E1",
    "text": "I2AI_3 E1\nExplain why problem formulation must follow goal formulation."
  },
  {
    "objectID": "lectures/I2AI/3/index.html#i2ai_3-e2",
    "href": "lectures/I2AI/3/index.html#i2ai_3-e2",
    "title": "Search",
    "section": "I2AI_3 E2",
    "text": "I2AI_3 E2\nGive a complete problem formulation for each of the following problems. Choose a formulation that is precise enough to be implemented.\n\nThere are six glass boxes in a row, each with a lock. Each of the first five boxes holds a key unlocking the next box in line; the last box holds a banana. You have the key to the first box, and you want the banana.\nThere is an n x n grid of squares, each square initially being either unpainted floor or a bottomless pit. You start standing on an unpainted floor square, and can either paint the square under you or move into an adjacent unpainted floor square. You want the whole floor painted."
  },
  {
    "objectID": "lectures/I2AI/3/index.html#i2ai_3-e3",
    "href": "lectures/I2AI/3/index.html#i2ai_3-e3",
    "title": "Search",
    "section": "I2AI_3 E3",
    "text": "I2AI_3 E3\nYour goal is to navigate a robot out of a maze. The robot starts in the center of the maze facing north. You can turn the robot to face north, east, south, or west. You can direct the robot to move forward a certain distance, although it will stop before hitting a wall.\n\nFormulate this problem. How large is the state space?\nIn navigating a maze, the only place we need to turn is at the intersection of two or more corridors. Reformulate this problem using this observation. How large is the state space now?\nFrom each point in the maze, we can move in any of the four directions until we reach a turning point, and this is the only action we need to do. Reformulate the problem using these actions. Do we need to keep track of the robotâ€™s orientation now?\nIn our initial description of the problem we already abstracted from the real world. Name three such simplifications we made."
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#applications",
    "href": "lectures/I2AI/3/slides.html#applications",
    "title": "Search",
    "section": "Applications",
    "text": "Applications\n\n\n\n\n\n\n\nExamples for search problems are (Russel and Norvig 2022, 87â€“88):\n\nRoute-finding problems (e.g., car navigation, airline travel-planning)\nTouring problems (e.g., the traveling salesperson problem)\nVLSI layout problems (positioning millions of components and connections on a chip)\nRobot navigation (e.g., vacuum robots)\nAutomatic assembly sequencing of complex objects (e.g., protein design)"
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#agents",
    "href": "lectures/I2AI/3/slides.html#agents",
    "title": "Search",
    "section": "Agents",
    "text": "Agents\nAgents that plan ahead by considering a sequence of actions that form a path to a goal state are called problem-solving agents (Russel and Norvig 2022, 81)\n\n\nThe computational process it undertakes is search\nThe representations the agents use are atomic representations\nThere are search algorithms for several environments\n\n\n\nHere only simple environments are considered (episodic, single agent, fully observable, deterministic, static, discrete, and known).\nâ€“> We assume that information about the environment are given (e.g., a map)\n\n\nThere are also search algorithms for problems in partially observable, nondeterministic, unknown, and continuous environments (i.e., complex environments) like local search methods (e.g., hill-climbing search, local beam search, evolutionary algorithms). For details please see Russel and Norvig (2022)."
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#process",
    "href": "lectures/I2AI/3/slides.html#process",
    "title": "Search",
    "section": "Process",
    "text": "Process\nIn simple environments, agents can follow a four-phase-problem-solving process (Russel and Norvig 2022, 81â€“82):\n\n\nGoal formulation: goals organize behavior by limiting the objectives and hence the actions to be considered\nProblem formulation: the agents devices a description of the states and actions necessary to reach the goalâ€”an abstract model of the relevant part of the environment\nSearch: the agent simulates sequences of actions in its model, searching until it finds a sequence that reaches the goal (i.e., the solution)\nExecution: the agent executes the actions in the solution, one at a time\n\n\n\n\n\n\n\n\n\nProblem formulation must follow goal formulation\n\n\nIn goal formulation, we decide which aspects of the world we are interested in, and which can be ignored or abstracted away. Then in problem formulation we decide how to manipulate the important aspects (and ignore the others). If we did problem formulation first we would not know what to include and what to leave out.\nIt can happen that there is a cycle of iterations between goal formulation, problem formulation, and problem-solving until one arrives at a sufficiently useful and efficient solution."
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#definition",
    "href": "lectures/I2AI/3/slides.html#definition",
    "title": "Search",
    "section": "Definition",
    "text": "Definition\nA search problem can be defined formally as (Russel and Norvig 2022, 83):\n\n\nThe state space: a set of possible states the environment can be in\nThe initial state: the state that the agent starts\nGoal states: a singe goal state, a small set of alternative goal states, or a property that applies to many states (e.g, no dirt in any location)\nThe actions available to the agent ACTIONS(s) where s is the current state\nA transition model: describes what each action does. RESULT(s,a) returns the state that results from doing action a in state s.\nAn action cost function: gives the numeric cost of applying action a in state s to reach state s' (ACTION-COST(s,a,s')\nA path: a sequence of actions\nA solution: a path from the initial state to the goal state"
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#modelling",
    "href": "lectures/I2AI/3/slides.html#modelling",
    "title": "Search",
    "section": "Modelling",
    "text": "Modelling\n\n\n\nFigure 1: Example: A simplified map of Romania, with road distances in miles; based on Russel and Norvig (2022, 82)\n\n\n\nFigureÂ 1 depicts the search problem as model, the state space graph (Russel and Norvig 2022, 82â€“84):\n\nState space: cities (vertices, each state occurs only once)\nInitial state: Arad\nGoal state: Bucharest (goal test: Is state == Bucharest?)\nActions: directed edges between the vertices (paths)\nAction costs: numbers on the paths\n\n\n\n\n\n\n\nAbstraction\n\n\nThe model is an abstract mathematical description, here a simple atomic state description. The model is an abstraction as it ignores many details of the reality (e.g., weather and scenery).\nA good problem formulation has the right level of detail (i.e., an appropriate level of abstraction).\nThe choice of a good abstraction involves removing as much detail as possible while retaining validity and ensuring that the abstract actions are easy to carry out. An abstraction is valid if any abstract solution can be elaborated into a solution in the more detailed world."
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#structure",
    "href": "lectures/I2AI/3/slides.html#structure",
    "title": "Search",
    "section": "Structure",
    "text": "Structure\n\n\n\nFigure 2: Example: A partial search tree for finding a route from Arad to Bucharest; based on Russel and Norvig (2022, 89)\n\n\n\nFigureÂ 2 visualizes the first few steps in finding a path from Arad (initial state) to Bucharest (goal).\n\nThe root node of the search tree is the initial state s (Arad)\nThe available ACTIONS(s), using the RESULTS(s,a) function are considered\nNew nodes, called child nodes or successor nodes are generated for each resulting states (i.e., the root node is expanded)\nEach child node has Arad as its parent node\nThe child node to be considered next needs to be selected (strategies see below)\n\nIn FigureÂ 2, nodes that have been expanded are white with bold letters; nodes on the frontier that have been generated but not yet expanded are in white and regular letters; the set of states corresponding to these two types of nodes are said to have been reached. Nodes that could be generated next are shown in faint dashed lines.\n\n\n\n\n\n\nDefinition of search trees\n\n\nA search tree is a â€œwhat ifâ€ tree of plans and their outcomes.\n\nThe start state is the root node,\nchildren correspond to successors,\nnodes show states, but correspond to PLANS that achieve those states\n\nThere are lots of repeated structure in the search tree. Thus, for most problems, the whole tree can never be actually built. In practice, both state space graphs and search trees are constructed on demand and as little as possible (Russel and Norvig 2022)."
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#algorithms",
    "href": "lectures/I2AI/3/slides.html#algorithms",
    "title": "Search",
    "section": "Algorithms",
    "text": "Algorithms\nSearch algorithms take a search problem as input and return a solution, or an indication of failure (Russel and Norvig 2022, 89).\n\n\nThey superimpose a search tree over the state-space graph,\nform various paths from the initial state, and\ntry to find a path that reaches a goal state.\n\n\n\nThey can implement\n\n\n\nuninformed search methods, which only have access to the problem definition\ninformed search methods, which have access to a heuristic function that estimates the costs of a solution (e.g., straight-line distance in route-finding problems)"
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#uniformed-search",
    "href": "lectures/I2AI/3/slides.html#uniformed-search",
    "title": "Search",
    "section": "Uniformed search",
    "text": "Uniformed search\n\n\nBreadth-first search: expands the shallowest nodes first\n\ncomplete, optimal for unit action costs\nexponential space complexity\nFIFO queue\n\nUniform-cost search: expands the node with the lowest path costs\n\ncomplete, optimal for general action costs\npriority queue using cumulative cost\n\nDepth-first search: expands the deepest unexpanded node first\n\nneither complete nor optimal\nlinear space complexity, a depth bound can be added\nLIFO queue\n\n\n\n\n\n\n\n\n\n\nQues used in search algorithms\n\n\nThree types of ques are used in search algorithms (Russel and Norvig 2022, 92):\n\nA priority queue first pops the node with the minimum costs according to some evaluation function, f.\nA LIFO queue or first-in-first-out queue first pops the node that was added to the queue first.\nA LIFO queue or last-in-first-out queue (also known as a stack) pops first the most recently added node.\n\n\n\n\n\n\n\n\n\n\nMeasuring problem-solving performance\n\n\nThe performance of problem-solving algorithms can be evaluated in four ways (Russel and Norvig 2022, 93):\n\nCompleteness: Is the algorithm guaranteed to find a solution when there is one, and to correctly report failure when there is not?\nCost optimality: Does it find a solution with the lowest path cost of all solutions?\nTime complexity: How long does it take to find a solution (e.g., measured in seconds or by the number of states and actions considered)?\nSpace complexity: How much memory is needed to perform the search?"
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#online-search",
    "href": "lectures/I2AI/3/slides.html#online-search",
    "title": "Search",
    "section": "Online search",
    "text": "Online search\nThe agents considered so far use offline search algorithm. They compute a complete solution before taking their first action.\nOnline search agents interleaves computation and action:\n\nTakes action,\nobserves the environment, and\ncomputes the next action\n\nThese agents can discover successor only for a state that is occupied or that is learned (i.e., contained in a map created online)\nOnline search is a good idea in dynamic or semi-dynamic environments."
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#i2ai_3-e1",
    "href": "lectures/I2AI/3/slides.html#i2ai_3-e1",
    "title": "Search",
    "section": "I2AI_3 E1",
    "text": "I2AI_3 E1\nExplain why problem formulation must follow goal formulation."
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#i2ai_3-e2",
    "href": "lectures/I2AI/3/slides.html#i2ai_3-e2",
    "title": "Search",
    "section": "I2AI_3 E2",
    "text": "I2AI_3 E2\nGive a complete problem formulation for each of the following problems. Choose a formulation that is precise enough to be implemented.\n\nThere are six glass boxes in a row, each with a lock. Each of the first five boxes holds a key unlocking the next box in line; the last box holds a banana. You have the key to the first box, and you want the banana.\nThere is an n x n grid of squares, each square initially being either unpainted floor or a bottomless pit. You start standing on an unpainted floor square, and can either paint the square under you or move into an adjacent unpainted floor square. You want the whole floor painted."
  },
  {
    "objectID": "lectures/I2AI/3/slides.html#i2ai_3-e3",
    "href": "lectures/I2AI/3/slides.html#i2ai_3-e3",
    "title": "Search",
    "section": "I2AI_3 E3",
    "text": "I2AI_3 E3\nYour goal is to navigate a robot out of a maze. The robot starts in the center of the maze facing north. You can turn the robot to face north, east, south, or west. You can direct the robot to move forward a certain distance, although it will stop before hitting a wall.\n\nFormulate this problem. How large is the state space?\nIn navigating a maze, the only place we need to turn is at the intersection of two or more corridors. Reformulate this problem using this observation. How large is the state space now?\nFrom each point in the maze, we can move in any of the four directions until we reach a turning point, and this is the only action we need to do. Reformulate the problem using these actions. Do we need to keep track of the robotâ€™s orientation now?\nIn our initial description of the problem we already abstracted from the real world. Name three such simplifications we made."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#agent",
    "href": "lectures/I2AI/2/index.html#agent",
    "title": "Intelligent agents",
    "section": "Agent",
    "text": "Agent\n\n\n\nFigure 1: Agents interact with environments through sensors and actuators\n\n\n\nAn agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\n\nThe term percept refers to the content an agentâ€™s sensors are perceiving\nAn agentâ€™s percept sequence is the complete history of everything the agent has ever perceived\nThe agent function maps any given percept sequence to an action (an abstract mathematical description)\nThe agent function for an AI agent will be implemented by an agent program (a concrete implementation, running within some physical system)\n\n\n\n\n\n\n\nExample\n\n\n\nTo illustrate these ideas Russel and Norvig (2022, 55) use a simple exampleâ€”the vacuum-cleaner world, which consists of a robotic vacuum-cleaning agent in a world consisting of squares that can be either dirty or clean. The vacuum agent perceives which square it is in and whether there is dirt in the square. The agent starts in square A. The available actions are to move to the right, move to the left, suck up the dirt, or do nothing. One very simple agent function is the following: if the current square is dirty, then suck; otherwise, move to the other square.\n\n\nAccording to Russel and Norvig (2022, 56), all areas of engineering can be seen as designing artifacts that interact with the world. AI operates at the most interesting end to the spectrum, where the artifacts have significant computational resources and the task environment requires nontrivial decision making."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#rational-agent",
    "href": "lectures/I2AI/2/index.html#rational-agent",
    "title": "Intelligent agents",
    "section": "Rational agent",
    "text": "Rational agent\nA rational agent is one that does the right thing.\n\n\nFor each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has (Russel and Norvig 2022, 58).\n\n\n\nWhat is rational at any given time depends on four things:\n\nThe performance measure that defines the criterion of success\nThe agentâ€™s prior knowledge of the environment\nThe actions that the agent can performance\nThe agentâ€™s percept sequence to date\n\n\n\n\n\n\n\nExample\n\n\n\nUnder following circumstances, the vacuum cleaning agent is rational:\n\nThe performance measure of the vacuum cleaner might award one point for each clean square at each time step, over a â€œlifetimeâ€ of 1,000 time steps (to prevent the cleaner to oscillate needlessly back and forth)\nThe â€œgeographyâ€ of the environment is known a priori but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking cleans the current square. The Right and Left actions move the agent one square except when this would take the agent outside the environment in which case the agent remains where it is\nThe only available action is Right, Left, and Suck\nThe agent correctly perceives its location and wether that location contains dirt\n\n(For details such as tabulated agent functions please see Russel and Norvig (2022))\n\n\n\n\nIt can be quite hard to formulate a performance measure correctly, however:\n\nIf we use, to achieve our purposes, a mechanical agency with those operation we cannot interfere once we have started it [â€¦] we had better be quite sure that the purpose built into the machine is the purpose which we really desire (Wiener 1960, 1358)"
  },
  {
    "objectID": "lectures/I2AI/2/index.html#rationality",
    "href": "lectures/I2AI/2/index.html#rationality",
    "title": "Intelligent agents",
    "section": "Rationality",
    "text": "Rationality\nRationality is not the same as perfection.\n\n\nRationality maximizes expected performance\nPerfection maximizes actual performance\nPerfection requires omniscience\nRational choice depends only on the percept sequence to date\n\n\n\nAs the environment is usually not completely known a priori and completely predictable (or stable), information gathering and learning are important parts of rationality (Russel and Norvig 2022, 59).\n\n\n\n\n\n\nExample\n\n\n\nThe vacuum cleaner needs to explore an initially unknown environment (i.e., exploration) to maximize its expected performance. In addition, a vacuum cleaner that learns to predict where and when additional dirt will appear will do better than one that does not."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#components",
    "href": "lectures/I2AI/2/index.html#components",
    "title": "Intelligent agents",
    "section": "Components",
    "text": "Components\nBefore designing an agent (the solution), the task environment (the problem) must be specified as fully as possible, including\n\n\nthe performance measure (P),\nthe environment (E),\nthe actuators (A), and\nthe sensors (S)\n\n\n\nRussel and Norvig (2022) call the task environment PEAS.\n\n\n\n\n\n\n\n\nExample of an PEAS description\n\n\n\nTask environment of a taxi driver agent\n\nP: Safe, fast, legal, comfortable, maximize profits, minimize impact on other road users\nE: Roads, other road users, police, pedestrians, customers, weather\nA: Steering, accelerator, brake, signal horn, display, speech\nS: Cameras, radar, speedometer, GPS, engine, sensors, accelerometer, microphones, touchscreen\n\nSource: Russel and Norvig (2022, 61)"
  },
  {
    "objectID": "lectures/I2AI/2/index.html#properties",
    "href": "lectures/I2AI/2/index.html#properties",
    "title": "Intelligent agents",
    "section": "Properties",
    "text": "Properties\nTask environments can be categorized along following dimensions (Russel and Norvig 2022, 62â€“64):\n\n\nFully observable vs. partially observable\nSingle agent vs. multi-agent\nDeterministic vs. nondeterministic\nEpisodic vs. sequential\nStatic vs. dynamic\nDiscrete vs. continuous\nKnown vs. unknown\n\n\n\nIf an agentâ€™s sensors give it access to the full state of the environment at any point in time, then we say that the task environment is fully observable (e.g., image analysis).\nWhen multiple agents intend to maximize a performance measure that depends on the behavior of other agents, we say the environment is multi-agent (e.g., chess).\nWhen the environment is completely determined by the current state and the actions performed by the agent(s), it is called a deterministic environment (e.g., crossword puzzle). When a model of the environment explicitly uses probabilities, it is called a stochastic environment (e.g., poker).\nIf an agentâ€™s experience is divided into atomic episodes in which the agent receives a perception and then performs a single action, and if the next episode does not depend on the actions performed in the previous episodes, then we say that the task environment is episodic (e.g., image analysis).\nIf the environment changes while an agent is deliberating, then the environment is dynamic (e.g., taxi driving).\nIf the environment has a finite number of different states, we speak of discrete environments (e.g., chess).\nIf the outcomes (or outcome probabilities) for all actions are given, then the environment is known (e.g., solitaire card game).\nSource: Russel and Norvig (2022), p.62-64\n\n\nThe hardest case is partially observable, multi-agent, nondeterministic, sequential, dynamic, and continuous."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#simple-reflex-agents",
    "href": "lectures/I2AI/2/index.html#simple-reflex-agents",
    "title": "Intelligent agents",
    "section": "Simple reflex agents",
    "text": "Simple reflex agents\n\n\n\nFigure 2: A simple reflex agent\n\n\n\n\nRectangles are used to denote the current internal state of the agentâ€™s decision process, rectangles with rounded corners to represent the background information used in the process.\n\nSimple reflex agents select actions on the basis of the current percept, ignoring the rest of the percept history. Thus, these agents work only if the correct decision can be made on the basis of just the current percept. The environment needs to be fully observable (Russel and Norvig 2022, 68)."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#model-based-reflex-agents",
    "href": "lectures/I2AI/2/index.html#model-based-reflex-agents",
    "title": "Intelligent agents",
    "section": "Model-based reflex agents",
    "text": "Model-based reflex agents\n\n\n\nFigure 3: A model-based reflex agent\n\n\n\nModel-based reflex agents use transition models and sensor models to keep track of the state of the world as perceived by the sensors (i.e., internal state). (Russel and Norvig 2022, 70).\nThe transition model reflects â€œhow the world works,â€ i.e., how the world evolves (a) independently of the agent and (b) depending on the agentâ€™s actions.\nThe sensor model reflects how the state of the world is reflected in the agentâ€™s percepts (i.e., by its sensors).\n\n\n\n\n\n\nTypes of representation of states and the transitions between them\n\n\n\nThe representations of states can be placed along an axis of increasing complexity and expressive power (Russel and Norvig 2022, 76â€“77):\n\nAtomic representation: a state is a blackbox with no internal structure (A â€“> B)\nFactored representation: a state consists of a vector of attribute values; values can be Boolean, real-valued, or one of a fixed set of symbols; factored states can share some attributes and not others, which makes it easier to identify transitions between states\nStructured representation: a state includes objects, each of which may have attributes of its own as well as relationships to other objects; structured representations underlie relational databases and first-order logic, first-oder probability models, and much of natural language understanding.\n\nThe more expressive language is much more concise, but makes reasoning and learning more complex."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#goal-based-agents",
    "href": "lectures/I2AI/2/index.html#goal-based-agents",
    "title": "Intelligent agents",
    "section": "Goal-based agents",
    "text": "Goal-based agents\n\n\n\nFigure 4: A model-based, goal-based agent\n\n\n\nA model-based, goal-based agent keeps track of the world state as well as a set of goals it is trying to achieve. Such an agent chooses an action that will (eventually) lead to the achievement of its goals (Russel and Norvig 2022, 72)."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#utility-based-agents",
    "href": "lectures/I2AI/2/index.html#utility-based-agents",
    "title": "Intelligent agents",
    "section": "Utility-based agents",
    "text": "Utility-based agents\n\n\n\nFigure 5: A model-based, utility-based agent\n\n\n\nA model-based, utility-based agent uses a model of the world, along with a utility function that measures its preferences among states of the world. It chooses the action that leads to the best expected utility, where expected utility is computed by averaging over all possible states, weighted by the probability of the outcome (Russel and Norvig 2022, 73).\nThe utility function is essentially an internalization of the performance measure.\nA utility-based agent has many advantages in terms of flexibility and learning, which are particularly helpful in environments characterized by partial observability and nondeterminism.\nIn addition, there are cases where the goals are insufficient but a utility-based agent can still make rational decisions based on the probabilities and the utilities of the outcomes:\n\nWhen there are conflicting goals, the utility function specifies the appropriate tradeoff.\nLikelihood of success (i.e., goal achievement) can be weighed against the importance of the goals\n\nMode- and utility-based agents are difficult to implement. They need to model and keep track of the task environment, which requires ingenious sensors, sophisticated algorithms, and a high computational complexity.\nThere are also utility-based agents that are not model-based. These agents just learn what action is best in a particular situation without any â€œunderstandingâ€ of its impact on the environment (e.g., based on reinforcement learning)."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#learning-agents",
    "href": "lectures/I2AI/2/index.html#learning-agents",
    "title": "Intelligent agents",
    "section": "Learning agents",
    "text": "Learning agents\n\n\n\nFigure 6: A learning agent\n\n\n\nEach type of agent can be either hand-programmed or created as a learning agent. The behavior of learning agents is (also) determined by their own experience, while the behavior of hand-programmed agents is solely determined by their initial programming. Thus, learning agents have greater autonony.\nA learning agent consists of four conceptual components (Russel and Norvig 2022, p- 74-75), as shown in FigureÂ 6:\n\nThe performance element is responsible for taking percepts and selecting actions (i.e., what has previously been considered the entire agent program).\nThe learning element is responsible for improvements. It uses feedback from the critic on how the agent is doing and determines how the performance element should be modified to do better in the future. It can make changes to any of the â€œknowledge componentsâ€ shown in the agent diagrams (i.e., condition-action-rules, transition model, sensor model)\nThe performance standard is responsible to inform the agent about the meaning of percepts â€” are they good nor not (e.g., the meaning of receiving no tips from passengers is a negative contribution to an automated taxisâ€™s overall performance). The standard is fixed and cannot be influenced by the agent.\nThe problem generator is responsible for suggesting actions that lead to new and informative experiences. The problem generator suggests exploratory actions that may be suboptimal in the short term, but can lead to the discovery of better actions in the long term."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e1",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e1",
    "title": "Intelligent agents",
    "section": "I2AI_2 E1",
    "text": "I2AI_2 E1\nDefine in your own words the following terms:\n\nAgent\nEnvironment\nSensor\nActuator\nPercept\nAgent function\nAgent program"
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e2",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e2",
    "title": "Intelligent agents",
    "section": "I2AI_2 E2",
    "text": "I2AI_2 E2\nFor each of the following agents, specify the sensors, actuators, and environment:\n\nMicrowave oven\nChess program\nAutonomous supply delivery"
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e3",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e3",
    "title": "Intelligent agents",
    "section": "I2AI_2 E3",
    "text": "I2AI_2 E3\nDescribe a task environments in which the performance measure is easy to specify completely and correctly, and a in which it is not."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e4",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e4",
    "title": "Intelligent agents",
    "section": "I2AI_2 E4",
    "text": "I2AI_2 E4\nFor each of the following assertions, say whether it is true or false and support your answer with examples or counterexamples where appropriate.\n\nAn agent that senses only partial information about the state cannot be perfectly rational.\nThere exist task environments in which no pure reflex agent can behave rationally.\nThere exists a task environment in which every agent is rational.\nThe input to an agent program is the same as the input to the agent function.\nEvery agent is rational in an unobservable environment.\nThere is a model-based reflex agent that can remember all of its percepts.\nSuppose agent A1 is rational and agent A2 is irrational. There exists a task environment where A2â€™s actual score will be greater than A1â€™s actual score."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e5",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e5",
    "title": "Intelligent agents",
    "section": "I2AI_2 E5",
    "text": "I2AI_2 E5\nFor each of the following activities, give a PEAS description of the task environment and characterize it in terms of the properties discussed in class.\n\nPlaying soccer.\nExploring the subsurface oceans of Titan.\nShopping for used AI books on the Internet.\nPlaying a tennis match."
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e6",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e6",
    "title": "Intelligent agents",
    "section": "I2AI_2 E6",
    "text": "I2AI_2 E6\nFor each of the following task environment properties, rank the example task environments from most to least according to how well the environment satisfies the property.\nLay out any assumptions you make to reach your conclusions.\n\nFully observable: driving; document classification; tutoring a student in calculus; skin cancer diagnosis from images\nContinuous: driving; spoken conversation; written conversation; climate engineering by stratospheric aerosol injection\nStochastic: driving; sudoku; poker; soccer\nStatic: chat room; checkers; tax planning; tennis"
  },
  {
    "objectID": "lectures/I2AI/2/index.html#i2ai_2-e7",
    "href": "lectures/I2AI/2/index.html#i2ai_2-e7",
    "title": "Intelligent agents",
    "section": "I2AI_2 E7",
    "text": "I2AI_2 E7\nDefine in your own words the following terms\n\nRationality\nAutonomy\nReflex agent,\nModel-based agent\nGoal-based agent\nUtility-based agent\nLearning agent"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#agent",
    "href": "lectures/I2AI/2/slides.html#agent",
    "title": "Intelligent agents",
    "section": "Agent",
    "text": "Agent\n\n\n\nFigure 1: Agents interact with environments through sensors and actuators\n\n\n\nAn agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\n\nThe term percept refers to the content an agentâ€™s sensors are perceiving\nAn agentâ€™s percept sequence is the complete history of everything the agent has ever perceived\nThe agent function maps any given percept sequence to an action (an abstract mathematical description)\nThe agent function for an AI agent will be implemented by an agent program (a concrete implementation, running within some physical system)\n\n\n\n\n\n\n\nExample\n\n\nTo illustrate these ideas Russel and Norvig (2022, 55) use a simple exampleâ€”the vacuum-cleaner world, which consists of a robotic vacuum-cleaning agent in a world consisting of squares that can be either dirty or clean. The vacuum agent perceives which square it is in and whether there is dirt in the square. The agent starts in square A. The available actions are to move to the right, move to the left, suck up the dirt, or do nothing. One very simple agent function is the following: if the current square is dirty, then suck; otherwise, move to the other square.\n\n\n\nAccording to Russel and Norvig (2022, 56), all areas of engineering can be seen as designing artifacts that interact with the world. AI operates at the most interesting end to the spectrum, where the artifacts have significant computational resources and the task environment requires nontrivial decision making."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#rational-agent",
    "href": "lectures/I2AI/2/slides.html#rational-agent",
    "title": "Intelligent agents",
    "section": "Rational agent",
    "text": "Rational agent\nA rational agent is one that does the right thing.\n\n\nFor each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has (Russel and Norvig 2022, 58).\n\n\n\nWhat is rational at any given time depends on four things:\n\nThe performance measure that defines the criterion of success\nThe agentâ€™s prior knowledge of the environment\nThe actions that the agent can performance\nThe agentâ€™s percept sequence to date\n\n\n\n\n\n\n\nExample\n\n\nUnder following circumstances, the vacuum cleaning agent is rational:\n\nThe performance measure of the vacuum cleaner might award one point for each clean square at each time step, over a â€œlifetimeâ€ of 1,000 time steps (to prevent the cleaner to oscillate needlessly back and forth)\nThe â€œgeographyâ€ of the environment is known a priori but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking cleans the current square. The Right and Left actions move the agent one square except when this would take the agent outside the environment in which case the agent remains where it is\nThe only available action is Right, Left, and Suck\nThe agent correctly perceives its location and wether that location contains dirt\n\n(For details such as tabulated agent functions please see Russel and Norvig (2022))\n\n\n\n\n\nIt can be quite hard to formulate a performance measure correctly, however:\n\nIf we use, to achieve our purposes, a mechanical agency with those operation we cannot interfere once we have started it [â€¦] we had better be quite sure that the purpose built into the machine is the purpose which we really desire (Wiener 1960, 1358)"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#rationality",
    "href": "lectures/I2AI/2/slides.html#rationality",
    "title": "Intelligent agents",
    "section": "Rationality",
    "text": "Rationality\nRationality is not the same as perfection.\n\n\nRationality maximizes expected performance\nPerfection maximizes actual performance\nPerfection requires omniscience\nRational choice depends only on the percept sequence to date\n\n\n\nAs the environment is usually not completely known a priori and completely predictable (or stable), information gathering and learning are important parts of rationality (Russel and Norvig 2022, 59).\n\n\n\n\n\n\nExample\n\n\nThe vacuum cleaner needs to explore an initially unknown environment (i.e., exploration) to maximize its expected performance. In addition, a vacuum cleaner that learns to predict where and when additional dirt will appear will do better than one that does not."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#components",
    "href": "lectures/I2AI/2/slides.html#components",
    "title": "Intelligent agents",
    "section": "Components",
    "text": "Components\nBefore designing an agent (the solution), the task environment (the problem) must be specified as fully as possible, including\n\n\nthe performance measure (P),\nthe environment (E),\nthe actuators (A), and\nthe sensors (S)\n\n\n\nRussel and Norvig (2022) call the task environment PEAS.\n\n\n\n\n\n\n\n\nExample of an PEAS description\n\n\nTask environment of a taxi driver agent\n\nP: Safe, fast, legal, comfortable, maximize profits, minimize impact on other road users\nE: Roads, other road users, police, pedestrians, customers, weather\nA: Steering, accelerator, brake, signal horn, display, speech\nS: Cameras, radar, speedometer, GPS, engine, sensors, accelerometer, microphones, touchscreen\n\nSource: Russel and Norvig (2022, 61)"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#properties",
    "href": "lectures/I2AI/2/slides.html#properties",
    "title": "Intelligent agents",
    "section": "Properties",
    "text": "Properties\nTask environments can be categorized along following dimensions (Russel and Norvig 2022, 62â€“64):\n\n\nFully observable vs. partially observable\nSingle agent vs. multi-agent\nDeterministic vs. nondeterministic\nEpisodic vs. sequential\nStatic vs. dynamic\nDiscrete vs. continuous\nKnown vs. unknown\n\n\n\nIf an agentâ€™s sensors give it access to the full state of the environment at any point in time, then we say that the task environment is fully observable (e.g., image analysis).\nWhen multiple agents intend to maximize a performance measure that depends on the behavior of other agents, we say the environment is multi-agent (e.g., chess).\nWhen the environment is completely determined by the current state and the actions performed by the agent(s), it is called a deterministic environment (e.g., crossword puzzle). When a model of the environment explicitly uses probabilities, it is called a stochastic environment (e.g., poker).\nIf an agentâ€™s experience is divided into atomic episodes in which the agent receives a perception and then performs a single action, and if the next episode does not depend on the actions performed in the previous episodes, then we say that the task environment is episodic (e.g., image analysis).\nIf the environment changes while an agent is deliberating, then the environment is dynamic (e.g., taxi driving).\nIf the environment has a finite number of different states, we speak of discrete environments (e.g., chess).\nIf the outcomes (or outcome probabilities) for all actions are given, then the environment is known (e.g., solitaire card game).\nSource: Russel and Norvig (2022), p.62-64\n\n\nThe hardest case is partially observable, multi-agent, nondeterministic, sequential, dynamic, and continuous."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#simple-reflex-agents",
    "href": "lectures/I2AI/2/slides.html#simple-reflex-agents",
    "title": "Intelligent agents",
    "section": "Simple reflex agents",
    "text": "Simple reflex agents\n\n\n\nFigure 2: A simple reflex agent\n\n\n\n\nRectangles are used to denote the current internal state of the agentâ€™s decision process, rectangles with rounded corners to represent the background information used in the process.\n\nSimple reflex agents select actions on the basis of the current percept, ignoring the rest of the percept history. Thus, these agents work only if the correct decision can be made on the basis of just the current percept. The environment needs to be fully observable (Russel and Norvig 2022, 68)."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#model-based-reflex-agents",
    "href": "lectures/I2AI/2/slides.html#model-based-reflex-agents",
    "title": "Intelligent agents",
    "section": "Model-based reflex agents",
    "text": "Model-based reflex agents\n\n\n\nFigure 3: A model-based reflex agent\n\n\n\nModel-based reflex agents use transition models and sensor models to keep track of the state of the world as perceived by the sensors (i.e., internal state). (Russel and Norvig 2022, 70).\nThe transition model reflects â€œhow the world works,â€ i.e., how the world evolves (a) independently of the agent and (b) depending on the agentâ€™s actions.\nThe sensor model reflects how the state of the world is reflected in the agentâ€™s percepts (i.e., by its sensors).\n\n\n\n\n\n\nTypes of representation of states and the transitions between them\n\n\nThe representations of states can be placed along an axis of increasing complexity and expressive power (Russel and Norvig 2022, 76â€“77):\n\nAtomic representation: a state is a blackbox with no internal structure (A â€“> B)\nFactored representation: a state consists of a vector of attribute values; values can be Boolean, real-valued, or one of a fixed set of symbols; factored states can share some attributes and not others, which makes it easier to identify transitions between states\nStructured representation: a state includes objects, each of which may have attributes of its own as well as relationships to other objects; structured representations underlie relational databases and first-order logic, first-oder probability models, and much of natural language understanding.\n\nThe more expressive language is much more concise, but makes reasoning and learning more complex."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#goal-based-agents",
    "href": "lectures/I2AI/2/slides.html#goal-based-agents",
    "title": "Intelligent agents",
    "section": "Goal-based agents",
    "text": "Goal-based agents\n\n\n\nFigure 4: A model-based, goal-based agent\n\n\n\nA model-based, goal-based agent keeps track of the world state as well as a set of goals it is trying to achieve. Such an agent chooses an action that will (eventually) lead to the achievement of its goals (Russel and Norvig 2022, 72)."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#utility-based-agents",
    "href": "lectures/I2AI/2/slides.html#utility-based-agents",
    "title": "Intelligent agents",
    "section": "Utility-based agents",
    "text": "Utility-based agents\n\n\n\nFigure 5: A model-based, utility-based agent\n\n\n\nA model-based, utility-based agent uses a model of the world, along with a utility function that measures its preferences among states of the world. It chooses the action that leads to the best expected utility, where expected utility is computed by averaging over all possible states, weighted by the probability of the outcome (Russel and Norvig 2022, 73).\nThe utility function is essentially an internalization of the performance measure.\nA utility-based agent has many advantages in terms of flexibility and learning, which are particularly helpful in environments characterized by partial observability and nondeterminism.\nIn addition, there are cases where the goals are insufficient but a utility-based agent can still make rational decisions based on the probabilities and the utilities of the outcomes:\n\nWhen there are conflicting goals, the utility function specifies the appropriate tradeoff.\nLikelihood of success (i.e., goal achievement) can be weighed against the importance of the goals\n\nMode- and utility-based agents are difficult to implement. They need to model and keep track of the task environment, which requires ingenious sensors, sophisticated algorithms, and a high computational complexity.\nThere are also utility-based agents that are not model-based. These agents just learn what action is best in a particular situation without any â€œunderstandingâ€ of its impact on the environment (e.g., based on reinforcement learning)."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#learning-agents",
    "href": "lectures/I2AI/2/slides.html#learning-agents",
    "title": "Intelligent agents",
    "section": "Learning agents",
    "text": "Learning agents\n\n\n\nFigure 6: A learning agent\n\n\n\nEach type of agent can be either hand-programmed or created as a learning agent. The behavior of learning agents is (also) determined by their own experience, while the behavior of hand-programmed agents is solely determined by their initial programming. Thus, learning agents have greater autonony.\nA learning agent consists of four conceptual components (Russel and Norvig 2022, p- 74-75), as shown in FigureÂ 6:\n\nThe performance element is responsible for taking percepts and selecting actions (i.e., what has previously been considered the entire agent program).\nThe learning element is responsible for improvements. It uses feedback from the critic on how the agent is doing and determines how the performance element should be modified to do better in the future. It can make changes to any of the â€œknowledge componentsâ€ shown in the agent diagrams (i.e., condition-action-rules, transition model, sensor model)\nThe performance standard is responsible to inform the agent about the meaning of percepts â€” are they good nor not (e.g., the meaning of receiving no tips from passengers is a negative contribution to an automated taxisâ€™s overall performance). The standard is fixed and cannot be influenced by the agent.\nThe problem generator is responsible for suggesting actions that lead to new and informative experiences. The problem generator suggests exploratory actions that may be suboptimal in the short term, but can lead to the discovery of better actions in the long term."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e1",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e1",
    "title": "Intelligent agents",
    "section": "I2AI_2 E1",
    "text": "I2AI_2 E1\nDefine in your own words the following terms:\n\nAgent\nEnvironment\nSensor\nActuator\nPercept\nAgent function\nAgent program"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e2",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e2",
    "title": "Intelligent agents",
    "section": "I2AI_2 E2",
    "text": "I2AI_2 E2\nFor each of the following agents, specify the sensors, actuators, and environment:\n\nMicrowave oven\nChess program\nAutonomous supply delivery"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e3",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e3",
    "title": "Intelligent agents",
    "section": "I2AI_2 E3",
    "text": "I2AI_2 E3\nDescribe a task environments in which the performance measure is easy to specify completely and correctly, and a in which it is not."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e4",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e4",
    "title": "Intelligent agents",
    "section": "I2AI_2 E4",
    "text": "I2AI_2 E4\nFor each of the following assertions, say whether it is true or false and support your answer with examples or counterexamples where appropriate.\n\nAn agent that senses only partial information about the state cannot be perfectly rational.\nThere exist task environments in which no pure reflex agent can behave rationally.\nThere exists a task environment in which every agent is rational.\nThe input to an agent program is the same as the input to the agent function.\nEvery agent is rational in an unobservable environment.\nThere is a model-based reflex agent that can remember all of its percepts.\nSuppose agent A1 is rational and agent A2 is irrational. There exists a task environment where A2â€™s actual score will be greater than A1â€™s actual score."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e5",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e5",
    "title": "Intelligent agents",
    "section": "I2AI_2 E5",
    "text": "I2AI_2 E5\nFor each of the following activities, give a PEAS description of the task environment and characterize it in terms of the properties discussed in class.\n\nPlaying soccer.\nExploring the subsurface oceans of Titan.\nShopping for used AI books on the Internet.\nPlaying a tennis match."
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e6",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e6",
    "title": "Intelligent agents",
    "section": "I2AI_2 E6",
    "text": "I2AI_2 E6\nFor each of the following task environment properties, rank the example task environments from most to least according to how well the environment satisfies the property.\nLay out any assumptions you make to reach your conclusions.\n\nFully observable: driving; document classification; tutoring a student in calculus; skin cancer diagnosis from images\nContinuous: driving; spoken conversation; written conversation; climate engineering by stratospheric aerosol injection\nStochastic: driving; sudoku; poker; soccer\nStatic: chat room; checkers; tax planning; tennis"
  },
  {
    "objectID": "lectures/I2AI/2/slides.html#i2ai_2-e7",
    "href": "lectures/I2AI/2/slides.html#i2ai_2-e7",
    "title": "Intelligent agents",
    "section": "I2AI_2 E7",
    "text": "I2AI_2 E7\nDefine in your own words the following terms\n\nRationality\nAutonomy\nReflex agent,\nModel-based agent\nGoal-based agent\nUtility-based agent\nLearning agent"
  },
  {
    "objectID": "lectures/GIM/0/index.html#einheiten",
    "href": "lectures/GIM/0/index.html#einheiten",
    "title": "Administratives ðŸ§",
    "section": "Einheiten",
    "text": "Einheiten\nDie Vorlesung ist in 10 Kapitel (K) gegliedert\n\n\nK1 EinfÃ¼hrung: Was ist das und weshalb ist es wichtig?\nK2 Das digitale Zeitalter: Was macht unsere Zeit besonders?\nK3 IT: Was ist die Basis von Informationssystemen?\nK4 Datenbanken #1: Wie kÃ¶nnen Daten strukturiert werden?\nK5 Datenbanken #2: Wie kÃ¶nnen Datenstrukturen abgebildet werden?\nK6 Anwendungssysteme: Welche gibt es und wozu?\nK7 GeschÃ¤ftsprozessmanagement: Wie kÃ¶nnen AblÃ¤ufe gesteuert werden?\nK8 Prozess-Modellierung: Wie kÃ¶nnen Prozesse verstÃ¤ndlich abbilden?\nK9 IT-Management: Wie steure ich IT im Unternehmen?\nK10 Digitale GeschÃ¤ftsmodelle: Was ist das nÃ¤chste groÃŸe Ding?"
  },
  {
    "objectID": "lectures/GIM/0/index.html#lernmaterialien",
    "href": "lectures/GIM/0/index.html#lernmaterialien",
    "title": "Administratives ðŸ§",
    "section": "Lernmaterialien",
    "text": "Lernmaterialien\nJedes Kapitel enthÃ¤lt folgende Lernmaterialien:\n\n\nVorlesungsskript\nPodcast (Kommentare zu den Inhalten des Kapitels und ErgÃ¤nzungen; keine 1:1 Besprechung der Folien)\nÃœbungen (Wiederholung, Reflektion und Vertiefung)\nReferenz zu weiterfÃ¼hrender Literatur (insbesondere Krcmar (2015), Fank (2018), Mertens u.Â a. (2016), Lemke, Brenner, und Kirchner (2017) und BÃ¤chle, Daurer, und Kolb (2021))\nMoodle-Quiz\n\n\n\nAlle Inhalte sind prÃ¼fungsrelevant."
  },
  {
    "objectID": "lectures/GIM/0/index.html#ablauf-und-aufwÃ¤nde",
    "href": "lectures/GIM/0/index.html#ablauf-und-aufwÃ¤nde",
    "title": "Administratives ðŸ§",
    "section": "Ablauf und AufwÃ¤nde",
    "text": "Ablauf und AufwÃ¤nde\nDer Kurs basiert auf einem Blended Learning Konzept (â€žFlipped Classroomâ€œ)\n\n\nSie erarbeiten sich die Inhalte im Selbststudium (10 x ca. 3,5 h)\nWir wiederholen und vertiefen in der Veranstaltung (13 x ca. 2,5 h)\n\nZusammenfassung der Inhalte (Big Picture)\nAntwort auf Ihre Fragen (Q&A)\nÃœbungen (oft in Kleingruppen)\n\nIn der Portfolio-PrÃ¼fung wenden Sie Ihr Wissen an\n\nBearbeitung Essay (THE-1; ca. 6 h)\nBearbeitung Ãœbungsblatt (THE-2; ca. 3 h)\nAbschlussprÃ¼fung\n\n\n\n\nDer Kurs ist mit 4 ECTS gewichtet. Die ECTS entsprechen einem Arbeitsaufwand von ca. 120 h.\nDie ungefÃ¤hre Verteilung des Arbeitsaufwands entnehmen Sie bitte obiger Auflistung. Die restliche Zeit ist fÃ¼r das Selbststudium, insbesondere die Vorbereitung auf die AbschlussprÃ¼fung (THE-3) vorgesehen.\nDie Erfahrung aus vorangegangen Kursen lehrt, dass ohne die Teilnahme an den Live-Sessions und die Bearbeitung der Ãœbungen das Bestehen des Kurses mit einer ordentlichen Note kaum mÃ¶glich ist.\nDieses Lern-Set-Up soll Sie dazu befÃ¤higen\n\neigenverantwortlich mit Lernangeboten umzugehen und wirksame Lernstrategien zu entwickeln,\nden Einsatz von digitalen Werkzeugen in Unternehmen gesellschaftskritisch zu reflektieren,\nZusammenhÃ¤nge zu erfassen, auf den Punkt zu bringen und fÃ¼r andere verstÃ¤ndlich zu erklÃ¤ren"
  },
  {
    "objectID": "lectures/GIM/0/index.html#prÃ¼fungsleistung",
    "href": "lectures/GIM/0/index.html#prÃ¼fungsleistung",
    "title": "Administratives ðŸ§",
    "section": "PrÃ¼fungsleistung",
    "text": "PrÃ¼fungsleistung\nDie Vorlesung wird anhand einer Portfolio-PrÃ¼fung bewertet\n\n\nDie Portfolio-PrÃ¼fung besteht aus vier unterschiedlichen Teilen:\n\nQuizze (THE-0; je ca. 2 Wochen Bearbeitungszeit, 10 Punkte)2\nPressemeldung (THE-1; ca. 3 Wochen Bearbeitungszeit, 15 Punkte)\nÃœbungsblatt (THE-2; ca. 2 Tage Bearbeitungszeit, 15 Punkte)\nAbschlussprÃ¼fung (ca. 60 Minuten Bearbeitungszeit, 50 Punkte)\n\nDie Gesamtzahl der in diesem Kurs erreichbaren Punkte betrÃ¤gt 90. Sie benÃ¶tigen 45 Punkte zum Bestehen."
  },
  {
    "objectID": "lectures/GIM/0/slides.html#einheiten",
    "href": "lectures/GIM/0/slides.html#einheiten",
    "title": "Administratives ðŸ§",
    "section": "Einheiten",
    "text": "Einheiten\nDie Vorlesung ist in 10 Kapitel (K) gegliedert\n\n\nK1 EinfÃ¼hrung: Was ist das und weshalb ist es wichtig?\nK2 Das digitale Zeitalter: Was macht unsere Zeit besonders?\nK3 IT: Was ist die Basis von Informationssystemen?\nK4 Datenbanken #1: Wie kÃ¶nnen Daten strukturiert werden?\nK5 Datenbanken #2: Wie kÃ¶nnen Datenstrukturen abgebildet werden?\nK6 Anwendungssysteme: Welche gibt es und wozu?\nK7 GeschÃ¤ftsprozessmanagement: Wie kÃ¶nnen AblÃ¤ufe gesteuert werden?\nK8 Prozess-Modellierung: Wie kÃ¶nnen Prozesse verstÃ¤ndlich abbilden?\nK9 IT-Management: Wie steure ich IT im Unternehmen?\nK10 Digitale GeschÃ¤ftsmodelle: Was ist das nÃ¤chste groÃŸe Ding?"
  },
  {
    "objectID": "lectures/GIM/0/slides.html#lernmaterialien",
    "href": "lectures/GIM/0/slides.html#lernmaterialien",
    "title": "Administratives ðŸ§",
    "section": "Lernmaterialien",
    "text": "Lernmaterialien\nJedes Kapitel enthÃ¤lt folgende Lernmaterialien:\n\n\nVorlesungsskript\nPodcast (Kommentare zu den Inhalten des Kapitels und ErgÃ¤nzungen; keine 1:1 Besprechung der Folien)\nÃœbungen (Wiederholung, Reflektion und Vertiefung)\nReferenz zu weiterfÃ¼hrender Literatur (insbesondere Krcmar (2015), Fank (2018), Mertens u.Â a. (2016), Lemke, Brenner, und Kirchner (2017) und BÃ¤chle, Daurer, und Kolb (2021))\nMoodle-Quiz\n\n\n\nAlle Inhalte sind prÃ¼fungsrelevant."
  },
  {
    "objectID": "lectures/GIM/0/slides.html#ablauf-und-aufwÃ¤nde",
    "href": "lectures/GIM/0/slides.html#ablauf-und-aufwÃ¤nde",
    "title": "Administratives ðŸ§",
    "section": "Ablauf und AufwÃ¤nde",
    "text": "Ablauf und AufwÃ¤nde\nDer Kurs basiert auf einem Blended Learning Konzept (â€žFlipped Classroomâ€œ)\n\n\nSie erarbeiten sich die Inhalte im Selbststudium (10 x ca. 3,5 h)\nWir wiederholen und vertiefen in der Veranstaltung (13 x ca. 2,5 h)\n\nZusammenfassung der Inhalte (Big Picture)\nAntwort auf Ihre Fragen (Q&A)\nÃœbungen (oft in Kleingruppen)\n\nIn der Portfolio-PrÃ¼fung wenden Sie Ihr Wissen an\n\nBearbeitung Essay (THE-1; ca. 6 h)\nBearbeitung Ãœbungsblatt (THE-2; ca. 3 h)\nAbschlussprÃ¼fung\n\n\n\n\nDer Kurs ist mit 4 ECTS gewichtet. Die ECTS entsprechen einem Arbeitsaufwand von ca. 120 h.\nDie ungefÃ¤hre Verteilung des Arbeitsaufwands entnehmen Sie bitte obiger Auflistung. Die restliche Zeit ist fÃ¼r das Selbststudium, insbesondere die Vorbereitung auf die AbschlussprÃ¼fung (THE-3) vorgesehen.\nDie Erfahrung aus vorangegangen Kursen lehrt, dass ohne die Teilnahme an den Live-Sessions und die Bearbeitung der Ãœbungen das Bestehen des Kurses mit einer ordentlichen Note kaum mÃ¶glich ist.\nDieses Lern-Set-Up soll Sie dazu befÃ¤higen\n\neigenverantwortlich mit Lernangeboten umzugehen und wirksame Lernstrategien zu entwickeln,\nden Einsatz von digitalen Werkzeugen in Unternehmen gesellschaftskritisch zu reflektieren,\nZusammenhÃ¤nge zu erfassen, auf den Punkt zu bringen und fÃ¼r andere verstÃ¤ndlich zu erklÃ¤ren"
  },
  {
    "objectID": "lectures/GIM/0/slides.html#prÃ¼fungsleistung",
    "href": "lectures/GIM/0/slides.html#prÃ¼fungsleistung",
    "title": "Administratives ðŸ§",
    "section": "PrÃ¼fungsleistung",
    "text": "PrÃ¼fungsleistung\nDie Vorlesung wird anhand einer Portfolio-PrÃ¼fung bewertet\n\n\nDie Portfolio-PrÃ¼fung besteht aus vier unterschiedlichen Teilen:\n\nQuizze (THE-0; je ca. 2 Wochen Bearbeitungszeit, 10 Punkte)2\nPressemeldung (THE-1; ca. 3 Wochen Bearbeitungszeit, 15 Punkte)\nÃœbungsblatt (THE-2; ca. 2 Tage Bearbeitungszeit, 15 Punkte)\nAbschlussprÃ¼fung (ca. 60 Minuten Bearbeitungszeit, 50 Punkte)\n\nDie Gesamtzahl der in diesem Kurs erreichbaren Punkte betrÃ¤gt 90. Sie benÃ¶tigen 45 Punkte zum Bestehen."
  },
  {
    "objectID": "lectures/DIiI/0/index.html",
    "href": "lectures/DIiI/0/index.html",
    "title": "Administrivia ðŸ§",
    "section": "",
    "text": "Top\nSlides\n\n\nGeneral remarks\nThis course will be taught using traditional synchronous lectures. The focus of the live sessions is on discussing real world cases and developing theoretically-based explanations. In addition, you will do an original research on a digital innovation case on your own (see assignment).\nIt is of really important that you prepare yourself for every session. You either need to work through teaching materials in advance or prepare to present the deliverables of your own case (see TableÂ 1).\n\n\nContents\nThis course offers you the possibility to learn about industry-specific challenges and how this influences the way of organizing, managing and communicating digital innovations, in particular:\n\n\nSpecifics of industries (context, products, markets, obstacles, enablers)\nIndustry-specific applicability and adaptations of innovation strategies\nMethods for discovering need finding, ideation and prototyping of digital innovations\nCase studies of digital innovation in industries\nTheories that explain the mechanisms of digital innovations\n\n\n\n\nLearning objectives\nDuring this course, you should advance your skills in the following areas:\n\n\nEnhanced knowledge of various industries and their special characteristics\nUnderstanding of specifics of managing digital innovations in these industries\nKnowledge of theoretical models that help to explain and manage these innovations\nAbility to analyze specificities and complexities of different industries\nCapacity to adapt digital innovation management methods and tools accordingly\nAbility to clearly communicate conclusions and the underlying rationale\nAbility to evaluate new information and to question existing assumptions\nExpertise to develop independent contributions to practical and theoretical discourse\n\n\n\n\nEffort\nYou will gain 5 ECTS for this course. This equals approx. 150 hours workload, of which you need most to pass the course.\n\n\nLive sessions (approx. 40 hours)\nPreparation of the live sessions (approx. 20 hours)\nCase research and writing (approx. 90 hours)\n\n\n\nPlease prepare your schedule accordingly.\n\n\n\nGrading\nThe grade is based on the evaluation of the written case study (see assignment).\nThe presentations are mandatory but will not be graded individually.\n\n\nSchedule\n\nIt is of importance that you prepare yourself for the sessions and work on the case study continually. Please prepare your schedule accordingly.\n\n\n\n\nTable 1: Schedule summer term 2022 (may be subjected to changes)\n\n\n\n\n\n\n\nDate\nTopic\nPreparation\n\n\n\n\n03/15/2022\nAdministrivia & new patterns of innovation\n-\n\n\n03/16/2022\nWashington Post â€” digital transformation in media (SWOT & BM canvas)\nReading (case)\n\n\n03/17/2022\nThreadless.com â€” creating value through online communities (crowdsourcing & innovation)\nReading (case)\n\n\n03/24/2022\nInterim presentation & coaching: ideas\nPresentation\n\n\n03/31/2022\nWieland â€” virtualizing product development; guest speech (digital twin)\n-\n\n\n04/07/2022\nBVB â€” innovation in retail; guest speech (signaling theory) ZOOM\n-\n\n\n04/14/2022\nInterim presentation & coaching: problem & solution\nPresentation\n\n\n04/21/2022\nSCHOTT â€” a two sided approach for DI (path constitution theory)\nReading (case)\n\n\n04/28/2022\nInterim presentation & coaching: industry & company\nPresentation\n\n\n05/05/2022\nDeutsche Bank - innovation in finance, a safe bank? guest speech (ambidexterity)\n-\n\n\n05/12/2022\nInterim presentation & coaching: approach\nPresentation\n\n\n05/19/2022\nSpringest â€” IT-enabled holacratic organizations (self-managing organizations)\nReading (case)\n\n\n06/02/2022\nInterim presentation & coaching: discussion\nPresentation\n\n\n06/23/2022\nFinal presentations\nPresentation\n\n\n07/07/2022\nDeadline case study\nSeminar paper"
  },
  {
    "objectID": "lectures/DIiI/assignment/index.html#paper",
    "href": "lectures/DIiI/assignment/index.html#paper",
    "title": "Assignment",
    "section": "Paper",
    "text": "Paper\nThe seminar paper is composed of following elements\n\n\nCase summary: A brief description of the case, the digital innovation and its impact on the organization and industry.\nThe Hook: the beginning of the case that grabs the reader and generates interest in reading further. It should stimulate interest or curiosity in the reader for what is to come by suggesting the main challenge that is addressed by the digital innovation.\nIndustry: here you characterize the industry and outline any information that is required or helpful for understanding the context of the innovation and the innovation process, e.g., relevant services or products, relevant frameworks like regulations, the primary factor that produces profit and key challenges addressed by the innovation described in this case study.\nCompany: provides a decent overview of the company (organization, respectively) including the most salient features about the business and challenges faced that are related to the innovation.\nProblem: you outline and reflect the challenges which the digital innovation addresses (should address) and synthesize them into a concise problem statement.\nApproach: covers how the organization has developed and implemented the digital innovation and, if data is available, what challenges needed to be tackled.\nSolution: provides a rich description of the digital innovation, how it approaches the initially outlined problem statement and which effects it has generated on the organization (e.g., revenues) and the industry.\nDiscussion: you comment, provide analysis and identify theories, or connect concepts or learning that help explain the mechanisms and effects of the innovation.\nLessons learned: you should make at least one firm recommendation for practitioners dealing with innovation management in the industry and explain why you have made this recommendation.\nAppendix: covers at least comprehensible information on how you did collect the research or this case study. You may also add any information that is helpful to gain a broader understanding of the case but is not necessary to follow your line of though in the main body of the text.\n\n\n\n\n\n\nSummary\nHook (intro & motivation)\nIndustry (characteristics, challenges)\nCompany (overview, specifics)\nProblem\n\n\n\n\n\nApproach (innovation process)\nSolution\nDiscussion (explanation)\nLessons Learned (recommendations)\nAppendix (methods)"
  },
  {
    "objectID": "lectures/DIiI/assignment/index.html#industries",
    "href": "lectures/DIiI/assignment/index.html#industries",
    "title": "Assignment",
    "section": "Industries",
    "text": "Industries\nThe case should focus on a digital innovation in one of the following industries\n\nHealthcare\nManufacturing\nLogistics/transportation\nRetail\nProfessional services"
  },
  {
    "objectID": "lectures/DIiI/assignment/index.html#presentations",
    "href": "lectures/DIiI/assignment/index.html#presentations",
    "title": "Assignment",
    "section": "Presentations",
    "text": "Presentations\nYou will present your interim results repeatedly throughout the semester\n\n\nIdeas (brief intro to two to three digital innovations)\nGATE â€” assignment of the topics\nIndustry & company (characteristics & challenges)\nProblem and solution\nApproach (development & implementation)\nDiscussion (theoretically grounded explanations)\nSummary & lessons learned\n\nFor dates please see the schedule"
  },
  {
    "objectID": "lectures/DIiI/assignment/slides.html#paper",
    "href": "lectures/DIiI/assignment/slides.html#paper",
    "title": "Assignment",
    "section": "Paper",
    "text": "Paper\nThe seminar paper is composed of following elements\n\n\nCase summary: A brief description of the case, the digital innovation and its impact on the organization and industry.\nThe Hook: the beginning of the case that grabs the reader and generates interest in reading further. It should stimulate interest or curiosity in the reader for what is to come by suggesting the main challenge that is addressed by the digital innovation.\nIndustry: here you characterize the industry and outline any information that is required or helpful for understanding the context of the innovation and the innovation process, e.g., relevant services or products, relevant frameworks like regulations, the primary factor that produces profit and key challenges addressed by the innovation described in this case study.\nCompany: provides a decent overview of the company (organization, respectively) including the most salient features about the business and challenges faced that are related to the innovation.\nProblem: you outline and reflect the challenges which the digital innovation addresses (should address) and synthesize them into a concise problem statement.\nApproach: covers how the organization has developed and implemented the digital innovation and, if data is available, what challenges needed to be tackled.\nSolution: provides a rich description of the digital innovation, how it approaches the initially outlined problem statement and which effects it has generated on the organization (e.g., revenues) and the industry.\nDiscussion: you comment, provide analysis and identify theories, or connect concepts or learning that help explain the mechanisms and effects of the innovation.\nLessons learned: you should make at least one firm recommendation for practitioners dealing with innovation management in the industry and explain why you have made this recommendation.\nAppendix: covers at least comprehensible information on how you did collect the research or this case study. You may also add any information that is helpful to gain a broader understanding of the case but is not necessary to follow your line of though in the main body of the text.\n\n\n\n\n\n\nSummary\nHook (intro & motivation)\nIndustry (characteristics, challenges)\nCompany (overview, specifics)\nProblem\n\n\n\n\n\nApproach (innovation process)\nSolution\nDiscussion (explanation)\nLessons Learned (recommendations)\nAppendix (methods)"
  },
  {
    "objectID": "lectures/DIiI/assignment/slides.html#industries",
    "href": "lectures/DIiI/assignment/slides.html#industries",
    "title": "Assignment",
    "section": "Industries",
    "text": "Industries\nThe case should focus on a digital innovation in one of the following industries\n\nHealthcare\nManufacturing\nLogistics/transportation\nRetail\nProfessional services"
  },
  {
    "objectID": "lectures/DIiI/assignment/slides.html#presentations",
    "href": "lectures/DIiI/assignment/slides.html#presentations",
    "title": "Assignment",
    "section": "Presentations",
    "text": "Presentations\nYou will present your interim results repeatedly throughout the semester\n\n\nIdeas (brief intro to two to three digital innovations)\nGATE â€” assignment of the topics\nIndustry & company (characteristics & challenges)\nProblem and solution\nApproach (development & implementation)\nDiscussion (theoretically grounded explanations)\nSummary & lessons learned\n\nFor dates please see the schedule"
  }
]