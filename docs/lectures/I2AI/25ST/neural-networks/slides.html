<!DOCTYPE html>
<html lang="en"><head>
<link href="../../../../assets/favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.40">

  <meta name="author" content="Andy Weeger">
  <meta name="dcterms.date" content="2025-06-01">
  <title>awe.lectures – Neural Networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../../../site_libs/revealjs/dist/theme/quarto-bcefe2d09215085bbf591c744dc9b0ae.css">
  
  <script type="text/javascript" charset="UTF-8">
  document.addEventListener('DOMContentLoaded', function () {
  cookieconsent.run({
    "notice_banner_type":"interstitial",
    "consent_type":"express",
    "palette":"dark",
    "language":"en",
    "page_load_consent_levels":["strictly-necessary"],
    "notice_banner_reject_button_hide":false,
    "preferences_center_close_button_hide":false,
    "website_name":""
    ,
  "language":"en"
    });
  });
  </script> 
    
  <link href="../../../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Neural Networks – awe.lectures">
<meta property="og:description" content="Introduction to AI (I2AI)">
<meta property="og:site_name" content="awe.lectures">
</head>
<body class="quarto-light">
<div class="footer custom">

    <div class="version">
        V1.2
    </div>

    <div class="footnote">
        Andy Weeger / Neu-Ulm University of Applied Sciences Neu-Ulm / Introduction to Artificial Intelligence (AI)
   </div>

    <div class="logo">
        <svg viewbox="0 0 125 39" xmlns="http://www.w3.org/2000/svg" version="1.1" space="preserve">
            <g>
             <title>HNU</title>
             <path d="m8.58222,0l7.5,0l0,12.7l-7.5,0l0,-12.7zm7.4,24.1l6.5,0l0,-3l-13.9,0l0,15.4l-3.9,0l0,2.3l11.3,0l0,-14.7zm24,-24.1l-7.4,0l0,36.5l-3.9,0l0,2.3l11.3,0l0,-38.8zm60.2,28.4c1.1,0 2.1,-0.1 2.9,-0.4c-1.3,-1.2 -1.9,-3.2 -1.9,-6.4l0,-21.6l-7.4,0l0,19.3c0,5.9 1.5,9.1 6.4,9.1z" id="svg_1"></path>
             <path d="m96.78222,37c2.8,1.6 6.4,2.5 10.6,2.5c11.2,0 17.7,-6 17.7,-16.3l0,-23.2l-7.4,0l0,21.1c0,11.3 -7.7,16.1 -17.6,16.1c-1.1,0 -2.2,-0.1 -3.3,-0.2zm-30.8,-27l-7.1,-10l-7.3,0l14.4,20.4l0,-10.4zm17,28.8l0,-38.8l-7.5,0l0,36.5l-3.5,0l1.7,2.3l9.3,0zm-24,-14.8l-8,-10.8l0,23.3l-3,0l0,2.3l11,0l0,-14.8z" id="svg_2"></path>
            </g>   
        </svg>
    </div>

</div>
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-color="#0333ff" data-background-image="../assets/bg.jpeg" data-background-opacity="1" data-background-size="cover" class="quarto-title-block">
  <h1 class="title">Neural Networks</h1>
  <p class="subtitle">Introduction to AI (I2AI)</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Andy Weeger 
</div>
        <p class="quarto-title-affiliation">
            Neu-Ulm University of Applied Sciences
          </p>
    </div>
</div>

  <p class="date">June 1, 2025</p>
</section>
<section>
<section id="introduction" class="title-slide slide level1 headline-only">
<h1>Introduction</h1>

</section>
<section id="discussion" class="title-slide slide level2 html-hidden discussion-slide unlisted">
<h2>Discussion</h2>
<div class="larger">
<p>What makes handwritten digit recognition <strong>trivial for humans</strong> but <strong>extremely difficult for traditional programming</strong>?</p>
</div>
</section>

<section id="limits-of-traditional-programming" class="title-slide slide level2">
<h2>Limits of traditional programming</h2>
<div class="large">
<p>Traditional programming approaches fail at tasks that humans find effortless.</p>
</div>
<div class="fragment">
<p>For instance:</p>
<div>
<ul>
<li class="fragment"><strong>Recognizing handwritten digits</strong>: Each “3” looks different, yet we instantly recognize the pattern</li>
<li class="fragment"><strong>Understanding context</strong>: “The bank” could refer to a financial institution or a river’s edge</li>
<li class="fragment"><strong>Learning from examples</strong>: We don’t need explicit rules to recognize new instances</li>
</ul>
</div>
<aside class="notes">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>The intelligence paradox</strong></p>
</div>
<div class="callout-content">
<p>Traditional programming relies on explicit rules and algorithms. For image recognition, you’d need to write code that handles every possible variation of how a digit could be drawn - different angles, sizes, writing styles, and lighting conditions. This quickly becomes intractable.</p>
<p>Human brains, however, excel at pattern recognition through learning from examples. We see many instances of the digit “3” and somehow extract the underlying pattern without being given explicit rules about what makes a “3” a “3”.</p>
<p>This paradox - tasks that are trivial for biological intelligence but nearly impossible for traditional programming - led to the development of neural networks and machine learning approaches that attempt to mimic how biological systems learn from data.</p>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>

<section id="recap-machine-learning" class="title-slide slide level2">
<h2>Recap: machine learning</h2>
<div class="columns">
<div class="column">
<div class="fragment">
<p><strong>Traditional programming:</strong></p>
<p><span class="math inline">\(Input + Program \rightarrow Output\)</span></p>
</div>
</div><div class="column">
<div class="fragment">
<p><strong>Machine learning:</strong></p>
<p><span class="math inline">\(Input + Output \rightarrow Program\)</span></p>
</div>
</div></div>
<div class="fragment">
<p><span class="h4"><strong>Differences</strong></span></p>
<div>
<ul>
<li class="fragment">Instead of writing explicit rules, we provide <strong>examples</strong> (training data)</li>
<li class="fragment">The machine <strong>learns patterns</strong> from these examples</li>
<li class="fragment">The resulting model can then make <strong>predictions</strong> on new, unseen data</li>
</ul>
</div>
<aside class="notes">
<p>Traditional programming requires us to understand and explicitly code the relationship between inputs and outputs. For complex tasks like image recognition, this becomes impossible because we can’t enumerate all the rules.</p>
<p>Machine learning flips this paradigm: we provide many examples of inputs paired with their correct outputs, and let the algorithm discover the underlying patterns. This is particularly powerful for tasks where the rules are too complex to code explicitly or where we don’t fully understand the underlying mechanisms ourselves.</p>
<p>The key insight is that many intelligent behaviors can emerge from relatively simple learning rules applied to large amounts of data, rather than requiring explicit programming of complex behaviors. This observation connects to the foundational work on neural networks by <span class="citation" data-cites="rumelhart1986learning">Rumelhart, Hinton, and Williams (<a href="#/literature" role="doc-biblioref" onclick="">1986</a>)</span> and the theoretical foundations of universal approximation <span class="citation" data-cites="cybenko1989approximation hornik1989multilayer">(<a href="#/literature" role="doc-biblioref" onclick="">Cybenko 1989</a>; <a href="#/literature" role="doc-biblioref" onclick="">Hornik, Stinchcombe, and White 1989</a>)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>
</section>
<section>
<section id="neural-networks" class="title-slide slide level1 headline-only">
<h1>Neural networks</h1>

</section>
<section id="introduction-1" class="title-slide slide level2">
<h2>Introduction</h2>
<p>Neural networks solve problems that traditional programming cannot handle:</p>
<div>
<ul>
<li class="fragment"><strong>Pattern recognition</strong> in noisy, variable data</li>
<li class="fragment"><strong>Decision making</strong> with incomplete information<br>
</li>
<li class="fragment"><strong>Automation</strong> of complex cognitive tasks</li>
<li class="fragment"><strong>Scaling</strong> human-like judgment to massive datasets</li>
</ul>
</div>
<aside class="notes">
<p>The beauty of neural networks lies in their universality - the same basic architecture that recognizes handwritten digits can be adapted to recognize faces, translate languages, or play games. This is because they learn to detect increasingly complex patterns through multiple layers of simple operations.</p>
<p>Understanding neural networks isn’t about memorizing mathematical formulas — it’s about recognizing when and how this technology can create business value. Neural networks excel in situations where:</p>
<ol type="1">
<li><p><strong>Rules are hard to specify</strong>: Try writing explicit rules for recognizing the digit “3” across thousands of different handwriting styles. Traditional programming would require an impossibly complex set of if-then statements.</p></li>
<li><p><strong>Human expertise is expensive to scale</strong>: A human can easily recognize digits, but hiring humans to process millions of documents isn’t feasible. Neural networks can replicate human-like pattern recognition at machine speed and scale.</p></li>
<li><p><strong>Data is abundant but messy</strong>: Real-world data rarely fits neat categories. Neural networks can find patterns in noisy, incomplete, or variable data that would break traditional algorithms.</p></li>
<li><p><strong>Adaptability is crucial</strong>: Business environments change constantly. Neural networks can be retrained on new data, allowing systems to adapt to changing conditions without complete reprogramming.</p></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="discussion-1" class="title-slide slide level2 html-hidden discussion-slide unlisted" data-background-color="#efefef">
<h2>Discussion</h2>
<div class="large">
<p>If you had to <strong>design a learning system</strong> inspired by the brain, what key components would you include?</p>
</div>
</section>

<section id="what-is-a-neuron" class="title-slide slide level2">
<h2>What is a neuron?</h2>
<div class="medium">
<p>A neuron<br>
<strong>receives inputs</strong> → <strong>weights them</strong> → <strong>sums up</strong> → <strong>activates</strong></p>
</div>
<div>
<ul>
<li class="fragment">This number is called the <strong>activation</strong> of the neuron</li>
<li class="fragment">High activation (close to 1.0) = neuron is “firing” or “lit up”</li>
<li class="fragment">Low activation (close to 0.0) = neuron is inactive</li>
<li class="fragment">Think of it as <strong>how excited</strong> the neuron is about a particular feature</li>
</ul>
</div>
<aside class="notes">
<aside class="notes">
<p>The neuron is the fundamental computational unit that makes neural networks possible. While inspired by biological neurons, artificial neurons are much simpler mathematical functions. Understanding this building block is crucial because the entire network’s behavior emerges from millions of these simple operations.</p>
<ol type="1">
<li><p><strong>Receiving inputs</strong>: Each neuron receives numerical values from the previous layer. In the first layer, these might be pixel intensities (0 for black, 1 for white). In deeper layers, these are the outputs of neurons from the previous layer.</p></li>
<li><p><strong>Weighting inputs</strong>: Each connection has a “weight” - a number that determines how much influence that input has. Positive weights amplify the signal, negative weights suppress it, and weights near zero essentially ignore that input. These weights are the “knowledge” the network learns.</p></li>
<li><p><strong>Summing</strong>: The neuron calculates a weighted sum: (input₁ × weight₁) + (input₂ × weight₂) + … + bias. The bias is like a threshold - it shifts the activation point of the neuron.</p></li>
<li><p><strong>Activation function</strong>: The sum gets passed through a function (like sigmoid or ReLU) that determines the neuron’s output. This introduces non-linearity, allowing the network to learn complex patterns rather than just linear relationships.</p></li>
</ol>
<p><span class="h4"><strong>Why this design works:</strong></span></p>
<ul>
<li><strong>Simplicity</strong>: Each neuron does something very simple, making the system robust and parallelizable</li>
<li><strong>Composability</strong>: Simple operations combine to create complex behaviors</li>
<li><strong>Differentiability</strong>: The mathematical smoothness allows for efficient learning algorithms</li>
<li><strong>Biological inspiration</strong>: While simplified, this captures key aspects of how biological neurons process information</li>
</ul>
<p>The magic happens when thousands of these simple units work together in layers, each learning to detect different aspects of the input pattern.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Biological inspiration</strong></p>
</div>
<div class="callout-content">
<p>Real neurons in the brain can be in various states of activation - they can fire action potentials at different rates, or remain quiet. The artificial neuron is a dramatic simplification, reducing this complex behavior to a single number between 0 and 1.</p>
<p>This simplification is intentional: by abstracting away the biological complexity, we can focus on the computational principles. The key insight is that neurons can represent information through their level of activation, and that these activations can be combined and transformed through networks to process complex information.</p>
<p>While the biological brain is vastly more complex, this simplified model has proven remarkably effective for a wide range of tasks, suggesting that some aspects of intelligence can emerge from relatively simple computational units arranged in the right structure.</p>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="network-architecture" class="title-slide slide level2">
<h2>Network architecture</h2>
<div class="html-hidden">
<div class="r-stack">
<div id="fig-nna-1" class="quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-nna-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="images/neural-network-architecture-1.svg" class="img-fluid">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nna-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Basic neural network structure
</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/neural-network-architecture-2.svg" class="fragment img-fluid"></p>
<figcaption> </figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/neural-network-architecture.svg" class="fragment img-fluid"></p>
<figcaption> </figcaption>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div id="fig-nna-1" class="quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-nna-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="images/neural-network-architecture.svg" class="img-fluid">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nna-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Basic neural network structure
</figcaption>
</figure>
</div>
<p>The hierarchical organization of neural networks mirrors how human visual processing works, and this parallel isn’t coincidental — it’s one of the key insights that makes deep learning so powerful.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="connections-between-neurons" class="title-slide slide level2">
<h2>Connections between neurons</h2>
<div class="columns">
<div class="column">
<p>Each connection between neurons has a <strong>weight</strong> (positive or negative) — a number that gets adusted during learning.</p>
<div>
<ul>
<li class="fragment"><strong>Positive weight</strong>: If the first neuron fires, it encourages the second neuron to fire</li>
<li class="fragment"><strong>Negative weight</strong>: If the first neuron fires, it discourages the second neuron from firing</li>
<li class="fragment"><strong>Bias</strong>: A constant added to shift when the neuron should activate</li>
</ul>
</div>
</div><div class="column">
<div id="fig-nnweights" class="quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-nnweights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="images/weights.svg" class="img-fluid">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nnweights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Weights in a neural network
</figcaption>
</figure>
</div>
</div></div>
<aside class="notes">
<p><span class="h4"><strong>Weight mechanics</strong></span></p>
<p><strong>Positive vs.&nbsp;negative weights:</strong></p>
<ul>
<li><strong>Positive weights</strong> act like “encouragers” - when the input neuron is active (high value), it pushes the receiving neuron toward activation</li>
<li><strong>Negative weights</strong> act like “inhibitors” - when the input neuron is active, it pushes the receiving neuron toward inactivity</li>
<li><strong>Zero weights</strong> mean the connection is effectively ignored</li>
</ul>
<p><strong>Weight magnitude:</strong></p>
<ul>
<li><strong>Large positive weights</strong> create strong encouraging connections</li>
<li><strong>Large negative weights</strong> create strong inhibitory connections<br>
</li>
<li><strong>Small weights</strong> (near zero) have minimal influence</li>
<li>The network learns which connections should be strong and which should be weak</li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>The mathematical foundation</strong></p>
</div>
<div class="callout-content">
<p>This weighted sum with bias is the fundamental computation in neural networks. The weights determine how much influence each input has on the output, while the bias determines the baseline level of activation.</p>
<p>The sigmoid function <span class="math inline">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span> serves as a “squashing” function that ensures the output stays between 0 and 1, regardless of how large or small the weighted sum becomes. This is crucial for maintaining the “activation” interpretation of neuron outputs. Other <em>activation functions</em> commonly used are tanh, relu, and leaky relu.</p>
<p>The <em>bias</em> is particularly important because it allows the neuron to fire even when all inputs are zero, or to require a higher threshold before firing. Without bias, neurons could only learn patterns that pass through the origin, severely limiting the network’s expressiveness.</p>
<p>Understanding this computation is key to grasping how neural networks work: each neuron computes a weighted combination of its inputs, adds a bias, and applies a nonlinear function to produce its output. This forms the basis of the <strong>backpropagation algorithm</strong> developed by <span class="citation" data-cites="rumelhart1986learning">Rumelhart, Hinton, and Williams (<a href="#/literature" role="doc-biblioref" onclick="">1986</a>)</span>.</p>
<p>This perspective - viewing neural networks as complex mathematical functions - is crucial for understanding their power and limitations. The <strong>Universal Approximation Theorem</strong> <span class="citation" data-cites="cybenko1989approximation hornik1989multilayer">(<a href="#/literature" role="doc-biblioref" onclick="">Cybenko 1989</a>; <a href="#/literature" role="doc-biblioref" onclick="">Hornik, Stinchcombe, and White 1989</a>)</span> tells us that neural networks with sufficient hidden units can approximate any continuous function to arbitrary accuracy.</p>
<p>The weights and biases represent the “knobs and dials” that can be adjusted to make the network compute any function we want (within the constraints of the architecture). Training is the process of finding the right setting for these parameters.</p>
<p>The power of neural networks comes from this massive number of adjustable parameters, which allows them to learn complex patterns in data. However, this also presents challenges: how do we find the right values for all these parameters? This is where the learning algorithms come in.</p>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="example-digit-recognition" class="title-slide slide level2">
<h2>Example: digit recognition</h2>
<div class="columns">
<div class="column">
<p>Example architecture for detecting digits of the MNIST dataset<a href="#/footnotes" class="footnote-ref" id="fnref1" role="doc-noteref" data-footnote-href="#/fn1" onclick=""><sup>1</sup></a>:</p>
<p><em>28×28 pixels</em> → <em>Neural Network</em> → <em>10 probabilities</em></p>
<div>
<ul>
<li class="fragment"><strong>Input layer:</strong> 784 neurons (28×28 pixels)<br>
Each neuron represents one pixel’s brightness (0.0 = black, 1.0 = white)</li>
<li class="fragment"><strong>Hidden layers:</strong> 2 layers, 16 neurons each<br>
These learn to detect patterns and features</li>
<li class="fragment"><strong>Output layer:</strong> 10 neurons Each represents confidence for digits 0-9</li>
</ul>
</div>
<p><a href="https://www.3blue1brown.com/lessons/neural-network-analysis" class="fragment small html-hidden">Let’s play a bit …</a></p>
</div><div class="column">
<div id="fig-MNIST-4" class="quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-MNIST-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="images/MNIST-4.png" class="img-fluid">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-MNIST-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Network architecture for digit recognition
</figcaption>
</figure>
</div>
</div></div>
<aside class="notes">
<p>The architecture of our digit recognition network represents a carefully designed pipeline for transforming raw pixel data into digit classifications. Let’s understand why this specific structure makes sense:</p>
<p><strong>Input layer (784 neurons):</strong></p>
<ul>
<li>Each neuron represents one pixel in the 28×28 image</li>
<li>Values range from 0 (black) to 1 (white), representing grayscale intensity</li>
<li>This layer doesn’t perform computation - it just holds the input data</li>
<li>784 inputs might seem like a lot, but images require this level of detail to preserve important patterns</li>
</ul>
<p><strong>Hidden layer 1 (16 neurons):</strong></p>
<ul>
<li>This is where the real pattern detection begins</li>
<li>Each of these 16 neurons receives input from all 784 pixels</li>
<li>With 784 inputs × 16 neurons = 12,544 weights (plus 16 biases)</li>
<li>These neurons learn to detect fundamental features like edges, curves, and basic shapes</li>
<li>16 neurons is relatively small - real networks often use hundreds or thousands</li>
</ul>
<p><strong>Hidden layer 2 (16 neurons):</strong></p>
<ul>
<li>Each neuron connects to all 16 neurons from the previous layer</li>
<li>16 inputs × 16 neurons = 256 weights (plus 16 biases)</li>
<li>These neurons combine the basic features into more complex patterns</li>
<li>They might detect things like “loop at top” or “vertical line on left”</li>
</ul>
<p><strong>Output layer (10 neurons):</strong></p>
<ul>
<li>One neuron for each possible digit (0, 1, 2, …, 9)</li>
<li>16 inputs × 10 neurons = 160 weights (plus 10 biases)</li>
<li>Each neuron’s activation represents the network’s confidence that the input image shows that particular digit</li>
<li>The highest activation typically indicates the network’s “guess”</li>
</ul>
<p><strong>Total Parameters:</strong></p>
<ul>
<li>Weights: 12,544 + 256 + 160 = 12,960</li>
<li>Biases: 16 + 16 + 10 = 42</li>
<li><strong>Total: 13,002 adjustable parameters</strong></li>
</ul>
<p>This seems like a lot, but it’s actually quite modest by modern standards. Large language models can have billions of parameters. The key insight is that all these parameters work together to create a flexible function that can map any 28×28 image to a probability distribution over the 10 digit classes.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside></aside></section>
</section>
<section>
<section id="learning" class="title-slide slide level1 headline-only">
<h1>Learning</h1>

</section>
<section id="the-learning-problem" class="title-slide slide level2">
<h2>The learning problem</h2>
<div class="medium">
<p><strong>Goal</strong>: Find the values of all <em>k</em> parameters that make the network classify digits correctly.</p>
</div>
<p><strong>Challenge</strong>: This is a <em>k</em>-dimensional optimization problem!<br>
<span class="smaller">(In our digit example it is 13,002-dimensional)</span></p>
<div class="fragment">
<p>We need a systematic way to:</p>
<div>
<ul>
<li class="fragment">Measure how “wrong” the network currently is</li>
<li class="fragment">Determine which parameters to adjust</li>
<li class="fragment">Make small improvements iteratively</li>
</ul>
</div>
<aside class="notes">
<p>Optimizing in 13,002 dimensions is conceptually challenging for humans to visualize, but mathematically tractable. Each dimension represents one parameter (weight or bias) in the network.</p>
<p>The challenge is immense: with 13,002 parameters, there are potentially infinite ways to set these values. Most combinations will perform poorly, and we need to find the tiny subset that actually works well for digit recognition.</p>
<p>Traditional optimization approaches (like trying random combinations or exhaustive search) would take longer than the age of the universe. We need smarter approaches that can navigate this high-dimensional space efficiently.</p>
<p>The key insight is that we can use calculus — specifically derivatives — to determine the direction of steepest improvement. This allows us to make educated guesses about how to adjust parameters rather than random exploration.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>

<section id="cost-functions" class="title-slide slide level2">
<h2>Cost functions</h2>
<div class="html-hidden">
<div class="medium">
<p>Let’s measure “wrongness”</p>
</div>
</div>
<p>For a single training example, if the network outputs <span class="math inline">\((a_0, a_1, ..., a_9)\)</span> but the correct answer is digit <span class="math inline">\(k\)</span>:</p>
<p><strong>Desired output</strong>: <span class="math inline">\((0, 0, ..., 1, ..., 0)\)</span> (1 in position <span class="math inline">\(k\)</span>, 0 elsewhere)</p>
<p><strong>Cost for this example</strong>:</p>
<p><span class="math inline">\(C = \sum_{j=0}^{9} (a_j - y_j)^2\)</span></p>
<p>where <span class="math inline">\(y_j\)</span> is the desired output for neuron <span class="math inline">\(j\)</span>.</p>
<aside class="notes">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Why squared differences?</strong></p>
</div>
<div class="callout-content">
<p>The squared error cost function has several nice properties:</p>
<ol type="1">
<li><strong>Always positive</strong>: Squared terms ensure the cost is never negative</li>
<li><strong>Smooth and differentiable</strong>: We can compute gradients needed for optimization</li>
<li><strong>Penalizes large errors more</strong>: A network that’s very wrong gets penalized more than one that’s slightly wrong</li>
<li><strong>Zero when perfect</strong>: Cost is exactly 0 when the network output matches the desired output perfectly</li>
</ol>
<p>For digit recognition, if the correct answer is “3”, we want:</p>
<ul>
<li>Output neuron 3 to have activation close to 1.0</li>
<li>All other output neurons to have activation close to 0.0</li>
</ul>
<p>The cost function measures how far we are from this ideal. When the network is confident and correct, the cost is low. When the network is uncertain or wrong, the cost is high.</p>
<p>Alternative cost functions exist (like cross-entropy), but squared error is conceptually simpler and works well for educational purposes.</p>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="gradient-descent" class="title-slide slide level2">
<h2>Gradient descent</h2>
<div class="medium">
<p><strong>Intuition</strong>: Imagine the cost function as a landscape with hills and valleys. We want to find the lowest valley (minimum cost).</p>
</div>
<div class="fragment">
<p><strong>Gradient descent algorithm</strong>:</p>
<div>
<ol type="1">
<li class="fragment">Compute the <strong>gradient</strong> (direction of steepest increase in cost)</li>
<li class="fragment">Move in the <strong>opposite direction</strong> (direction of steepest decrease)</li>
<li class="fragment">Take small steps to avoid overshooting</li>
<li class="fragment">Repeat until you reach a minimum</li>
</ol>
</div>
<aside class="notes">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>The geography of optimization</strong></p>
</div>
<div class="callout-content">
<p>The landscape metaphor is powerful but limited. In 13,002 dimensions, we can’t visualize the actual landscape, but the mathematical principles remain the same.</p>
<p>Key insights about gradient descent:</p>
<ol type="1">
<li><p><strong>Local vs global minima</strong>: Like a real landscape, the cost function may have multiple valleys. Gradient descent finds a local minimum (nearby valley) but might miss the global minimum (deepest valley overall).</p></li>
<li><p><strong>Learning rate</strong>: This is a crucial hyperparameter:</p>
<ul>
<li>Too large: We might overshoot and oscillate around the minimum</li>
<li>Too small: Progress is very slow, and we might get stuck</li>
<li>Just right: Steady progress toward a minimum</li>
</ul></li>
<li><p><strong>High-dimensional intuition</strong>: In high dimensions, most points are neither maxima nor minima, but saddle points. This actually helps optimization because there are usually many directions that lead downhill.</p></li>
<li><p><strong>Why it works</strong>: Even though we can’t visualize 13,002-dimensional space, the mathematical guarantee is that moving in the negative gradient direction will decrease the cost (at least for small steps).</p></li>
</ol>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>

<section id="backpropagation" class="title-slide slide level2">
<h2>Backpropagation</h2>
<div class="medium">
<p><strong>Challenge</strong>: How do we compute the gradient of the cost function with respect to all <em>k</em> parameters efficiently?</p>
</div>
<div class="fragment">
<p><strong>Backpropagation algorithm</strong>:</p>
<div>
<ol type="1">
<li class="fragment"><strong>Forward pass</strong>: Run the network on a training example to get predictions</li>
<li class="fragment"><strong>Compute cost</strong>: Compare predictions to correct answers</li>
<li class="fragment"><strong>Backward pass</strong>: Use the chain rule<a href="#/footnotes" class="footnote-ref" id="fnref2" role="doc-noteref" data-footnote-href="#/fn2" onclick=""><sup>2</sup></a> to compute how each parameter affects the cost</li>
<li class="fragment"><strong>Update parameters</strong>: Adjust each parameter in the direction that reduces cost</li>
</ol>
</div>
</div>
<div class="fragment">
<div class="smaller">
<p>This elegant algorithm, formalized by <span class="citation" data-cites="rumelhart1986learning">Rumelhart, Hinton, and Williams (<a href="#/literature" role="doc-biblioref" onclick="">1986</a>)</span>, makes training deep networks computationally feasible <span class="citation" data-cites="sanderson2017backprop">(<a href="#/literature" role="doc-biblioref" onclick="">Sanderson 2017b</a>)</span>.</p>
</div>
<aside class="notes">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>The mathematical elegance of backpropagation</strong></p>
</div>
<div class="callout-content">
<p>Backpropagation is essentially an efficient application of the chain rule from calculus. The key insight is that we can compute gradients by working backwards through the network.</p>
<p><strong>Forward Pass Example</strong>: Input → Layer 1 → Layer 2 → Output → Cost</p>
<p><strong>Backward Pass</strong>: Cost → ∂Cost/∂Output → ∂Cost/∂Layer2 → ∂Cost/∂Layer1 → ∂Cost/∂Weights</p>
<p>For each parameter, we ask: “If I change this parameter by a tiny amount, how much does the cost change?” The chain rule lets us compute this efficiently by decomposing the influence into steps.</p>
<p>Think of it as tracing cause and effect:</p>
<ul>
<li>How did weight <em>W</em> affect neuron <em>N</em>?</li>
<li>How did neuron <em>N</em> affect the layer’s output?</li>
<li>How did the layer’s output affect the final prediction?</li>
<li>How did the final prediction contribute to the error?</li>
</ul>
<p><strong>Why “Backpropagation”?</strong>: We propagate the error backwards through the network. Starting from the final cost, we compute how much each layer contributed to that cost, then how much each neuron contributed, and finally how much each weight contributed.</p>
<p>This algorithm is remarkably efficient: computing the gradient for all parameters takes roughly the same computational time as computing the network’s output itself. This efficiency made training deep networks practical <span class="citation" data-cites="sanderson2017backprop">(<a href="#/literature" role="doc-biblioref" onclick="">Sanderson 2017b</a>)</span>.</p>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
<aside></aside></section>

<section id="learning-loop" class="title-slide slide level2">
<h2>Learning loop</h2>
<div class="medium">
<div>
<ol type="1">
<li class="fragment">Start with random weights</li>
<li class="fragment">Make a prediction (forward pass)</li>
<li class="fragment">Measure the error</li>
<li class="fragment">Trace back to find responsible weights (backpropagation)</li>
<li class="fragment">Adjust weights to reduce error</li>
<li class="fragment">Repeat with the next example</li>
</ol>
</div>
</div>
<div class="fragment">
<p>Through millions cycles, the network gradually learns to recognize even complex patterns.</p>
<aside class="notes">
<p>The remarkable thing is that complex behaviors (like recognizing handwriting) emerge from this simple process of error correction.</p>
<p>This transformation from random guesses to intelligent recognition happens purely through this iterative process of prediction, error measurement, and weight adjustment. No human explicitly programs the features - the network discovers these patterns automatically through experience.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>

<section id="using-mini-batches-for-training" class="title-slide slide level2">
<h2>Using mini-batches for training</h2>
<aside class="notes">
<p>There are three main approaches to gradient descent:</p>
<p><strong>Batch Gradient Descent</strong>: Use all training examples to compute gradient</p>
<ul>
<li>Pros: Most accurate gradient estimate</li>
<li>Cons: Very slow for large datasets, memory intensive</li>
</ul>
<p><strong>Stochastic Gradient Descent (SGD)</strong>: Use one example at a time</p>
<ul>
<li>Pros: Fast updates, can escape local minima due to noise</li>
<li>Cons: Very noisy, unstable convergence</li>
</ul>
<p><strong>Mini-batch SGD</strong>: Use small batches (typically 16-256 examples)</p>
<ul>
<li>Pros: Good balance of speed and stability</li>
<li>Cons: Requires tuning batch size</li>
</ul>
<p>Mini-batches provide several advantages:</p>
<ul>
<li><strong>Computational efficiency</strong>: Modern hardware (GPUs) is optimized for parallel processing of batches</li>
<li><strong>Better gradient estimates</strong>: Averaging over multiple examples reduces noise</li>
<li><strong>Memory efficiency</strong>: Process data in chunks rather than loading everything</li>
<li><strong>Regularization effect</strong>: The noise from mini-batching can help escape poor local minima</li>
</ul>
<p>The choice of batch size is another hyperparameter that affects training dynamics and final performance <span class="citation" data-cites="sanderson2017gradient">(<a href="#/literature" role="doc-biblioref" onclick="">Sanderson 2017a</a>)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<p><strong>Mini-batch stochastic gradient descent</strong>:</p>
<div>
<ul>
<li class="fragment"><strong>Shuffle</strong> the training data randomly</li>
<li class="fragment"><strong>Divide</strong> into small batches (e.g., 32 examples per batch)</li>
<li class="fragment">For each batch:
<ul>
<li class="fragment">Compute gradients for all examples in the batch</li>
<li class="fragment"><strong>Average</strong> the gradients across the batch</li>
<li class="fragment"><strong>Update</strong> parameters using the averaged gradient</li>
</ul></li>
<li class="fragment"><strong>Repeat</strong> for many epochs<a href="#/footnotes" class="footnote-ref" id="fnref3" role="doc-noteref" data-footnote-href="#/fn3" onclick=""><sup>3</sup></a></li>
</ul>
</div>
<aside></aside></section>

<section id="key-insights" class="title-slide slide level2">
<h2>Key insights</h2>
<div class="html-hidden">
<p>Neural networks excel at</p>
<div class="medium">
<div>
<ol type="1">
<li class="fragment"><strong>Pattern recognition</strong> in high-dimensional data<br>
</li>
<li class="fragment"><strong>Learning from examples</strong> rather than explicit rules<br>
</li>
<li class="fragment"><strong>Handling noisy, imperfect data</strong><br>
</li>
<li class="fragment"><strong>Scaling to massive datasets</strong><br>
</li>
<li class="fragment"><strong>Adapting to new data</strong> through retraining</li>
</ol>
</div>
</div>
</div>
<aside class="notes">
<ol type="1">
<li>Neural networks excel when the data has many features and complex relationships between them (e.g., images, text, customer behavior, financial markets).</li>
<li>Neural networks can find patterns in this complexity that would be impossible to detect manually or with simpler algorithms.</li>
<li>Neural networks are remarkably robust to noisy, imperfect data (e.g., missing values, measurement errors, outliers) because they learn statistical patterns rather than requiring perfect data.</li>
<li>Neural networks often improve with more data, unlike many traditional methods that plateau.</li>
<li>Business environments change constantly. Neural networks can be retrained on new data to.</li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="discussion-2" class="title-slide slide level2 html-hidden discussion-slide unlisted" data-background-color="#efefef">
<h2>Discussion</h2>
<div class="larger">
<p>We’ve learned how neural networks can recognize digits. How might we <strong>extend this approach</strong> to understand and generate <strong>language</strong>?</p>
</div>
</section>
</section>
<section>
<section id="transformers" class="title-slide slide level1 headline-only">
<h1>Transformers</h1>
<div class="medium">
<p>From images to language</p>
</div>
</section>
<section id="the-challenge" class="title-slide slide level2">
<h2>The challenge</h2>
<p>Key differences between images and text:</p>
<div>
<ul>
<li class="fragment"><strong>Images</strong> has <em>fixed size</em> (e.g., 28×28 pixels) and <em>spatial relationships</em> matter</li>
<li class="fragment"><strong>Text</strong> has <em>variable length</em>, <em>sequential relationships</em> matter, and <em>context</em> is crucial</li>
<li class="fragment"><strong>Word meaning</strong> depends heavily on surrounding words
<ul>
<li class="fragment">“The bank was flooded” vs “I went to the bank”</li>
<li class="fragment">“model” in “machine learning model” vs “fashion model”</li>
</ul></li>
</ul>
</div>
<div class="fragment">
<div class="medium">
<p>We need architectures designed specifically for <strong>sequential data</strong> with <strong>long-range dependencies</strong>.</p>
</div>
<aside class="notes">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Standard neural networks x language</strong></p>
</div>
<div class="callout-content">
<p>Standard neural networks, like our digit classifier, have limitations for language:</p>
<ol type="1">
<li><strong>Fixed Input Size</strong>: Traditional networks expect fixed-size inputs, but sentences have varying lengths</li>
<li><strong>No Sequential Understanding</strong>: Standard networks treat input positions independently - they can’t understand that word order matters</li>
<li><strong>No Long-Range Dependencies</strong>: Information from early in a sentence might be crucial for understanding words much later</li>
</ol>
<p>Early attempts to solve this included:</p>
<ul>
<li><strong>Recurrent Neural Networks (RNNs)</strong>: Process sequences one word at a time, but suffer from vanishing gradients for long sequences</li>
<li><strong>Convolutional Networks</strong>: Good for local patterns but struggle with long-range dependencies</li>
<li><strong>LSTM/GRU</strong>: Better than RNNs but still fundamentally sequential and slow to train</li>
</ul>
<p>The breakthrough came with <strong>Transformers</strong> <span class="citation" data-cites="vaswani2017attention">(<a href="#/literature" role="doc-biblioref" onclick="">Vaswani et al. 2017</a>)</span>, which solved these problems through a fundamentally different approach: <strong>attention mechanisms</strong> that allow every word to directly interact with every other word in the sequence.</p>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>

<section id="transformers-how-llms-work" class="title-slide slide level2 html-hidden">
<h2>Transformers (how LLMs work)</h2>
<iframe width="800" height="450" src="https://www.youtube.com/embed/wjZofJX0v4M?si=1G9AAspb6KBZ3pkv" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
</iframe>
</section>

<section id="what-is-a-transformer" class="title-slide slide level2">
<h2>What is a Transformer?</h2>
<div class="medium">
<p>A transformer is a neural network architecture specifically designed for processing sequences.</p>
</div>
<div class="fragment">
<p>The <strong>attention mechanism</strong> is the key innovation — it allows every element in the sequence to “attend to” every other element.</p>
<aside class="notes">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Transformer architecture</strong></p>
</div>
<div class="callout-content">
<p>The Transformer architecture, introduced in the landmark 2017 paper “Attention Is All You Need” <span class="citation" data-cites="vaswani2017attention">(<a href="#/literature" role="doc-biblioref" onclick="">Vaswani et al. 2017</a>)</span>, revolutionized natural language processing. The key insight was that attention mechanisms could replace recurrent and convolutional layers entirely.</p>
<p>Before transformers, most language models were based on RNNs or CNNs, which processed sequences step-by-step or with limited context windows. This made them slow to train and limited in their ability to capture long-range dependencies.</p>
<p>The attention mechanism allows for:</p>
<ul>
<li><strong>Parallel processing</strong>: All positions in a sequence can be processed simultaneously</li>
<li><strong>Long-range dependencies</strong>: Any word can directly attend to any other word, regardless of distance</li>
<li><strong>Interpretability</strong>: We can visualize what the model is “paying attention to”</li>
</ul>
<p>The impact has been enormous:</p>
<ul>
<li>GPT (Generative Pre-trained Transformer) family: GPT-1, GPT-2, GPT-3, GPT-4</li>
<li>BERT: Bidirectional transformer for understanding tasks</li>
<li>T5: Text-to-text transfer transformer</li>
<li>and hundreds of other transformer-based models</li>
</ul>
<p>The name “Transformer” comes from its ability to transform input sequences into output sequences through the attention mechanism.</p>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>

<section id="context-is-everything" class="title-slide slide level2">
<h2>Context is everything</h2>
<p>Consider these sentences:</p>
<ul>
<li>“The <em>tower</em> was very tall”</li>
<li>“The <em>Eiffel tower</em> was very tall”</li>
</ul>
<div class="fragment">
<p>The word “tower” should mean different things in different contexts:</p>
<ul>
<li>First case: Generic tower</li>
<li>Second case: Specific famous landmark in Paris</li>
</ul>
</div>
<div class="fragment">
<p><strong>Attention mechanism</strong> allow context words to <em>update</em> the meaning of other words.</p>
</div>
</section>

<section id="tokens-and-embeddings" class="title-slide slide level2">
<h2>Tokens and embeddings</h2>
<p><strong>Tokenization</strong> means that text is broken down into small chunks called <em>tokens</em> — a crucial preprocessing step that bridges human language and machine processing.</p>
<ul>
<li>“To date, the cleverest thinker of all time was…”</li>
<li>Becomes: [“To”, “date”, “,”, “the”, “cle”, “ve”, “rest”, “thinker”, “of”, “all”, “time”, “was”, “…”]</li>
</ul>
<div class="fragment">
<p>Each token gets converted to a <em>high-dimensional vector</em> (e.g., 12,288 dimensions for GPT-3) — so called <strong>embedding vectors</strong></p>
<ul>
<li>Similar tokens get <strong>similar vectors</strong></li>
<li>These vectors capture <strong>semantic meaning</strong></li>
</ul>
</div>
<div class="fragment">
<p>This vector representation is what the transformer actually processes - it never sees raw text, only these numerical vectors <span class="citation" data-cites="sanderson2024gpt">(<a href="#/literature" role="doc-biblioref" onclick="">Sanderson 2024a</a>)</span>.</p>
</div>
</section>

<section id="word-embeddings" class="title-slide slide level2">
<h2>Word Embeddings</h2>
<div class="medium">
<p>Directions in embedding space can encode <strong>semantic relationships</strong>.</p>
</div>
<div class="fragment">
<p><strong>Examples</strong>:</p>
<ul>
<li>Gender direction: <em>“king” - “man” + “woman” ≈ “queen”</em></li>
<li>Plurality direction: <em>“cat” - “cats”</em> captures singular vs plural</li>
<li>Country-capital: <em>“Germany” - “Berlin” + “France” ≈ “Paris”</em></li>
</ul>
</div>
<div class="fragment">
<p>The embedding layer learns to place semantically related words <strong>close together</strong> in the vector space.</p>
<aside class="notes">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>The geometry of meaning</strong></p>
</div>
<div class="callout-content">
<p>Word embeddings reveal that meaning has geometric structure. This isn’t just a mathematical curiosity - it reflects how language itself is structured:</p>
<p><strong>Analogical reasoning</strong> — the famous “king - man + woman = queen” example shows that semantic relationships can be captured as vector operations. This suggests that certain directions in the embedding space consistently encode specific semantic properties.</p>
<p><strong>Semantic clusters</strong> — words with similar meanings cluster together:</p>
<ul>
<li>Animals: “dog”, “cat”, “horse” are close to each other</li>
<li>Colors: “red”, “blue”, “green” form another cluster<br>
</li>
<li>Countries: “France”, “Germany”, “Italy” cluster together</li>
</ul>
<p><strong>Hierarchical structure</strong> — the space can capture hierarchies:</p>
<ul>
<li>“Animal” might be close to “Dog”, “Cat”, etc.</li>
<li>“Mammal” might be between “Animal” and “Dog”</li>
</ul>
<p><strong>Cultural and linguistic biases</strong> — embeddings can capture societal biases present in training data:</p>
<ul>
<li>Occupational gender stereotypes</li>
<li>Racial or cultural associations</li>
<li>This is both a feature (capturing human-like associations) and a bug (perpetuating unfair biases)</li>
</ul>
<p><strong>Training process</strong> — These embeddings aren’t hand-crafted but learned from data. The model discovers these geometric relationships by seeing how words are used together in context <span class="citation" data-cites="sanderson2024gpt">(<a href="#/literature" role="doc-biblioref" onclick="">Sanderson 2024a</a>)</span>.</p>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>

<section id="attention" class="title-slide slide level2">
<h2>Attention</h2>
<div class="medium">
<p>Rather than having fixed embeddings for each word, attention allows the embedding to be <strong>dynamically updated</strong> based on what other words are present in the context. This creates <strong>context-sensitive representations</strong> that can capture these nuanced meanings.</p>
</div>

<aside><div>
<p><span class="citation" data-cites="sanderson2024attention">(<a href="#/literature" role="doc-biblioref" onclick="">Sanderson 2024b</a>)</span></p>
</div></aside></section>
<section id="single-head-attention" class="slide level3">
<h3>Single-head attention</h3>
<div class="medium">
<p><strong>Goal</strong>: Update the embedding of some word on the context of that word.</p>
</div>
<div class="fragment">
<p><strong>Three key matrices</strong> (learned during training)<a href="#/footnotes" class="footnote-ref" id="fnref4" role="doc-noteref" data-footnote-href="#/fn4" onclick=""><sup>4</sup></a>:</p>
<ul>
<li><strong>Query matrix</strong> <span class="math inline">\(W_Q\)</span> indicates what types of context each word typically needs</li>
<li><strong>Key matrix</strong> <span class="math inline">\(W_K\)</span> indicates what types of context each word can provide</li>
<li><strong>Value matrix</strong> <span class="math inline">\(W_V\)</span> indicates what information to actually pass</li>
</ul>
</div>
<div class="fragment">
<p><strong>Process</strong>:</p>
<ol type="1">
<li>Compute <strong>attention scores</strong> between words</li>
<li>Create <strong>weighted combinations</strong> of information</li>
<li><strong>Update</strong> embeddings based on relevant context</li>
</ol>
</div>
<aside></aside></section>
<section id="example" class="slide level3">
<h3>Example</h3>
<aside class="notes">
<p>Let’s trace how attention helps resolve the ambiguity of “bank” (financial institution vs.&nbsp;riverbank).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<p>The target word is “bank” (needs contextual disambiguation), the context word is “flooded”</p>
<div class="fragment">
<p><span class="h4"><strong>The attention process</strong></span></p>
<div>
<ul>
<li class="fragment">Step 1: <strong>attention score:</strong><br>
“bank’s” query vector × “flooded’s” key vector = high similarity score<br>
(the model has learned that water-related words are highly relevant for disambiguating “bank”)</li>
<li class="fragment">Step 2: <strong>weighted information:</strong><br>
high attention score × “flooded’s” value vector = strong water/geography signal</li>
<li class="fragment">Step 3: <strong>contextualized embedding:</strong><br>
original “bank” embedding + weighted “flooded” information = “riverbank” meaning</li>
</ul>
</div>
</div>
<div class="fragment">
<p>The ambiguity is resolved: we’re talking about a riverbank, not a financial institution</p>
</div>
</section>
<section id="multi-head-attention" class="slide level3">
<h3>Multi-head attention</h3>
<p>In reality different types of relationships matter simultaneously, such as</p>
<div>
<ul>
<li class="fragment"><strong>Head 1</strong> might focus on <strong>grammatical relationships</strong> (subject-verb agreement)</li>
<li class="fragment"><strong>Head 2</strong> might focus on <strong>semantic relationships</strong> (synonyms, antonyms)</li>
<li class="fragment"><strong>Head 3</strong> might focus on <strong>coreference</strong> (pronouns to their referents)</li>
<li class="fragment"><strong>Head 4</strong> might focus on <strong>long-range dependencies</strong> (cause and effect)</li>
</ul>
</div>
<div class="fragment">
<p>Each head learns to specialize in different types of patterns and relationships.</p>
<div class="small fragment">
<p><strong>GPT-3 example</strong>: 96 attention heads per layer × 96 layers = <strong>9,216 total attention heads</strong></p>
</div>
</div>
</section>

<section id="feed-forward-networks-ffn" class="title-slide slide level2">
<h2>Feed-forward networks (FFN)</h2>
<div class="medium">
<p>After attention, each token passes through a FFN.</p>
</div>
<div class="fragment">
<p>FFNs are the “thinking” components that sit between attention layers in transformers. While attention figures out what information to gather, FFNs decide what to do with that information.</p>
</div>
<div class="fragment">
<p>Example:</p>
<ol type="1">
<li><strong>Attention:</strong> <em>Given ‘bank’ and ‘flooded,’ I should focus on the flooding information</em></li>
<li><strong>FFN:</strong> <em>Now that I know this is a flooded riverbank, I should activate concepts related to environmental damage and strengthen connections to geographic features</em></li>
</ol>
<aside class="notes">
<p>Following residual connections and layer normalization make deep transformers stable and trainable.</p>
<p>A residual connection means you add the input back to the output of a layer:</p>
<p><code>output = Layer(input) + input</code></p>
<p>Or more specific:</p>
<ul>
<li>After attention layer: <code>contextualized_embedding = attention(original_embedding) + original_embedding</code></li>
<li>After FFN layer: <code>final_output = ffn(contextualized_embedding) + contextualized_embedding</code></li>
</ul>
<p>Without residual connections: Information can get “lost” or distorted as it passes through many layers With residual connections: The original information is always preserved and combined with the processed version.</p>
<p>Layer normalization standardizes the values within each embedding vector to have mean close to 0 and standard deviation close to 1.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>

<section id="unembedding" class="title-slide slide level2">
<h2>Unembedding</h2>
<div class="medium">
<p>From vectors back to text.</p>
</div>
<div class="fragment">
<p>The unembedding process is how transformers convert their internal vector representations back into text predictions. It’s the crucial final step that makes language generation possible.</p>
<div class="columns">
<div class="column">
<p><span class="h4"><strong>Process</strong></span></p>
<div>
<ul>
<li class="fragment"><strong>Unembedding matrix</strong> <span class="math inline">\(W_U\)</span> maps from embedding dimension<a href="#/footnotes" class="footnote-ref" id="fnref5" role="doc-noteref" data-footnote-href="#/fn5" onclick=""><sup>5</sup></a> to vocabulary size<a href="#/footnotes" class="footnote-ref" id="fnref6" role="doc-noteref" data-footnote-href="#/fn6" onclick=""><sup>6</sup></a>
<ul>
<li class="fragment">One row per token in the vocabulary</li>
<li class="fragment">Produces raw scores possible next tokens</li>
</ul></li>
<li class="fragment"><strong>Softmax function</strong> converts raw scores to probability distribution</li>
<li class="fragment"><strong>Temperature</strong>: Controls randomness in sampling<a href="#/footnotes" class="footnote-ref" id="fnref7" role="doc-noteref" data-footnote-href="#/fn7" onclick=""><sup>7</sup></a></li>
</ul>
</div>
</div><div class="column">
<div class="fragment">
<p><span class="h4"><strong>Example</strong></span></p>
<ol type="1">
<li><strong>Context processing:</strong> “The capital of France is” → final vector</li>
<li><strong>Unembedding:</strong> Vector × W_U → raw scores for all 50,257 tokens</li>
<li><strong>Temperature scaling:</strong> Divide scores by temperature</li>
<li><strong>Softmax:</strong> Convert to probability distribution</li>
<li><strong>Sampling:</strong> Choose next token based on probabilities</li>
</ol>
</div>
</div></div>
</div>
<aside></aside></section>

<section id="transformer-architecture-overview" class="title-slide slide level2">
<h2>Transformer architecture overview</h2>
<div class="medium">
<p><strong>Key principle</strong>: Information flows through many layers of attention and processing (i.e., built through deep learning), allowing complex reasoning to emerge.</p>
</div>
<div>
<ol type="1">
<li class="fragment"><strong>Tokenization + embedding</strong>: Text → Vectors<br>
</li>
<li class="fragment"><strong>Attention blocks</strong>: Vectors communicate and update based on context</li>
<li class="fragment"><strong>Feed-forward layers</strong>: Independent processing of each vector</li>
<li class="fragment"><strong>Many layers</strong>: Alternate attention and feed-forward (e.g., 96 layers in GPT-3)</li>
<li class="fragment"><strong>Unembedding</strong>: Final vector → Probability distribution over next tokens</li>
</ol>
</div>
</section>
</section>
<section>
<section id="training" class="title-slide slide level1 headline-only">
<h1>Training</h1>

</section>
<section id="training-process" class="title-slide slide level2">
<h2>Training process</h2>
<div class="medium">
<p>No explicit labels needed — the text itself provides the training signal.</p>
</div>
<p>Next-token prediction seems simple but is remarkably powerful <span class="citation" data-cites="radford2019language">(<a href="#/literature" role="doc-biblioref" onclick="">Radford et al. 2019</a>)</span>:</p>
<div>
<ul>
<li class="fragment"><strong>Implicit learnings</strong> comprise grammar, facts, reasoning, coding and patterns</li>
<li class="fragment">More training data exposes the model to more patterns and knowledge (<strong>scale effects</strong>)</li>
<li class="fragment">More training time allows better optimization of the massive parameter space</li>
<li class="fragment">Training requires immense training infrastructure (GPT-3 training cost ~$4.6 million in compute)</li>
</ul>
</div>
</section>

<section id="emergent-capabilities" class="title-slide slide level2">
<h2>Emergent capabilities</h2>
<p>As models scale up, they develop capabilities that weren’t explicitly programmed:</p>
<div>
<ul>
<li class="fragment"><strong>Few-shot learning</strong>: Learn new tasks from just a few examples</li>
<li class="fragment"><strong>Chain-of-thought reasoning</strong>: Break complex problems into steps</li>
<li class="fragment"><strong>Code generation</strong>: Write and debug programs</li>
<li class="fragment"><strong>Mathematical reasoning</strong>: Solve word problems and equations</li>
<li class="fragment"><strong>Creative writing</strong>: Generate stories, poems, and scripts</li>
<li class="fragment"><strong>Instruction following</strong>: Understand and execute complex commands</li>
</ul>
</div>
<div class="fragment">
<p>Complex intelligence seem to emerge from the simple objective of predicting the next word.</p>
</div>
</section>

<section id="limitations-and-challenges" class="title-slide slide level2">
<h2>Limitations and Challenges</h2>
<div class="medium">
<p>Despite their impressive capabilities, current language models have significant limitations:</p>
</div>
<div>
<ul>
<li class="fragment"><strong>Hallucination</strong>: Generate plausible-sounding but false information</li>
<li class="fragment"><strong>Lack of true understanding</strong>: May memorize patterns without genuine comprehension</li>
<li class="fragment"><strong>Inconsistency</strong>: May give different answers to the same question</li>
<li class="fragment"><strong>Training data bias</strong>: Reflect biases present in internet text</li>
<li class="fragment"><strong>No learning from interaction</strong>: Can’t update their knowledge from conversations</li>
<li class="fragment"><strong>Computational requirements</strong>: Expensive to train and run</li>
</ul>
</div>
</section>

<section id="further-reads" class="title-slide slide level2">
<h2>Further reads</h2>
<p>Please check the resources provided by 3Blue1Brown on <a href="https://www.3blue1brown.com/topics/neural-networks">the basics of neural networks, and the math behind how they learn</a>.</p>
</section>

<section id="discussion-3" class="title-slide slide level2 html-hidden discussion-slide no-headline unlisted">
<h2>Discussion</h2>
<div class="larger">
<p>Given what we’ve learned about neural networks and transformers, what do you think are the <strong>most important challenges</strong> we need to solve to make AI systems more <strong>reliable and beneficial</strong>?</p>
</div>
</section>
</section>
<section>
<section id="exercises" class="title-slide slide level1 headline-only">
<h1>Exercises</h1>

</section>
<section id="neural-network-architecture" class="title-slide slide level2">
<h2>Neural network architecture</h2>
<p>Design a neural network for classifying emails as spam or not spam. Specify:</p>
<ol type="1">
<li><strong>Input representation</strong>: How would you convert an email into numbers?</li>
<li><strong>Output</strong>: How would you interpret the network’s output?</li>
<li><strong>Training data</strong>: What kind of examples would you need?</li>
</ol>
<p>Discuss the advantages and challenges of this approach compared to rule-based spam filtering.</p>
<aside class="notes">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Solution notes</strong></p>
</div>
<div class="callout-content">
<p><strong>Input representation options</strong></p>
<ul>
<li><strong>Bag of words</strong>: Count frequency of each word in vocabulary (e.g., 10,000 input neurons)</li>
<li><strong>TF-IDF</strong>: Weight word frequencies by inverse document frequency</li>
<li><strong>Word embeddings</strong>: Use pre-trained embeddings and average/pool them</li>
<li><strong>Character-level</strong>: Represent emails as sequences of characters</li>
</ul>
<p><strong>Output interpretation</strong></p>
<ul>
<li>Single output neuron with sigmoid activation</li>
<li>Value close to 1 = spam, close to 0 = not spam</li>
<li>Use threshold (e.g., 0.5) for binary classification</li>
</ul>
<p><strong>Training data requirements</strong></p>
<ul>
<li>Thousands of labeled emails (spam/not spam)</li>
<li>Balanced dataset or careful handling of class imbalance</li>
<li>Diverse examples covering different types of spam</li>
<li>Regular updates as spam techniques evolve</li>
</ul>
<p><strong>Advantages over rules</strong></p>
<ul>
<li>Automatically learns patterns from data</li>
<li>Adapts to new spam techniques</li>
<li>Can detect subtle combinations of features</li>
<li>Less manual maintenance required</li>
</ul>
<p><strong>Challenges</strong></p>
<ul>
<li>Requires large labeled datasets</li>
<li>Can be fooled by adversarial examples</li>
<li>Black box - hard to understand why decisions are made</li>
<li>May learn biases from training data</li>
</ul>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="attention-mechanism" class="title-slide slide level2">
<h2>Attention mechanism</h2>
<p>Consider the sentence: “The red car that John bought yesterday broke down on the highway.”</p>
<ol type="1">
<li><strong>Identify relationships</strong>: What words should attend to each other strongly?</li>
<li><strong>Multiple heads</strong>: Design 3 different attention heads that focus on different types of relationships.</li>
<li><strong>Context update</strong>: How should the embedding of “car” change after processing this sentence?</li>
</ol>
<aside class="notes">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Solution notes</strong></p>
</div>
<div class="callout-content">
<p><strong>Strong attention relationships</strong></p>
<ul>
<li>“red” → “car” (adjective modifies noun)</li>
<li>“car” → “broke” (subject-verb relationship)</li>
<li>“that” → “car” (relative pronoun reference)</li>
<li>“John” → “bought” (subject-verb)</li>
<li>“bought” → “car” (verb-object)</li>
<li>“yesterday” → “bought” (temporal modifier)</li>
<li>“broke” → “highway” (location context)</li>
</ul>
<p><strong>Three attention head types</strong></p>
<ul>
<li>Grammatical relationships
<ul>
<li>Focus on syntactic dependencies</li>
<li>“car” attends to “broke” (subject-verb)</li>
<li>“John” attends to “bought” (subject-verb)</li>
<li>Helps with grammatical consistency</li>
</ul></li>
<li>Modification relationships
<ul>
<li>Focus on descriptive relationships</li>
<li>“red” attends to “car”</li>
<li>“yesterday” attends to “bought”</li>
<li>Captures qualitative and temporal information Coreference and long-range</li>
<li>Focus on pronoun resolution and distant relationships</li>
<li>“that” attends to “car”</li>
<li>“broke” attends back to “car” (long-range subject)</li>
<li>Handles complex sentence structure</li>
</ul></li>
</ul>
<p><strong>Car embedding updates</strong></p>
<ul>
<li><strong>Initial</strong>: Generic car concept</li>
<li><strong>After “red”</strong>: Specific colored vehicle</li>
<li><strong>After “John bought”</strong>: Particular car owned by John</li>
<li><strong>After “yesterday”</strong>: Recently purchased car</li>
<li><strong>After “broke”</strong>: Problematic/unreliable vehicle</li>
<li><strong>Final representation</strong>: John’s recently-purchased red car with reliability issues</li>
</ul>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="transformer-training" class="title-slide slide level2">
<h2>Transformer training</h2>
<p>You’re training a small transformer to complete simple mathematical expressions like “2 + 3 = ?”</p>
<ol type="1">
<li><strong>Tokenization</strong>: How would you represent mathematical expressions as tokens?</li>
<li><strong>Training objective</strong>: What would be your training data and loss function?</li>
<li><strong>Challenges</strong>: What difficulties might arise, and how would you address them?</li>
<li><strong>Evaluation</strong>: How would you test if the model truly “understands” arithmetic?</li>
</ol>
<aside class="notes">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Solution notes</strong></p>
</div>
<div class="callout-content">
<p><strong>Tokenization strategies</strong></p>
<ul>
<li><strong>Character-level</strong>: [‘2’, ‘+’, ‘3’, ‘=’, ‘?’] - simple but may struggle with multi-digit numbers</li>
<li><strong>Number tokens</strong>: [‘2’, ‘+’, ‘3’, ‘=’, ‘?’] - treat each number as atomic token</li>
<li><strong>BPE encoding</strong>: Learn subword patterns for larger numbers</li>
<li><strong>Special tokens</strong>: [NUM_2, OP_PLUS, NUM_3, OP_EQUALS, MASK]</li>
</ul>
<p><strong>Training data and objective</strong></p>
<ul>
<li><strong>Data generation</strong>: Automatically generate arithmetic problems
<ul>
<li>Simple: “1 + 1 = 2”, “5 - 3 = 2”</li>
<li>Complex: “12 × 7 = 84”, “100 ÷ 4 = 25”</li>
</ul></li>
<li><strong>Objective</strong>: Next token prediction
<ul>
<li>Input: “2 + 3 =”</li>
<li>Target: “5”</li>
</ul></li>
<li><strong>Loss function</strong>: Cross-entropy loss on predicted vs.&nbsp;true next token</li>
</ul>
<p><strong>Challenges and solutions</strong></p>
<ul>
<li><strong>Out-of-distribution numbers</strong>: Train on wide range, test generalization</li>
<li><strong>Order of operations</strong>: Include parentheses: “(2 + 3) × 4 = 20”</li>
<li><strong>Digit-by-digit vs.&nbsp;holistic</strong>:
<ul>
<li>Problem: Might predict “1” then “2” for “12” without understanding the full number</li>
<li>Solution: Use single tokens for numbers or special training techniques</li>
</ul></li>
<li><strong>Systematic vs.&nbsp;memorization</strong>: Risk of memorizing rather than learning arithmetic</li>
</ul>
<p><strong>Evaluation strategies</strong></p>
<ul>
<li><strong>Held-out test set</strong>: Numbers and operations not seen in training</li>
<li><strong>Systematic generalization</strong>: Can model handle larger numbers than in training?</li>
<li><strong>Error analysis</strong>: Do mistakes follow patterns that suggest understanding vs.&nbsp;memorization?</li>
<li><strong>Compositional tests</strong>: Can model handle combinations like “2 + 3 × 4”?</li>
<li><strong>Ablation studies</strong>: How does performance vary with model size, training data size?</li>
</ul>
<p><strong>Evidence of understanding</strong></p>
<ul>
<li><strong>Generalization</strong>: Correct answers on unseen number combinations</li>
<li><strong>Consistency</strong>: Same answer for equivalent expressions (“2+3” vs “3+2”)</li>
<li><strong>Error patterns</strong>: Mistakes that make mathematical sense (off by one) vs.&nbsp;random errors</li>
<li><strong>Intermediate reasoning</strong>: Model generating step-by-step solutions</li>
</ul>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="ethics-and-ai-safety" class="title-slide slide level2">
<h2>Ethics and AI safety</h2>
<p>A company wants to deploy a large language model for automated customer service. Consider the following scenario:</p>
<p><strong>Situation</strong>: The AI occasionally provides incorrect information about product returns, leading to customer frustration and potential financial losses.</p>
<ol type="1">
<li><strong>Identify risks</strong>: What are the potential harms from this deployment?</li>
<li><strong>Mitigation strategies</strong>: How could the company reduce these risks?</li>
<li><strong>Monitoring</strong>: What metrics should they track to ensure safe operation?</li>
<li><strong>Human oversight</strong>: When should humans intervene in the AI’s responses?</li>
</ol>
<aside class="notes">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Solution notes</strong></p>
</div>
<div class="callout-content">
<p><strong>Potential risks and harms</strong>:</p>
<ul>
<li><strong>Customer harm</strong>: Incorrect return information could cost customers money</li>
<li><strong>Brand damage</strong>: Poor AI interactions damage company reputation</li>
<li><strong>Legal liability</strong>: Company might be liable for AI’s incorrect advice</li>
<li><strong>Bias amplification</strong>: AI might treat different customer groups unfairly</li>
<li><strong>Escalation</strong>: Frustrated customers might become abusive toward human agents</li>
<li><strong>Over-reliance</strong>: Customers might trust AI advice over written policies</li>
</ul>
<p><strong>Mitigation strategies</strong>:</p>
<ul>
<li><strong>Knowledge grounding</strong>: Connect AI to authoritative policy databases</li>
<li><strong>Confidence thresholds</strong>: Route uncertain queries to human agents</li>
<li><strong>Response templates</strong>: Limit AI to pre-approved response patterns for critical information</li>
<li><strong>Fact verification</strong>: Cross-check AI responses against official policies</li>
<li><strong>User education</strong>: Clearly indicate when users are interacting with AI</li>
<li><strong>Fallback mechanisms</strong>: Easy escalation path to human support</li>
</ul>
<p><strong>Monitoring metrics</strong>:</p>
<ul>
<li><strong>Accuracy rates</strong>: Percentage of correct responses on return policy queries</li>
<li><strong>Customer satisfaction</strong>: Post-interaction surveys and ratings</li>
<li><strong>Escalation rates</strong>: How often customers request human assistance</li>
<li><strong>Error types</strong>: Categorize and track different kinds of mistakes</li>
<li><strong>Bias metrics</strong>: Performance across different customer demographics</li>
<li><strong>Business impact</strong>: Track correlation between AI interactions and returns/complaints</li>
</ul>
<p><strong>Human oversight triggers</strong>:</p>
<ul>
<li><strong>High-stakes queries</strong>: Expensive items, complex return situations</li>
<li><strong>Uncertainty indicators</strong>: When AI confidence scores are low</li>
<li><strong>Customer frustration</strong>: Detecting anger or confusion in customer messages</li>
<li><strong>Policy exceptions</strong>: Cases requiring</li>
</ul>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="temperature-and-text-generation" class="title-slide slide level2">
<h2>Temperature and text generation</h2>
<p>You are working with a language model that produces the following raw scores (logits) for the next token after the prompt “The weather today is”:</p>
<p><strong>Raw scores</strong>: [sunny: 2.0, cloudy: 1.8, rainy: 1.2, snowy: 0.8, windy: 0.6]</p>
<ol type="1">
<li><strong>Calculate probabilities</strong>: compute the probability distribution using softmax for temperatures T = 0.5, T = 1.0, and T = 2.0.</li>
</ol>
<p><span class="math inline">\(P(token_i) = \frac{e^{score_i/T}}{\sum_j e^{score_j/T}}\)</span></p>
<ol start="2" type="1">
<li><strong>Analyze the effects</strong>:
<ul>
<li>Which temperature setting would be best for a <strong>weather report</strong> (factual, reliable)?</li>
<li>Which would be best for <strong>creative writing</strong> (varied, interesting)?</li>
<li>What happens as temperature approaches 0? As it approaches infinity?</li>
</ul></li>
<li><strong>Practical implications</strong>:
<ul>
<li>If you were building a <strong>chatbot for customer service</strong>, what temperature would you choose and why?</li>
<li>How might you <strong>dynamically adjust</strong> temperature based on the type of response needed?</li>
</ul></li>
</ol>
<aside class="notes">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Solution Notes</strong></p>
</div>
<div class="callout-content">
<p><span class="h4"><strong>Probabilities</strong></span></p>
<p><span class="math inline">\(P(token_i) = \frac{e^{score_i/T}}{\sum_j e^{score_j/T}}\)</span></p>
<p><strong>T = 0.5 (low/focused)</strong></p>
<ul>
<li>sunny: <span class="math inline">\(e^{2.0/0.5} = e^4 = 54.6\)</span></li>
<li>cloudy: <span class="math inline">\(e^{1.8/0.5} = e^{3.6} = 36.6\)</span></li>
<li>rainy: <span class="math inline">\(e^{1.2/0.5} = e^{2.4} = 11.0\)</span></li>
<li>snowy: <span class="math inline">\(e^{0.8/0.5} = e^{1.6} = 5.0\)</span></li>
<li>windy: <span class="math inline">\(e^{0.6/0.5} = e^{1.2} = 3.3\)</span></li>
</ul>
<p>Sum = 110.5<br>
Probabilities: [0.49, 0.33, 0.10, 0.05, 0.03]</p>
<p><strong>T = 1.0 (balanced)</strong></p>
<ul>
<li>sunny: <span class="math inline">\(e^{2.0} = 7.4\)</span></li>
<li>cloudy: <span class="math inline">\(e^{1.8} = 6.0\)</span></li>
<li>rainy: <span class="math inline">\(e^{1.2} = 3.3\)</span></li>
<li>snowy: <span class="math inline">\(e^{0.8} = 2.2\)</span></li>
<li>windy: <span class="math inline">\(e^{0.6} = 1.8\)</span></li>
</ul>
<p>Sum = 20.7<br>
Probabilities: [0.36, 0.29, 0.16, 0.11, 0.09]</p>
<p><strong>T = 2.0 (high/creative)</strong></p>
<ul>
<li>sunny: <span class="math inline">\(e^{1.0} = 2.7\)</span></li>
<li>cloudy: <span class="math inline">\(e^{0.9} = 2.5\)</span></li>
<li>rainy: <span class="math inline">\(e^{0.6} = 1.8\)</span></li>
<li>snowy: <span class="math inline">\(e^{0.4} = 1.5\)</span></li>
<li>windy: <span class="math inline">\(e^{0.3} = 1.3\)</span></li>
</ul>
<p>Sum = 9.8<br>
Probabilities: [0.28, 0.25, 0.18, 0.15, 0.13]</p>
<p><span class="h4"><strong>Analysis of effects</strong></span></p>
<ul>
<li><strong>Weather report</strong>: T = 0.5 (focused on most likely/accurate predictions)</li>
<li><strong>Creative writing</strong>: T = 2.0 (more variety and unexpected choices)</li>
<li><strong>As T → 0</strong>: Distribution becomes deterministic (always picks highest score)</li>
<li><strong>As T → ∞</strong>: Distribution becomes uniform (all choices equally likely)</li>
</ul>
<p><span class="h4"><strong>Practical implications</strong></span></p>
<ul>
<li><strong>Customer service chatbot</strong>: T = 0.3-0.7 (reliable, helpful responses)</li>
<li><strong>Dynamic adjustment</strong>:
<ul>
<li>Factual questions: Low temperature</li>
<li>Creative requests: High temperature</li>
<li>Could analyze prompt content to auto-adjust</li>
</ul></li>
</ul>
<p><span class="h4"><strong>Key Insights</strong></span></p>
<ul>
<li>Temperature is a crucial hyperparameter for controlling creativity vs.&nbsp;reliability</li>
<li>Lower temperature = more predictable, higher accuracy</li>
<li>Higher temperature = more diverse, creative outputs</li>
<li>The choice depends entirely on the application and desired behavior</li>
<li>Dynamic adjustment based on context can optimize user experience</li>
</ul>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
</section>
<section id="literature" class="title-slide slide level1 smaller scrollable">
<h1>Literature</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-cybenko1989approximation" class="csl-entry" role="listitem">
Cybenko, George. 1989. <span>“Approximation by Superpositions of a Sigmoidal Function.”</span> <em>Mathematics of Control, Signals, and Systems</em> 2 (4): 303–14.
</div>
<div id="ref-hornik1989multilayer" class="csl-entry" role="listitem">
Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. <span>“Multilayer Feedforward Networks Are Universal Approximators.”</span> <em>Neural Networks</em> 2 (5): 359–66.
</div>
<div id="ref-radford2019language" class="csl-entry" role="listitem">
Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. <span>“Language Models Are Unsupervised Multitask Learners.”</span> <em>OpenAI Blog</em> 1 (8): 9.
</div>
<div id="ref-rumelhart1986learning" class="csl-entry" role="listitem">
Rumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Nature</em> 323 (6088): 533–36.
</div>
<div id="ref-sanderson2017gradient" class="csl-entry" role="listitem">
Sanderson, Grant. 2017a. <span>“Gradient Descent, How Neural Networks Learn.”</span> 3Blue1Brown. <a href="https://www.3blue1brown.com/lessons/gradient-descent">https://www.3blue1brown.com/lessons/gradient-descent</a>.
</div>
<div id="ref-sanderson2017backprop" class="csl-entry" role="listitem">
———. 2017b. <span>“What Is Backpropagation Really Doing?”</span> 3Blue1Brown. <a href="https://www.3blue1brown.com/lessons/backpropagation">https://www.3blue1brown.com/lessons/backpropagation</a>.
</div>
<div id="ref-sanderson2024gpt" class="csl-entry" role="listitem">
———. 2024a. <span>“But What Is a GPT? Visual Intro to Transformers.”</span> 3Blue1Brown. <a href="https://www.3blue1brown.com/lessons/gpt">https://www.3blue1brown.com/lessons/gpt</a>.
</div>
<div id="ref-sanderson2024attention" class="csl-entry" role="listitem">
———. 2024b. <span>“Visualizing Attention, a Transformer’s Heart.”</span> 3Blue1Brown. <a href="https://www.3blue1brown.com/lessons/attention">https://www.3blue1brown.com/lessons/attention</a>.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
</div>


</section>

<section id="footnotes" class="footnotes footnotes-end-of-document smaller scrollable" role="doc-endnotes"><h3>Footnotes</h3>

<ol>
<li id="fn1"><p>The MNIST (Modified National Institute of Standards and Technology) dataset is a popular dataset used for training and testing image classification systems, especially in the world of machine learning. It contains 60,000 training images and 10,000 test images of handwritten digits.</p></li>
<li id="fn2"><p>For a visual explanation see <a href="https://www.3blue1brown.com/lessons/chain-rule-and-product-rule">3blue1brown — Visualizing the chain rule and product rule</a></p></li>
<li id="fn3"><p>An epoch is one complete pass through the entire training dataset. During one epoch, the model sees every training example exactly once. Training might stop after a certain number of epochs or when performance plateaus.</p></li>
<li id="fn4"><p>During training by means of backpropagation, the attention matrices <span class="math inline">\(W_Q\)</span>, <span class="math inline">\(W_K\)</span>, and <span class="math inline">\(W_V\)</span> learn patterns. Thus, these are essentially weights in the neural network — they’re learned parameters just like weights in any other layer.</p></li>
<li id="fn5"><p>An embedding dimension of 12,288 means each word/token is represented as a vector with 12,288 numbers. Each position captures some aspect of meaning - though not interpretable to humans.</p></li>
<li id="fn6"><p>A vocabulary size of 50,257 tokens means the model knows 50,257 different tokens (words, word pieces, punctuation, etc.).</p></li>
<li id="fn7"><p>High temperature → more random/creative; low temperature → more focused/deterministic</p></li>
</ol>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: false,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1080,

        height: 640,

        // Factor of the display size that should remain empty around the content
        margin: 0,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script>
    // Copy over background colors to new div
    divs = document.querySelectorAll('[data-background-color]');

    Array.from(divs).map(function (x) {
      const color = x.dataset.backgroundColor;

      var new_div = document.createElement('div');
      new_div.setAttribute("class", "background-color-div");
      new_div.style.backgroundColor = color;
      x.appendChild(new_div);
      x.removeAttribute("data-background-color");
    })

    // Remove background colors from backgrounds div
    Array.from(
      document.querySelectorAll("[data-background-hash]")
    ).map(function (x) {
      x.removeAttribute("data-background-hash");
      x.style.backgroundColor = null;
    })
    </script>

    <script>
    // Copy over background images to new div
    divs = document.querySelectorAll('[data-background-image]');

    Array.from(divs).map(function (x) {
      var new_div = document.createElement('div');
      new_div.setAttribute("class", "background-image-div");

      if (x.dataset.backgroundImage != undefined) {
        new_div.style.backgroundImage = "url(" + x.dataset.backgroundImage + ")";
        x.removeAttribute("data-background-image");
      }
      if (x.dataset.backgroundSize != undefined) {
        new_div.style.backgroundSize = x.dataset.backgroundSize;
        x.removeAttribute("data-background-size");
      }
      if (x.dataset.backgroundPosition != undefined) {
        new_div.style.backgroundPosition = x.dataset.backgroundPosition;
        x.removeAttribute("data-background-position");
      }
      if (x.dataset.backgroundRepeat != undefined) {
        new_div.style.backgroundRepeat = x.dataset.backgroundRepeat;
        x.removeAttribute("data-background-repeat");
      }
      if (x.dataset.backgroundOpacity != undefined) {
        new_div.style.backgroundOpacity = x.dataset.backgroundOpacity;
        x.removeAttribute("data-background-opacity");
      }

      x.appendChild(new_div);
    })
    </script>

    <script>
    // Copy over background vidoes

    // Run once when we load
    var video_div = document.querySelectorAll(".backgrounds video");
    var slide_div = document.querySelectorAll("[data-background-video]");

    for (let i = 0; i < video_div.length; i++) {
      video_div[i].setAttribute("class", "background-video-div");

      slide_div[i].appendChild(video_div[i]);
      slide_div[i].removeAttribute("data-background-video");
    }

    Reveal.getCurrentSlide().querySelectorAll(".background-video-div").forEach(x => x.play());

    // Each time we advance slides, as the background videos aren't loaded
    // unless they are a few slides away
    Reveal.on('slidechanged', event => {
      var video_div = document.querySelectorAll(".backgrounds video");
      var slide_div = document.querySelectorAll("[data-background-video]");

      for (let i = 0; i < video_div.length; i++) {
        video_div[i].setAttribute("class", "background-video-div");

        slide_div[i].appendChild(video_div[i]);
        slide_div[i].removeAttribute("data-background-video");
      }

      Reveal.getCurrentSlide().querySelectorAll(".background-video-div").forEach(x => x.play());
    });
    </script>

    <script>
    // Copy over background vidoes

    // Run once when we load
    var iframe_div = document.querySelectorAll(".backgrounds iframe");
    var slide_div = document.querySelectorAll("[data-background-iframe]");

    for (let i = 0; i < iframe_div.length; i++) {
      iframe_div[i].setAttribute("class", "background-iframe-div");

      slide_div[i].appendChild(iframe_div[i]);
      slide_div[i].removeAttribute("data-background-iframe");
    }

    // Each time we advance slides, as the background videos aren't loaded
    // unless they are a few slides away
    Reveal.on('slidechanged', event => {
      var iframe_div = document.querySelectorAll(".backgrounds iframe");
      var slide_div = document.querySelectorAll("[data-background-iframe]");

      for (let i = 0; i < iframe_div.length; i++) {
        iframe_div[i].setAttribute("class", "background-iframe-div");

        slide_div[i].appendChild(iframe_div[i]);
        slide_div[i].removeAttribute("data-background-iframe");
      }
    });
    </script>

    <script>
    // Clean up slide background styles
    divs = document.querySelectorAll('.slide-background-content');
    Array.from(divs).map(function (x) {
      x.style = null;
    })
    </script>

    <script>

      // Move menu button
      menu_div = document.querySelector(".slide-menu-button");
      document.querySelector(".slides").appendChild(menu_div);
      
      // Move progress bar
      menu_div = document.querySelector(".progress");
      document.querySelector(".slides").appendChild(menu_div);
      
      // Move slide number
      slide_number = document.querySelector(".slide-number");
      document.querySelector(".slides").appendChild(slide_number);
      
      // Move custom footer
      footer = document.querySelector(".footer.custom");
      document.querySelector(".slides").appendChild(footer);

    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/awe-hnu\.github\.io");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>