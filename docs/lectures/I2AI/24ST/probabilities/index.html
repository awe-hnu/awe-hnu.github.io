<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andy Weeger">

<title>Probabilities ‚Äì awe.lectures</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<link href="../../../../assets/favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-e148d791b7f69c3df03a25495d5eee6c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"interstitial",
  "consent_type":"express",
  "palette":"dark",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  
<meta name="robots" content="noindex">   

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Probabilities ‚Äì awe.lectures">
<meta property="og:description" content="üß† Introduction to AI">
<meta property="og:site_name" content="awe.lectures">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">awe ‚Äî Lecture Notes</span>
    </a>
  </div>
          <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../index.html"> 
<span class="menu-text">Start</span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">Probabilities</h1>
            <p class="subtitle lead">üß† Introduction to AI</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Lecture Notes</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Andy Weeger </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Feb 13, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">Jun 24, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<p><a href="slides.html" class="btn btn-primary" target="blank">Slides</a></p>
</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#decisions-under-uncertainty" id="toc-decisions-under-uncertainty" class="nav-link" data-scroll-target="#decisions-under-uncertainty">Decisions under uncertainty</a></li>
  <li><a href="#bayes-rule" id="toc-bayes-rule" class="nav-link" data-scroll-target="#bayes-rule">Bayes‚Äô rule</a></li>
  <li><a href="#naive-bayes-models" id="toc-naive-bayes-models" class="nav-link" data-scroll-target="#naive-bayes-models">Naive Bayes models</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">‚úèÔ∏è Exercises</a></li>
  <li><a href="#literature" id="toc-literature" class="nav-link" data-scroll-target="#literature">Literature</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="introduction" class="level1 vertical-center" data-background-color="#564ac6" data-background-image="../assets/bg.jpeg">
<h1 class="vertical-center" data-background-color="#564ac6" data-background-image="../assets/bg.jpeg">Introduction</h1>
<section id="qualification-problem" class="level2">
<h2 data-anchor-id="qualification-problem">Qualification problem</h2>
<p>Logic is good, but often <strong>conclusions under uncertainty</strong> need to be drawn, as ‚Ä¶</p>
<div class="incremental">
<ul class="incremental">
<li>the agent‚Äôs knowledge of the world is incomplete (not enough information) or uncertain (sensors are unreliable);</li>
<li>the agent must consider every possible explanation for its percepts (no matter how unlikely);</li>
<li>this leads to a large belief-state full of unlikely possibilities and arbitrarily large contingent plans; and</li>
<li>rules about the world are often incomplete (e.g., are all preconditions for an action known?) or even incorrect</li>
</ul>
</div>
<div class="notes">
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Qualification problem
</div>
</div>
<div class="callout-body-container callout-body">
<p>In philosophy and AI, the qualification problem is concerned with the impossibility of listing all the preconditions required for a real-world action to have its intended effect.</p>
<p>As we have learned, there is no complete solution within logic. System designers must use good judgment in deciding how much detail to specify in their model and what details to omit. The reason for that is quite simple: often, all the conditions for an action that are necessary to achieve the intended effect can‚Äôt be known. Or if they can be known, they often lead to a large belief-state full of unlikely possibilities. This is called the <strong>qualification problem in logic</strong>.</p>
<p>However, there are good news: probability theory allows all exceptions to be grouped together without explicitly naming them.</p>
</div>
</div>
</div>
</section>
<section id="example" class="level2">
<h2 data-anchor-id="example">Example</h2>
<p><strong>Goal:</strong> Be in Ulm at 8:15 to give a lecture</p>
<p>There are several <strong>plans</strong> that achieve the goal:</p>
<div class="incremental">
<ul class="incremental">
<li><span class="math inline">\(P_1:\)</span> Get up at 6:00, take the bike, arrive at 7:30, take a shower, ‚Ä¶</li>
<li><span class="math inline">\(P_2:\)</span> Get up at 6:30, take the car at 7:00, arrive at 7:45, ‚Ä¶</li>
<li>‚Ä¶</li>
</ul>
</div>
<p>All these plans are correct, but they imply different <strong>costs</strong> and different <strong>probabilities</strong> of actually achieving the goal.</p>
<p><span class="math inline">\(P_2\)</span> eventually is the plan of choice as the success rate of P1 is only 80%</p>
</section>
<section id="uncertainty-in-rules" class="level2">
<h2 data-anchor-id="uncertainty-in-rules">Uncertainty in rules</h2>
<p>Take an expert dental diagnosis system as an example.</p>
<p><span class="math inline">\(Toothache \implies Cavity\)</span></p>
<p>‚Üí This rule is incorrect, better:</p>
<p><span class="math inline">\(Toothache \implies Cavity \lor GumProblem \lor Abscess ...\)</span></p>
<p>‚Üí However, we don‚Äôt know all the causes<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Perhaps a causal rule is better?</p>
<p><span class="math inline">\(Cavity \implies Toothache\)</span></p>
<p>‚Üí This is still wrong and does not allow to reason from symptoms to causes.</p>
</section>
<section id="learnings" class="level2">
<h2 data-anchor-id="learnings">Learnings</h2>
<div class="incremental">
<ul class="incremental">
<li>We cannot enumerate all possible causes (i.e., <strong>laziness</strong>)</li>
<li>And even if we could, we do not know how correct the rules are (i.e., <strong>theoretical ignorance</strong><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>)</li>
<li>And even if we did there will always be uncertainty about the patient (i.e., <strong>practical ignorance</strong><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>)</li>
</ul>
</div>
<p>‚Üí Without perfect knowledge (i.e., <em>‚Äúthe rules of the game‚Äù</em>), logical rules do not help much</p>
</section>
<section id="uncertainty-in-facts" class="level2">
<h2 data-anchor-id="uncertainty-in-facts">Uncertainty in facts</h2>
<p>Let us suppose we wanted to support the localization of a robot with (constant) landmarks. With the availability of landmarks, we can narrow down on the area.</p>
<p>Problem: sensors can be <strong>imprecise</strong></p>
<div class="incremental">
<ul class="incremental">
<li>From the fact that a landmark was perceived, we cannot conclude with certainty that the robot is at that location</li>
<li>The same is true when no landmark is perceived</li>
<li>Only the probability increases or decreases</li>
</ul>
</div>
</section>
<section id="probability-theory" class="level2">
<h2 data-anchor-id="probability-theory">Probability theory</h2>
<div class="incremental">
<ul class="incremental">
<li>We (and other agents) are convinced by facts and rules only up to a certain degree</li>
<li>One possibility for expressing the degree of belief is to use <strong>probabilities</strong></li>
<li>The agent is 90% (or 0.9) convinced by its sensor information<br>
<em>(in 9 out of 10 cases, the information is correct‚Äìthe agent believes)</em></li>
<li>Probabilities sum up the ‚Äúuncertainty‚Äù that stems from lack of knowledge.</li>
<li>Probabilities are not to be confused with vagueness<br>
<em>The predicate tall is vague; the statement, ‚ÄúA man is 1.75‚Äì1.80m tall‚Äù is uncertain.</em></li>
</ul>
</div>
</section>
</section>
<section id="decisions-under-uncertainty" class="level1 vertical-center headline-only" data-background-color="#564ac6" data-background-image="../assets/bg.jpeg">
<h1 class="vertical-center headline-only" data-background-color="#564ac6" data-background-image="../assets/bg.jpeg">Decisions under uncertainty</h1>
<section id="decision-theory" class="level2">
<h2 data-anchor-id="decision-theory">Decision theory</h2>
<div class="incremental">
<ul class="incremental">
<li>We have a choice of actions (or plans)</li>
<li>These can lead to different solutions with different <strong>probabilities</strong></li>
<li>The actions have different (subjective) costs</li>
<li>The results have different (subjective) utilities</li>
</ul>
</div>
<p>It would be rational to choose the action with the <strong>maximum expected utility (MEU)</strong>‚Äîthe ‚Äúaverage‚Äù, or ‚Äústatistical mean‚Äù of the outcome utilities minus the costs of the actions leading to the outcome, weighted by the probability of the outcome.</p>
</section>
<section id="unconditional-probabilities" class="level2">
<h2 data-anchor-id="unconditional-probabilities">Unconditional probabilities</h2>
<p><span class="math inline">\(P(A)\)</span> denotes the unconditional probability or <strong>prior probability</strong> that <span class="math inline">\(A\)</span> will appear in the absence of any other information, e.g.&nbsp;<span class="math inline">\(P(Cavity) = 0.1\)</span></p>
<div class="incremental">
<ul class="incremental">
<li>Prior probabilities can be obtained from statistical analysis or general rules</li>
<li>A <strong>random variable</strong><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> can take on some range‚Äîthe set of possible values (e.g., Numbers, Boolean, arbitary tokens), e.g.&nbsp;<span class="math inline">\(P(Weather=sunny=0.6)\)</span></li>
<li>Logical connectors can be used to build <strong>probabilistic propositions</strong>, e.g.&nbsp;<span class="math inline">\(P(Cavity \land \neg Insured) = 0.06\)</span></li>
<li>Propositions can contain equations over random variables, e.g.&nbsp;<span class="math inline">\(P(NoonTemp=x) = \textrm{Uniform}(x;18C;26C)\)</span><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, usually called a <strong>probability density function</strong></li>
</ul>
</div>
</section>
<section id="conditional-probabilities" class="level2">
<h2 data-anchor-id="conditional-probabilities">Conditional probabilities</h2>
<p>New information (usually called <strong>evidence</strong>) can change the probability, e.g.&nbsp;the probability of a cavity increases if we know the patient has a toothache</p>
<p><span class="math inline">\(P(a|b)\)</span> is the <strong>conditional probability</strong> of <span class="math inline">\(a\)</span> given that all we know is <span class="math inline">\(b\)</span></p>
<p>Conditional probabilities (or posterior probabilities) are defined in terms of unconditional probabilities as follows:</p>
<p>For any propositions <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> (if <span class="math inline">\(P(b)&gt;0\)</span>), we have</p>
<p><span class="math display">\[
\begin{flalign}
P(a|b) = \frac{P(a \land b)}{P(b)} = \frac{P(a, b)}{P(b)}&amp;&amp;
\end{flalign}
\]</span></p>
<p>Or in a different form as <strong>product rule</strong></p>
<p><span class="math inline">\(P(a \land b) = P(a,b) = P(a|b)P(b) = P(b|a)P(a)\)</span><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
</section>
<section id="example-1" class="level2">
<h2 data-anchor-id="example-1">Example</h2>
<p>The <strong>product rules</strong> for all possible values of <em>Weather</em> and <em>Cavity</em> can be written as a single equation:</p>
<p><span class="math inline">\(P(Weather,Cavity)=P(Weather|Cavity)P(Cavity)\)</span></p>
<p>which corresponds to a system of equations (using abbreviations <em>W</em> and <em>C</em>):</p>
<p><span class="math inline">\(P(W=sun,C=true)   =  P(W=sun|C=true)P(C=true)\)</span></p>
<p><span class="math inline">\(P(W=rain,C=true)  =  P(W=rain|C=true)P(C=true)\)</span></p>
<p>‚Ä¶</p>
<p><span class="math inline">\(P(W=snow,C=false) =  P(W=snow|C=false)P(C=false)\)</span></p>
</section>
<section id="marginalization" class="level2">
<h2 data-anchor-id="marginalization">Marginalization</h2>
<p><strong>Marginalisation</strong> in probability refers to ‚Äúsumming out‚Äù the probability of a random variable <em>X</em> given the joint probability distribution of <em>X</em> with other variable(s).</p>
<p>For any sets of variables <em>X</em> and <em>Z</em> we have</p>
<p><span class="math display">\[
\begin{flalign}
P(X=x) = \sum_Z P(X=x,Z=z) &amp;&amp;
\end{flalign}
\]</span></p>
<p>To find <span class="math inline">\(P(X=x)\)</span>, we sum all the probability values where <span class="math inline">\(X=x\)</span> occurs with all the possible values of <em>Z</em>.</p>
<p>We can abbreviate <span class="math inline">\(P(X=x)\)</span> by <span class="math inline">\(P(x)\)</span> and <span class="math inline">\(P(X=x,Z=z)\)</span> by <span class="math inline">\(P(x,z)\)</span>.</p>
<p>Using the <strong>product rule</strong>, we can replace <span class="math inline">\(P(x,z)\)</span> by <span class="math inline">\(P(X|z)P(z)\)</span>,</p>
<p><span class="math display">\[
\begin{flalign}
P(x) = \sum_Z P(x|z)P(z) &amp;&amp;
\end{flalign}
\]</span></p>
</section>
<section id="probability-axioms" class="level2">
<h2 data-anchor-id="probability-axioms">Probability axioms</h2>
<p>The <strong>basic axioms of probability</strong> theory say that every possible world has a probability between 0 and 1 and that the total probability of the set of possible worlds is 1:</p>
<p><span class="math inline">\(0 \leq P(a) \leq 1 \quad \textrm{for every } \omega \textrm{ and} \sum_{\omega \in \Omega}P(\omega)=1\)</span></p>
<p>The probability of a disjunction can be derived by following formula, sometimes called <strong>inclusion-exclusion principle</strong>:</p>
<p><span class="math inline">\(P(a \lor b) = P(a) + P(b) - P(a \land b)\)</span></p>
<p>All other properties can be derived from these axioms, e.g.</p>
<p><span class="math inline">\(P(\neg A) = 1‚ÄìP(A)\)</span></p>
<p>follows from <span class="math inline">\(P(A \lor \neg A) = 1\)</span> and <span class="math inline">\(P(A \land \neg A) = 0\)</span></p>
</section>
<section id="joint-probability" class="level2">
<h2 data-anchor-id="joint-probability">Joint probability</h2>
<p>Probabilities can be assigned to every proposition; an <strong>atomic event</strong> is an assignment of values to all random variables (i.e., a complete specification of a state)</p>
<p>Example: Let <em>X</em> and <span class="math inline">\(Y\)</span> be boolean variables, leading to following four atomic events: <span class="math inline">\(X \land Y, \neg X \land Y, X \land \neg Y, \neg X \land \neg Y\)</span></p>
<p>The joint probability distribution <span class="math inline">\(P(x_1,...,x_n)\)</span> assigns a probability to every atomic event.</p>
<div id="tbl-distribution" class="quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;">Toothache</td>
<td style="text-align: center;">¬¨Toothache</td>
</tr>
<tr class="even">
<td style="text-align: right;">Cavity</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.06</td>
</tr>
<tr class="odd">
<td style="text-align: right;">¬¨Cavity</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.89</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Probabilities of the atomic events (full joint distribution for the <em>Toothache</em>, <em>Cavity</em> world)
</figcaption>
</figure>
</div>
<p>Since all atomic events are disjoint, the sum of all fields is 1.</p>
</section>
<section id="working-with-joint-probabilities" class="level2">
<h2 data-anchor-id="working-with-joint-probabilities">Working with joint probabilities</h2>
<p>All relevant probabilities can be computed using the joint probability by expressing them as a <strong>disjunction of atomic events</strong>. Example:</p>
<p><span class="math display">\[
\begin{align}
P(Cavity \lor Toothache) &amp; =  P(Cavity \land Toothache) \\
&amp;+ P(\neg Cavity \land Toothache) \\
&amp;+ P(Cavity \land \neg Toothache)
\end{align}
\]</span></p>
<p>Unconditional probabilities are obtained by adding across a row or column:</p>
<p><span class="math inline">\(P(Cavity) = P(Cavity \land Toothache) + P(Cavity \land \neg Toothache)\)</span></p>
<p>While conditional probabilities are obtained by using the product rule:</p>
<p><span class="math inline">\(P(Cavity|Toothache) = \frac{P(Cavity \land Toothache)}{P(Toothache)}=\frac{0.04}{0.04+0.01}=0.80\)</span></p>
</section>
<section id="problems-with-joint-probabilities" class="level2">
<h2 data-anchor-id="problems-with-joint-probabilities">Problems with joint probabilities</h2>
<p>We can easily obtain all probabilities from the joint probability. The joint probability, however, involves <span class="math inline">\(k^n\)</span> values, if there are <span class="math inline">\(n\)</span> random variables with <span class="math inline">\(k\)</span> values.</p>
<p>‚Üí Implies difficulties of representation and assessment</p>
<p>Questions:</p>
<ul>
<li>Is there a more compact way of representing joint probabilities?</li>
<li>Is there an efficient method to work with this representation?</li>
</ul>
<p>Not in general, but it can work in many cases. Modern systems work directly with conditional probabilities and make assumptions on the independence of variables<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> in order to simplify calculations.</p>
</section>
</section>
<section id="bayes-rule" class="level1 vertical-center headline-only" data-background-color="#564ac6" data-background-image="../assets/bg.jpeg">
<h1 class="vertical-center headline-only" data-background-color="#564ac6" data-background-image="../assets/bg.jpeg">Bayes‚Äô rule</h1>
<section id="derivation" class="level2">
<h2 data-anchor-id="derivation">Derivation</h2>
<p>We already know the product rule that can be written in two forms:</p>
<p><span class="math inline">\(P(a \land b) = P(a|b) P(b) \quad and \quad P(a \land b) = P(b|a) P(a)\)</span></p>
<p>By equating the right-hand sides and dividing by <span class="math inline">\(P(a)\)</span>, we get</p>
<p><span class="math inline">\(P(b|a) = \frac{P(a|b)P(b)}{P(a)}\quad\)</span> (known as <strong>Bayes‚Äô rule</strong>)</p>
<p>For multi-valued variables (set of equalities):</p>
<p><span class="math inline">\(P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)}\)</span></p>
<p>Generalization (conditioning on background evidence <span class="math inline">\(e\)</span>):</p>
<p><span class="math inline">\(P(Y|X,e) = \frac{P(X|Y,e) P(Y|e)}{P(X|e)}\)</span></p>
</section>
<section id="value-of-bayes-rule" class="level2">
<h2 data-anchor-id="value-of-bayes-rule">Value of Bayes‚Äô rule</h2>
<p>Bayes‚Äô rule only allows us to compute the single term <span class="math inline">\(P(b|a)\)</span> in terms of three terms:</p>
<p><span class="math inline">\(P(a|b)\)</span>, <span class="math inline">\(P(b)\)</span>, and <span class="math inline">\(P(a)\)</span></p>
<p>That is useful in practice as there are many cases where we do have good probability estimates for these three numbers and need to compute the fourth</p>
<p>Often, we perceive as evidence the <em>effect</em> of some known <em>cause</em> and we would like to determine that cause. In that case, Bayes‚Äô rule becomes:</p>
<p><span class="math inline">\(P(cause|\mathit{effect}) = \frac{P(\mathit{effect}|cause)P(cause)}{P(\mathit{effect})}\)</span></p>
</section>
<section id="example-2" class="level2">
<h2 data-anchor-id="example-2">Example</h2>
<p><span class="math display">\[
\begin{align}
P(Toothache | Cavity) = {} &amp;  0.04 \\
P(Cavity) = {} &amp; 0.1 \\
P(Toothache) = {} &amp; 0.05 \\
P(Cavity | Toothache) = {} &amp;  \frac{0.04*0.1}{0.05} = 0.8
\end{align}
\]</span></p>
<div class="incremental">
<ul class="incremental">
<li>Here only information that quantifies the relationship between <em>Caviety</em> and <em>Toothache</em> in the <strong>causal direction</strong> is given, the diagnosis is derived by applying Baye‚Äôs rule</li>
<li>If there is quantative information in the <strong>diagnostic direction</strong> from symptons to causes, there is no need to use Baye‚Äôs rule</li>
<li>However, causal knowledge is more robust than diagnostic knowledge (imagine, e.g., a cavity epidemic<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>)</li>
</ul>
</div>
</section>
<section id="relative-probability" class="level2">
<h2 data-anchor-id="relative-probability">Relative probability</h2>
<p>Often we would consider multiple causes. For example, a dentist would also like to consider the probability that the patient has a gum disease.</p>
<p><span class="math display">\[\begin{align}
P(Toothache | GumDisease) = {} &amp;  0.7 \\
P(GumDisease) = {} &amp; 0.02
\end{align}\]</span></p>
<p>Which diagonsis is more probable?</p>
<p><span class="math inline">\(P(c|t) = \frac{P(t | c)P(c)}{P(t)} \quad\)</span> or <span class="math inline">\(\quad P(g | t) = \frac{P(t | g)P(g)}{P(t)}\)</span></p>
<p>If we are only interested in the relative probability, we need not assess <span class="math inline">\(P(T)\)</span>:</p>
<p><span class="math inline">\(\frac{P(c|t)}{P(g|t)} = \frac{P(t|c)P(c)}{P(t)} \frac{P(t)}{P(t|g)P(g)} = \frac{P(t|c)P(c)}{P(t|g)P(g)} = \frac{0.4*0.1}{0.7*0.02} = 2.857\)</span></p>
<p>‚Üí Important for excluding possible diagnoses</p>
</section>
<section id="normalization" class="level2">
<h2 data-anchor-id="normalization">Normalization</h2>
<p>If we wish to determine the absolute probability of <span class="math inline">\(P(c | t)\)</span> and we do not know <span class="math inline">\(P(t)\)</span>, we can also carry out a complete case analysis (e.g.&nbsp;for <span class="math inline">\(c\)</span> and <span class="math inline">\(\neg c\)</span>)</p>
<p>We would use the fact that</p>
<p><span class="math inline">\(P(c | t) + P(\neg c | t) = 1\)</span> (here <span class="math inline">\(c\)</span> and <span class="math inline">\(t\)</span> are boolean variables)</p>
<p><span class="math display">\[
\begin{flalign}
P(c | t) = {} &amp; \frac{P(t|c)P(c)}{P(t)} &amp;&amp; (1.1) \\
P(\neg c | t) = {} &amp; \frac{P(t|\neg c)P(\neg c)}{P(t)} &amp;&amp; (1.2) \\
P(c | t) + P(\neg c | t) = {} &amp; \frac{P(t|c)P(c)}{P(t)} + \frac{P(t|\neg c)P(\neg c)}{P(t)} &amp;&amp; (1.3) \\
P(t) = {} &amp; P(t|c) P(c) + P(t|\neg c) P(\neg c) &amp;&amp; (1.4)
\end{flalign}
\]</span></p>
<hr>
<p>By substituting equation (1.4) into the first equation (1.1), we get:</p>
<p><span class="math display">\[
\begin{flalign}
P(c|t) = {} &amp;  \frac{P(t|c)P(c)}{P(t|c)P(c)+P(t|\neg c)P(\neg c)} &amp;&amp; (1.5)
\end{flalign}
\]</span></p>
<hr>
<section id="example-3" class="level3">
<h3 data-anchor-id="example-3">Example</h3>
<p>Your doctor tells you that you have tested positive for a serious but rare (1/10000) disease (<span class="math inline">\(D\)</span>). This test (<span class="math inline">\(T\)</span>) is correct to 99% (1% false positive &amp; 1% false negative results). What does this mean for you?</p>
<div class="fragment">
<p><span class="math display">\[
\begin{flalign}
P(D|T) = \frac{P(T|D) P(D)}{P(T)} = \frac{P(T|D) P(D)}{P(T|D) P(D) + P(T| \neg D) P( \neg D)} &amp;&amp;
\end{flalign}
\]</span> <span class="smaller">Applying marginalization to Bayes Law with Boolean variables.</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[
\begin{flalign}
P(D) = 0.0001 \quad P(T|D) = 0.99 \quad P(T | \neg D) = 0.01 &amp;&amp;
\end{flalign}
\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[
\begin{flalign}
P(D|T) = \frac{0.99 \times 0.0001}{0.99 \times 0.0001 + 0.01 \times 0.9999} = \frac{0.000099}{0.010088} ‚âà 0.01 &amp;&amp;
\end{flalign}
\]</span></p>
</div>
<div class="fragment">
<p><strong>Bottom line:</strong> if the inaccuracy of the test is much greater than the frequency of occurrence of the disease, then a positive result is not as threatening as one might think.</p>
</div>
<hr>
</section>
<section id="normalizing-constant" class="level3">
<h3 data-anchor-id="normalizing-constant">Normalizing constant</h3>
<p>For random variables with multiple values:</p>
<p><span class="math display">\[
\begin{flalign}
P(Y|X) = {} &amp; \frac{P(X|Y)P(Y)}{P(X)} &amp;&amp; \\
\end{flalign}
\]</span></p>
<div class="fragment">
<p><span class="math display">\[
\begin{flalign}
\alpha = \frac{1}{P(X)} = \sum_i P(X,Y_i)P(Y_i) &amp;&amp; \\
\end{flalign}
\]</span></p>
<p>The sum of all possible values of the unknown parameter <span class="math inline">\(P(X)\)</span> is often infinite. Thus, the normalization constant <span class="math inline">\(\alpha\)</span> is often determined approximately. We get:</p>
</div>
<div class="fragment">
<p><span class="math display">\[
\begin{flalign}
P(Y|X) = {} &amp; \alpha P(X|Y) P(Y) &amp;&amp;
\end{flalign}
\]</span></p>
</div>
<hr>
</section>
</section>
<section id="generalization" class="level2" data-visibility="hidden">
<h2 data-visibility="hidden" data-anchor-id="generalization">Generalization</h2>
<p>Let <span class="math inline">\(e\)</span> be the observed value of an evidence variable <span class="math inline">\(E\)</span>, and let <span class="math inline">\(Y\)</span> be the remaining unobserved variables.</p>
<p>The query is <span class="math inline">\(P(X|e)\)</span> and can be evaluated as</p>
<p><span class="math display">\[
\begin{flalign}
P(X|e) = \frac{P(X,e)}{P(e)} = \alpha P(X,e) = \alpha \sum_y P(X,e,y) &amp;&amp;
\end{flalign}
\]</span></p>
<p>Since <span class="math inline">\(e\)</span> is known, the factor <span class="math inline">\(1 / P(e)\)</span> is the same for all values of <em>X</em>.</p>
<p>In fact, it can be viewed as a <strong>normalization constant</strong> for the distrubution <span class="math inline">\(P(X|e)\)</span>, ensuring that it adds up to 1.</p>
<div class="notes">
<p>The summation is over all possible <span class="math inline">\(y\)</span>s (i.e., all possible combinations of values of the unobserved variables <span class="math inline">\(Y\)</span>). Notice that together, the variables <em>X</em>, <span class="math inline">\(E\)</span>, and <span class="math inline">\(Y\)</span> constitute the complete set of variables for the domain, so <span class="math inline">\(P(X,e,y)\)</span> is simply a subset of probabilities from the full joint distribution.</p>
</div>
</section>
<section id="multiple-evidence" class="level2">
<h2 data-anchor-id="multiple-evidence">Multiple evidence</h2>
<p>A dentist‚Äôs probe catches in the aching tooth of a patient.</p>
<p>Using Bayes‚Äô rule, we can calculate:</p>
<p><span class="math display">\[
\begin{flalign}
P(Cav | Catch) = 0.95 &amp;&amp;
\end{flalign}
\]</span></p>
<p>But how does the combined evidence help?</p>
<p>With the help of Bayes‚Äô rule, the dentist could conclude:</p>
<p><span class="math display">\[
\begin{flalign}
P(Cav | Toothache \land Catch) = \frac{P(Toothache \land Catch|Cav)P(Cav)}{P(Toothache \land Catch)} &amp;&amp;
\end{flalign}
\]</span></p>
<p><span class="math display">\[
\begin{flalign}
P(Cav | Toothache \land Catch) = \alpha{P(Toothache \land Catch|Cav)P(Cav)} &amp;&amp; (2.0)
\end{flalign}
\]</span></p>
<hr>
<p><strong>Problem:</strong> The dentist needs to know the conditional probabilities of the conjunction <span class="math inline">\(Toothache \land Catch\)</span> for each value of <span class="math inline">\(Cav\)</span>, i.e., <strong>diagnostic knowledge of all combinations of symptoms</strong> in the general case. This might be possible for just two evidence variables, but does not scale up.</p>
<p>As it would reduce the complexity of the inference problem, it would be nice if <span class="math inline">\(Toothache\)</span> and <span class="math inline">\(Catch\)</span> were independent, but they are not: if a probe catches in the tooth, it probably has cavity which probably causes toothache.</p>
<p>However, <em>given we know whether the tooth has cavity</em>, these variables <em>are</em> independent, as each is directly caused by the cavity, but neither has a direct effect on the other<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>:</p>
<p><span class="math display">\[
\begin{flalign}
P(Toothache \land Catch | Cav) = P(Toothache | Cav) P(Catch | Cav) &amp;&amp; (2.1)
\end{flalign}
\]</span></p>
</section>
<section id="conditional-independence" class="level2">
<h2 data-anchor-id="conditional-independence">Conditional independence</h2>
<p>The general definition of <strong>conditional independence</strong> of two variables <em>X</em> and <span class="math inline">\(Y\)</span> given a third variable <em>Z</em> is:</p>
<p><span class="math display">\[
\begin{flalign}
P(X,Y|Z) = P(X|Z)P(Y|Z) &amp;&amp;
\end{flalign}
\]</span></p>
<p>In the dentist example, it seems reasonable to assert conditional independence of the variables <span class="math inline">\(Toothache\)</span> and <span class="math inline">\(Catch\)</span> given <span class="math inline">\(Cav\)</span>, thus we can insert equation (2.1) into (2.0) leading to (2.2)</p>
<p><span class="math display">\[
\begin{flalign}
P(Cav | Toothache \land Catch) = \alpha{P(Toothache \land Catch|Cav)P(Cav)} &amp;&amp; (2.0)
\end{flalign}
\]</span></p>
<p><span class="math display">\[
\begin{flalign}
P(Toothache \land Catch | Cav) = P(Toothache | Cav) P(Catch | Cav) &amp;&amp; (2.1)
\end{flalign}
\]</span></p>
<p><span class="math display">\[
\begin{flalign}
P(Cav | Toothache \land Catch) = \alpha{P(Toothache | Cav) P(Catch | Cav)P(Cav)} &amp;&amp; (2.2)
\end{flalign}
\]</span></p>
</section>
</section>
<section id="naive-bayes-models" class="level1 vertical-center headline-only" data-background-color="#564ac6" data-background-image="../assets/bg.jpeg">
<h1 class="vertical-center headline-only" data-background-color="#564ac6" data-background-image="../assets/bg.jpeg">Naive Bayes models</h1>
<div class="notes">
<p>The dentistry example illustrates a commonly occuring pattern in which a single cause directly influences a number of effects, all of which are conditionally independent, given the cause.</p>
<p>The full joint distribution can be written as</p>
<p><span class="math display">\[
\begin{flalign}
P(Cause| \mathit{Effect_1},..., \mathit{Effect_n}) = P(Cause) \prod_{i=1} P(\mathit{Effect_i}|Cause) &amp;&amp;
\end{flalign}
\]</span></p>
<p>Such a probability distribution is called a <strong>naive Bayes model</strong>‚Äî‚Äúnaive‚Äù because it is often used (as a simplifying assumption) in cases where the ‚Äúeffect‚Äù variables are <em>not</em> strictly independent given the ‚Äúcause‚Äù variable.</p>
<p>In practice, naive Bayes systems often work very well, even when the <strong>conditional independence assumption</strong> is not strictly true.</p>
</div>
<section id="recursive-bayesian-updating" class="level2">
<h2 data-anchor-id="recursive-bayesian-updating">Recursive Bayesian updating</h2>
<p>Multiple evidence can be reduced to prior probabilities and conditional probabilities (assuming conditional independence).</p>
<p>The <strong>general combination rule</strong>, if <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span> are independent given <em>X</em> is</p>
<p><span class="math display">\[
\begin{flalign}
P(C|Z_1, Z_2) = \alpha P(C) P(Z_1|C) P(Z_2|C) &amp;&amp;
\end{flalign}
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the normalization constant.</p>
<p>The generalization is called <strong>recursive Bayesian updating:</strong></p>
<p><span class="math display">\[
\begin{flalign}
P(Cause|e) = \alpha P(Cause) \prod_j P(e_j|Cause) &amp;&amp;
\end{flalign}
\]</span></p>
<div class="notes">
<p>Reinterpreting this equation in words: for each possible cause, multiply the prior probability of the cause by the product of the conditional probabilities of the observed effects given the cause; then normalize the result.</p>
</div>
</section>
<section id="types-of-variables" class="level2">
<h2 data-anchor-id="types-of-variables">Types of variables</h2>
<p>Variables can be discrete or continuous</p>
<p><strong>Discrete variables</strong></p>
<ul>
<li><span class="math inline">\(Weather\)</span>: sunny, rain, cloudy, snow</li>
<li><span class="math inline">\(Cavity\)</span>: true, false (boolean)</li>
</ul>
<p><strong>Continuous variables</strong></p>
<ul>
<li>Tomorrow‚Äôs maximum temperature in Berkeley</li>
<li>Domain can be the entire real line or any subset</li>
<li>Distributions for continuous variables are typically given by probability density functions</li>
</ul>
</section>
<section id="text-classification" class="level2">
<h2 data-anchor-id="text-classification">Text classification</h2>
<p>Imagine following task: given a text, decide which of a predefined set of classes or categories it belongs to.</p>
<ul>
<li><em>Cause</em> is the <em>Category</em> variable</li>
<li><em>Effect</em> variables are the presence or abscence of a certaing key words, <em>HasWord<sub>i</sub></em></li>
</ul>
<p>Example sentences:</p>
<ul>
<li>Stock rallied on Monday, with major indexes gaining 1% as optimism persisted over the first quarter earnings season.</li>
<li>Heavy rain continued to pound much of the east coast on Monday, with flood warnings issued in New York City and other locations</li>
</ul>
<hr>
<p>The task is to classify each sentence into a <em>Category</em>‚Äîthe major sections of the newspaper (<code>[news, sports, business, weather, entertainment]</code>)</p>
<p>The naive Bayes model consists of</p>
<ul>
<li>the prior probabilities <span class="math inline">\(P(Category)\)</span> and</li>
<li>the conditional probabilities <span class="math inline">\(P(HasWord_i|Category)\)</span></li>
</ul>
<p>For each category c <span class="math inline">\(P(Category=c)\)</span> is estimated as the fraction of all previously seen documents that are of category c<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>.</p>
<p>Similarily, <span class="math inline">\(P(HasWord_i|Category)\)</span> is estimated as the fraction of documents of each category that contain word <span class="math inline">\(i\)</span><a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>.</p>
<hr>
<p>To categorize a new document, we check which key words appear in the document and then apply <strong>recursive Bayesian updating</strong> to obtain the posterior probablity distribution over categories:</p>
<p><span class="math display">\[
\begin{flalign}
P(Cause|e) = \alpha P(Cause) \prod_j P(e_j|Cause) &amp;&amp;
\end{flalign}
\]</span></p>
<p>The category with the highest posterior probability is taken.</p>
<div class="notes">
<p>The naive Bayes model assumes that words occur independently in documents, with frequencies determined by the document category. This independence assumption is clearly violated in practice. For example, the phrase ‚Äúfirst quarter‚Äù occurs more frequently in business (or sports) articles than would be suggested by multiplying the probabilities of ‚Äúfirst‚Äù and ‚Äúquarter.‚Äù The violation of independence usually means that the final posterior probabilities will be much closer to 1 or 0 than they should be; in other words, the model is overconfident in its predictions. On the other hand, even with these errors, the ranking of the possible categories is often quite accurate. Naive Bayes models are widely used for language determination, document retrieval, spam filtering, and other classification tasks. For tasks such as medical diagnosis, where the actual values of the posterior probabilities really matter‚Äìfor example, in deciding wether to perform an appendoctomy‚Äìone would usually prefer to use more sophisticated models such as Bayesian networks <span class="citation" data-cites="RusselNorvig2022AIMA">(<a href="#ref-RusselNorvig2022AIMA" role="doc-biblioref">Russel and Norvig 2022, 422</a>)</span>.</p>
</div>
</section>
</section>
<section id="summary" class="level1 vertical-center" data-background-color="#0333ff">
<h1 class="vertical-center" data-background-color="#0333ff">Summary</h1>
<div class="incremental">
<ul class="incremental">
<li>Uncertainty is unavoidable in complex, dynamic worlds in which agents are ignorant.</li>
<li>Probabilities express the agent‚Äôs inability to reach a definite decision. They summarize the agent‚Äôs beliefs.</li>
<li>Conditional and unconditional probabilities can be formulated over propositions.</li>
<li>Bayes‚Äô rule allows us to calculate known probabilities from unknown probabilities.</li>
<li>Multiple evidence (assuming independence) can be effectively incorporated using recursive Bayesian updating.</li>
</ul>
</div>
</section>
<section id="exercises" class="level1 vertical-center" data-background-color="black">
<h1 class="vertical-center" data-background-color="black">‚úèÔ∏è Exercises</h1>
<section id="conditional-probability" class="level2">
<h2 data-anchor-id="conditional-probability">Conditional probability</h2>
<p>Show from the definition of conditional probability that <span class="math inline">\(P(a | b \land a) = 1\)</span>.</p>
<div class="notes">
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Solution notes</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Proof</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<p>Open only if you need help.</p>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<p>The definition of conditional probability states that the conditional probability of A given B is the probability of the intersection of A and B divided by the probability of B.</p>
<p>For any propositions <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> (if <span class="math inline">\(P(b)&gt;0\)</span>), we have</p>
<p><span class="math display">\[
\begin{flalign}
P(a|b)=\frac{P(a \land b)}{P(b)} &amp;&amp;
\end{flalign}
\]</span></p>
<p>Thus:</p>
<p><span class="math display">\[
\begin{flalign}
P(a|b \land a) = \frac{P(a\land(b \land a))}{P(b \land a)} &amp;&amp;
\end{flalign}
\]</span> Which is equal to</p>
<p><span class="math display">\[
\begin{flalign}
P(a|b \land a) = \frac{P(b \land a)}{P(b \land a)} = 1 &amp;&amp;
\end{flalign}
\]</span></p>
</div>
</div>
</div>
</div>
</section>
<section id="beliefs" class="level2">
<h2 data-anchor-id="beliefs">Beliefs</h2>
<p>Would it be rational for an agent to hold the three beliefs <span class="math inline">\(P(A)=0.4\)</span>, <span class="math inline">\(P(B)=0.3\)</span>, and <span class="math inline">\(P(A \lor B)=0.5\)</span>? If so, what range of probabilities would be rational for the agent to hold for <span class="math inline">\(A \land B\)</span>? Make up a table like the one in <a href="#tbl-distribution" class="quarto-xref">Table&nbsp;1</a>, and show how it supports your argument about rationality.</p>
<div class="notes">
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Solution note</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Answer</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<p>Open only if you need help.</p>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<table class="table">
<tbody>
<tr class="odd">
<td style="text-align: right;"></td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">¬¨B</td>
</tr>
<tr class="even">
<td style="text-align: right;">A</td>
<td style="text-align: center;">a</td>
<td style="text-align: center;">b</td>
</tr>
<tr class="odd">
<td style="text-align: right;">¬¨A</td>
<td style="text-align: center;">c</td>
<td style="text-align: center;">d</td>
</tr>
</tbody>
</table>
<ul>
<li><span class="math inline">\(P(A) = a + b = 0.4\)</span></li>
<li><span class="math inline">\(P(B) = a + c = 0.3\)</span></li>
<li><span class="math inline">\(P(A \lor B) = a + b + c = 0.5\)</span></li>
<li><span class="math inline">\(P(True) = a + b + c + d = 1\)</span></li>
</ul>
<p>From these, it is straightforward to infer that a = 0.2, b = 0.2, c = 0.1, and d = 0.5.</p>
<p>Therefore, <span class="math inline">\(P(A \land B) = a = 0.2\)</span>. Thus the probabilities given are consistent with a rational assignment, and the probability <span class="math inline">\(P(A \land B)\)</span> is exactly determined.</p>
</div>
</div>
</div>
</div>
</section>
<section id="medical-tests" class="level2">
<h2 data-anchor-id="medical-tests">Medical tests</h2>
<p>Consider two medical tests, A and B, for a virus. Test A is 95% effective at recognizing the virus when it is present, but has a 10% false positive rate (indicating that the virus is present, when it is not). Test B is 90% effective at recognizing the virus, but has a 5% false positive rate. The two tests use independent methods of identifying the virus. The virus is carried by 1% of all people.</p>
<p>Say that a person is tested for the virus using only one of the tests, and that test comes back positive for carrying the virus. Which test returning positive is more indicative of someone really carrying the virus? Justify your answer mathematically.</p>
<div class="notes">
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Solution note</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">Answer</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<p>Open only if you need help.</p>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<p>Let <span class="math inline">\(V\)</span> be the statement that the patient has the virus, and <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> the statements that the medical tests A and B returned positive, respectively. The problem statement gives:</p>
<p><span class="math display">\[
\begin{flalign}
P(V ) = 0.01 &amp;&amp; \\
P(A|V ) = 0.95 &amp;&amp; \\
P(A| \neg V ) = 0.10 &amp;&amp; \\
P(B| V ) = 0.90 &amp;&amp; \\
P(B| \neg V ) = 0.05 &amp;&amp; \\
\end{flalign}
\]</span></p>
<p>The test whose positive result is more indicative of the virus being present is the one whose posterior probability, <span class="math inline">\(P(V |A)\)</span> or <span class="math inline">\(P(V |B)\)</span> is largest.</p>
<p>One can compute these probabilities directly from the information given, finding that <span class="math inline">\(P(V |A) = 0.0876\)</span> and <span class="math inline">\(P(V |B) = 0.1538\)</span>, so B is more indicative.</p>
</div>
</div>
</div>
</div>
</section>
<section id="conditional-independence-1" class="level2">
<h2 data-anchor-id="conditional-independence-1">Conditional independence</h2>
<p>Show that the statement of conditional independence</p>
<p><span class="math display">\[
\begin{flalign}
P(A,B|C)=P(A|C)P(B|C) &amp;&amp;
\end{flalign}
\]</span></p>
<p>is equivalent to each of the statements</p>
<p><span class="math display">\[
\begin{flalign}
P(A|B,C)=P(A|C) \quad \textrm{and} \quad P(B|A,C)=P(B|C) &amp;&amp;
\end{flalign}
\]</span></p>
<div class="notes">
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true">Solution notes</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false">Proof</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<p>Open only if you need help.</p>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<p>The key to this exercise is rigorous and frequent application of the definition of conditional probability, <span class="math inline">\(P(X|Y) = \frac{P(X,Y)}{P(Y)}\)</span>.</p>
<p>The original statement that we are given is:</p>
<p><span class="math inline">\(P(A,B|C) = P(A|C)P(B|C)\)</span></p>
<p>We start by applying the definition of conditional probability to two of the terms in this statement:</p>
<p><span class="math display">\[
\begin{flalign}
P(A,B|C) = \frac{P(A,B,C)}{P(C)} \quad \textrm{and} \quad P(B|C) = \frac{P(B,C)}{P(C)} &amp;&amp;
\end{flalign}
\]</span></p>
<p>Now we substitute the right-hand side of these definitions for the left-hand sides in the original statement to get:</p>
<p><span class="math display">\[
\begin{flalign}
\frac{P(A,B,C)}{P(C)} = P(A|C) \frac{P(B,C)}{P(C)} &amp;&amp;
\end{flalign}
\]</span></p>
<p>Now we need the definition of conditional probability once more:</p>
<p><span class="math display">\[
\begin{flalign}
P(X|Y) &amp;= \frac{P(X,Y)}{P(Y)} \\
= P(X,Y) &amp;= P(X|Y)P(Y) \\
\text{where } X = A \text{ and } Y &amp;= B,C \\
P(A,B,C) &amp;= P(A|B,C)P(B,C) \\
\end{flalign}
\]</span></p>
<p>We substitute this right-hand side for P(A,B,C) to get:</p>
<p><span class="math display">\[
\begin{flalign}
\frac{P(A|B,C)P(B,C)}{P(C)} = P(A|C) \frac{P(B,C)}{P(C)} &amp;&amp;
\end{flalign}
\]</span></p>
<p>Finally, we cancel the <span class="math inline">\(P(B,C)\)</span> and <span class="math inline">\(P(C)\)</span>s to get:</p>
<p><span class="math inline">\(P(A|B,C) = P(A|C)\)</span></p>
<p>The second part of the exercise follows from by a similar derivation, or by noticing that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are interchangeable in the original statement (because multiplication is commutative and <span class="math inline">\(A,B\)</span> means the same as <span class="math inline">\(B,A\)</span>).</p>
</div>
</div>
</div>
</div>
</section>
<section id="pacman" class="level2">
<h2 data-anchor-id="pacman">Pacman</h2>
<p>Pacman has developed a hobby of fishing. Over the years, he has learned that a day can be considered fit or unfit for fishing <span class="math inline">\(Y\)</span> which results in three features: whether or not Ms.&nbsp;Pacman can show up <span class="math inline">\(M\)</span>, the temperature of the day <span class="math inline">\(T\)</span>, and how high the water level is <span class="math inline">\(W\)</span>. Pacman models it as an Naive Bayes classification problem.</p>
<p>We wish to calculate the probability a day is fit for fishing given features of the day. Consider the conditional probability tables that Pacman has estimated over the years:</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(Y\)</span></th>
<th><span class="math inline">\(P(Y)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>yes</td>
<td>0.1</td>
</tr>
<tr class="even">
<td>no.</td>
<td>0.9</td>
</tr>
</tbody>
</table>
<div class="columns">
<div class="column">
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(M\)</span></th>
<th><span class="math inline">\(Y\)</span></th>
<th><span class="math inline">\(P(M|Y)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>yes</td>
<td>yes</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>no</td>
<td>yes</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>yes</td>
<td>no</td>
<td>0.2</td>
</tr>
<tr class="even">
<td>no</td>
<td>no</td>
<td>0.8</td>
</tr>
</tbody>
</table>
</div><div class="column">
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(W\)</span></th>
<th><span class="math inline">\(Y\)</span></th>
<th><span class="math inline">\(P(W|Y)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>high</td>
<td>yes</td>
<td>0.1</td>
</tr>
<tr class="even">
<td>low</td>
<td>yes</td>
<td>0.9</td>
</tr>
<tr class="odd">
<td>high</td>
<td>no</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>low</td>
<td>no</td>
<td>0.5</td>
</tr>
</tbody>
</table>
</div>
</div>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(T\)</span></th>
<th><span class="math inline">\(Y\)</span></th>
<th><span class="math inline">\(P(T|Y)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cold</td>
<td>yes</td>
<td>0.2</td>
</tr>
<tr class="even">
<td>warm</td>
<td>yes</td>
<td>0.3</td>
</tr>
<tr class="odd">
<td>hot</td>
<td>yes</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>cold</td>
<td>no</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>warm</td>
<td>no</td>
<td>0.2</td>
</tr>
<tr class="even">
<td>hot</td>
<td>no</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<p>Using the method of Naive Bayes, if Ms.&nbsp;Pacman is available, the weather is cold, and the water level is high, do we predict that the day is fit for fishing?</p>
</section>
</section>
<section id="literature" class="level1">
<h1>Literature</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-RusselNorvig2022AIMA" class="csl-entry" role="listitem">
Russel, Stuart, and Peter Norvig. 2022. <em>Artificial Intelligence: A Modern Approach</em>. Harlow: Pearson Education.
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>See qualification problem<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>Medical science has no complete theory for the domain<a href="#fnref2" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p>For instance, the coincidence of having a toothache and a cavity that are unrelated, or the fact that not all tests have been run<a href="#fnref3" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn4"><p>Variables in probability theory are called random variables<a href="#fnref4" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn5"><p>Represents the belief that the temperature at noon is distributed uniformly between 18 and 26 degrees Celcius<a href="#fnref5" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn6"><p><span class="math inline">\(P(a \land b) = P(a|b)P(b)\)</span> equals <span class="math inline">\(P(a \land b) = P(b|a)P(a)\)</span><a href="#fnref6" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn7"><p>Factoring large joint distributions into smaller joint distributions, using absolute indepence. For instance, weather and dental problems are independent, thus, these can be split.<a href="#fnref7" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn8"><p>If there is a cavity epidemic and <span class="math inline">\(P(Cavity)\)</span> increases, <span class="math inline">\(P(Toothache | Cavity)\)</span> does not change, but <span class="math inline">\(P(Toothache)\)</span> and <span class="math inline">\(P(Cavity | Toothache)\)</span> will change proportionally<a href="#fnref8" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn9"><p>Toothache depends on the state of the nerves in the tooth, whereas the probe‚Äôs accuracy depends primarily on the dentist‚Äôs skill, to which the toothache is irrelevant.<a href="#fnref9" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn10"><p>For example, if 9% of the articles are about weather, we set <span class="math inline">\(P(Category=weather)=0.09\)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn11"><p>For example, if 37% of articles about business contain word 6, ‚Äústocks,‚Äù so <span class="math inline">\(P(HasWord_6=true|Category=business)\)</span> is set to 0.37<a href="#fnref11" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/awe-hnu\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2025, Andy Weeger
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">

<div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../../index.html">
<p>Start</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../../../about.html">
<p>About</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://www.hnu.de" target="_blank">
<p>HNU</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../../../imprint.html">
<p>Imprint</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>