<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andy Weeger">

<title>Generative AI ‚Äì awe.lectures</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<link href="../../../../assets/favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-0d0e73eb18f90536e3864aaf52036d38.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"interstitial",
  "consent_type":"express",
  "palette":"dark",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  
<meta name="robots" content="noindex">   


<meta property="og:title" content="Generative AI ‚Äì awe.lectures">
<meta property="og:description" content="üß† Introduction to AI">
<meta property="og:site_name" content="awe.lectures">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">awe ‚Äî Lecture Notes</span>
    </a>
  </div>
          <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../index.html"> 
<span class="menu-text">Start</span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">Generative AI</h1>
            <p class="subtitle lead">üß† Introduction to AI</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Lecture Notes</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Andy Weeger </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Apr 13, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">May 6, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<p><a href="slides.html" class="btn btn-primary" target="blank">Slides</a></p>
</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#resources" id="toc-resources" class="nav-link active" data-scroll-target="#resources">Resources</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">‚úèÔ∏è Exercises</a></li>
  <li><a href="#literature" id="toc-literature" class="nav-link" data-scroll-target="#literature">Literature</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="resources" class="level1">
<h1>Resources</h1>
<p>In preparation for the lecture, you need to read <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">Stephen Wolfram‚Äôs article on what ChatGPT is doing and why it works</a></p>
<p>If you have only limited understanding of what GenAI is, please go through: <a href="https://www.geeksforgeeks.org/what-is-generative-ai/">Geeks for Geeks articla on the basis of generative AI</a></p>
<p>If you want to do a deepdive, please consider working through <a href="https://microsoft.github.io/AI-For-Beginners/">Microsoft‚Äôs Artificial Intelligence for Beginners - A Curriculum</a></p>
</section>
<section id="exercises" class="level1 vertical-center" data-background-color="black">
<h1 class="vertical-center" data-background-color="black">‚úèÔ∏è Exercises</h1>
<section id="deep-learning" class="level2">
<h2 data-anchor-id="deep-learning">Deep Learning</h2>
<p>What are the two things you have newly learned about <strong>deep learning</strong>?</p>
</section>
<section id="key-concepts" class="level2">
<h2 data-anchor-id="key-concepts">Key concepts</h2>
<p>Identify, list and explain the key concepts discussed in the article in your own words (e.g., temperature).</p>
<p>Reflect on how these concepts contribute to ChatGPT‚Äôs functionality.</p>
</section>
<section id="loss-function" class="level2">
<h2 data-anchor-id="loss-function">Loss function</h2>
<p>Explain in your own words what a ‚Äúloss function‚Äù, sometimes also called ‚Äúcost function‚Äù, is.</p>
<p>How does the loss function change over the course of training a neural network?</p>
</section>
<section id="learning" class="level2 scrollable">
<h2 class="scrollable" data-anchor-id="learning">Learning</h2>
<p>Analyze following statements and determine if they are true or false. Justify your answer.</p>
<div class="smaller">
<ol type="1">
<li>It can be easier to solve more complicated problems with neural nets than simpler ones.</li>
<li>Optimizing neural network training relies heavily on trial-and-error approaches. Researchers have gradually built a collection of effective techniques through experimentation.</li>
<li>The field of neural network training is shifting away from building models entirely from scratch. Instead, researchers are increasingly using two main approaches: transfer learning<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> and data augmentation<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</li>
<li>In the early days of neural networks features have been discovered through the training process, which allowed the network to identify patterns that might not be obvious to human experts. In modern neural networks, features are often hand-crafted by domain experts.</li>
<li>Features are not directly stored within individual neurons. Instead, the network‚Äôs ability to identify features emerges from the collective behavior of many neurons and their connections.</li>
<li>One technique for neural network training is to iterate through the entire dataset multiple times, allowing the network to learn from each example.</li>
<li>A common approach for generating training data for Large Language Models (LLMs) involves a technique called ‚Äúguessing‚Äù. This method takes an existing piece of text, removes a portion of the ending (masking it out), and presents it to the LLM. The LLM is then tasked with predicting the guessed portion, essentially completing the text. By comparing its prediction to the original, unmasked text, the LLM learns the patterns and relationships within language.</li>
<li>Even with powerful hardware like GPUs, training large neural networks can be inefficient. This is because current computer architectures often have a separation between memory (where data is stored) and processing units (like CPUs or GPUs) that limits how much data can be accessed simultaneously. This separation forces the network to process information in small chunks, with most of the network waiting for the relevant data to be fetched from memory. This can significantly slow down the training process.</li>
<li>The ability of LLMs to handle tasks like writing essays challenges our understanding of computational difficulty. It seems that these tasks, while complex for computers in the past, may be computationally simpler than we initially believed.</li>
</ol>
</div>
<!--

Re 8: its masking, not guessing. The rest is fine.

-->
<div class="notes">
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Features vs.&nbsp;neurons
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Features</strong> are the detectable characteristics or patterns within the data that the network aims to identify. They are essentially the building blocks that the network uses for tasks like classification or prediction <span class="citation" data-cites="Wolfram2023GPT">(<a href="#ref-Wolfram2023GPT" role="doc-biblioref">Wolfram 2023</a>)</span>.</p>
<p><strong>Neurons</strong> These are the individual processing units that make up the network. They are responsible for receiving input, applying an activation function, and sending the output to other neurons <span class="citation" data-cites="Wolfram2023GPT">(<a href="#ref-Wolfram2023GPT" role="doc-biblioref">Wolfram 2023</a>)</span>.</p>
<p>An analogy: Imagine a team of detectives working on a case. Features would be the individual clues they find at the crime scene (fingerprints, shoe prints, etc.). Each detective (neuron) analyzes the clues and shares their findings with others. Through collaboration and information sharing, the team (network) builds a bigger picture of the crime (recognizes the features) and solves the case (completes the task).</p>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learnability vs.&nbsp;computational irreducibility
</div>
</div>
<div class="callout-body-container callout-body">
<p>Learnability refers to a machine learning system‚Äôs ability to acquire knowledge from data. For a machine learning system to learn effectively, there need to be underlying patterns and regularities in the data. By identifying these regularities, the system can compress the data and make accurate predictions or classifications on new, unseen data. The more a system can compress data by identifying these regularities, the more efficient and powerful it becomes.</p>
<p>Computational irreducibility refers to problems that are inherently complex and may not have simple, learnable patterns. The concept suggests that there may be a fundamental limit to the level of regularity that exists in certain problems.</p>
<p>While learnability thrives on regularities, computational irreducibility limits regularity. Or put another way, there‚Äôs an ultimate tradeoff between capability and trainability: the more you want a system to make ‚Äútrue use‚Äù of its computational capabilities, the more it‚Äôs going to show computational irreducibility, and the less it‚Äôs going to be trainable.</p>
<p>Regarding the capabilities of current LLMs: Instead what we should conclude is that tasks‚Äîlike writing essays‚Äîthat we humans could do, but we didn‚Äôt think computers could do, are actually in some sense computationally easier than we thought <span class="citation" data-cites="Wolfram2023GPT">(<a href="#ref-Wolfram2023GPT" role="doc-biblioref">Wolfram 2023</a>)</span>.</p>
</div>
</div>
</div>
</section>
<section id="embeddings" class="level2">
<h2 data-anchor-id="embeddings">Embeddings</h2>
<ol type="1">
<li>Form small groups of two to three students.</li>
<li>Discuss the concept of embeddings within your group.</li>
<li>Compile a list of 10 words relevant to a specific topic (e.g., technology, sports, food).</li>
<li>Create a word-embedding that makes use of a four-dimensional space that captures the semantic relationship of the 10 words.</li>
<li>Reflect on how word embeddings capture semantic relationships between words and how they contribute to language understanding in AI systems like ChatGPT.</li>
</ol>
<!---

the dimensions represent features or attributes in the vector space used to represent words via word embeddings. Each dimension corresponds to a specific aspect or characteristic of the word's meaning or context, and the value along each dimension indicates the word's position or strength in that particular attribute.

For example, in the representation:

Innovation [0.2, 0.5, ‚àí0.1, 0.3]

- The first dimension (0.2) might represent how closely related the word "innovation" is to technological advancements.
- The second dimension (0.5) could indicate the level of novelty or originality associated with the concept of innovation.
- The third dimension (-0.1) might capture any negative connotations or drawbacks commonly associated with innovation.
- ...

Each dimension contributes to the overall representation of the word in the vector space, allowing for nuanced relationships and meanings to be captured. These dimensions are determined by the training process of the word embedding model, which analyzes the co-occurrence patterns of words in a large corpus of text data and learns to map words to vectors in such a way that preserves semantic relationships.

--->
<div class="notes">
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Embedding
</div>
</div>
<div class="callout-body-container callout-body">
<p>Embedding refers to a low-dimensional vector representation of a piece of data, typically used for categorical variables like words, images, or even users. This process of transforming discrete data into a continuous vector space is called embedding.</p>
<p>Embeddings are important, as</p>
<ul>
<li>raw data like words or categories are not directly usable by neural networks, which operate on numbers. Embeddings bridge this gap by converting these discrete units into numerical vectors.</li>
<li>ideally, the vectors capture semantic relationships between the data points. For example, words with similar meanings should have embeddings that are closer together in the vector space.</li>
<li>they allow neural networks to perform various operations like addition, subtraction, and similarity calculations. This allows the network to learn complex relationships between the data points.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="transformers" class="level2">
<h2 data-anchor-id="transformers">Transformers</h2>
<p>Form groups of two and do additional research on the architecture and building blocks of the most notable feature of technologies like GPT, so called transformers. Prepare to explain the concept to the group.</p>
<p>Good read: <a href="https://medium.com/@tech-gumptions/transformer-architecture-simplified-3fb501d461c8#:~:text=The%20transformer%20architecture%20consists%20of,translation%20or%20a%20text%20continuation">Medium ‚Äî Transformer Architecture Simplified</a></p>
<div class="notes">
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transformer architecture
</div>
</div>
<div class="callout-body-container callout-body">
<p>Large language models like GPT are neural nets that are particularly set up for dealing with language. Their most notable feature is a piece of neural net architecture called a ‚Äútransformer‚Äù.</p>
<p>You can imagine a transformer as a powerful machine for understanding sequences, like sentences. A transformer has two main parts: an encoder and a decoder.</p>
<p>The encoder ‚Ä¶</p>
<ul>
<li>reads the input sequence (like a sentence), and</li>
<li>uses special building blocks called <strong>attention blocks</strong> to figure out how the words relate to each other. Attention is like focusing on the most important parts of a conversation with multiple people.</li>
<li>These attention blocks work alongside other building blocks called <strong>feed-forward blocks</strong> that help the model learn more complex patterns beyond just word relationships.</li>
<li>Together, the attention and feed-forward blocks within each encoder layer create a condensed representation of the entire sentence. This captures the meaning and connections between words<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</li>
</ul>
<p>The decoder ‚Ä¶</p>
<ul>
<li>uses the condensed representation from the encoder, and</li>
<li>generates the output sequence word by word, like translating a sentence or writing a summary.</li>
<li>While generating, it also pays attention to the words it already created, like following a conversation turn by turn.</li>
</ul>
<p>Attention is what makes transformers special. This allows them to understand long-distance relationships between words, important for tasks like translation or summarization. In addition and unlike some other models, transformers can analyze all parts of the sequence at once, making them faster. Overall, transformers are like super-analyzers that can break down complex sequences and understand the hidden connections within them.</p>
</div>
</div>
</div>
</section>
<section id="limitations" class="level2">
<h2 data-anchor-id="limitations">Limitations</h2>
<p>Consider what limitations you have perceived and/or heard about when using Large Language Models (LLM). Relate the limitations to the things you have learnt about how LLMs work and find explanations for these limitations. Prepare a short presentation about the most interesting limitation and the explanation you found.</p>
</section>
<section id="mega-prompts" class="level2">
<h2 data-anchor-id="mega-prompts">Mega prompts</h2>
<p>Research about ‚Äúmega prompts‚Äù and create a mega prompt that turns ChatGPT into a research question creator coach that guides you through multiple steps in finding a good research question on a topic that raises your interest.</p>
<p>Create a research question using the coach and reflect if using GPT as a guide is a meaningful strategy.</p>
</section>
<section id="ethical-concerns" class="level2">
<h2 data-anchor-id="ethical-concerns">Ethical concerns</h2>
<p>Identify ethical concerns related to AI and language models, choose one ethical concern and discuss how it applies to ChatGPT.</p>
</section>
</section>
<section id="literature" class="level1">
<h1>Literature</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Wolfram2023GPT" class="csl-entry" role="listitem">
Wolfram, Stephen. 2023. <em>What Is <span>ChatGPT</span> Doing ... And Why Does It Work?</em> Sanage Publishing House.
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Transfer learning involves incorporating the knowledge of a pre-trained network into a new model, allowing the new model to learn faster and achieve better performance.<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>Data augmentation refers to the use of pre-trained networks to generate new training examples, expanding the available data and potentially improving the performance of the new model<a href="#fnref2" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p>The output of the transformer encoder is a higher-dimensional representation of the entire input sequence. It captures not only the meaning of individual words but also the relationships and context between them. This representation is often much richer and more complex than a single embedding vector.<a href="#fnref3" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/awe-hnu\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2025, Andy Weeger
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">

<div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../../index.html">
<p>Start</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../../../about.html">
<p>About</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://www.hnu.de" target="_blank">
<p>HNU</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../../../imprint.html">
<p>Imprint</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>