<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andy Weeger">

<title>Bayesian Networks – awe.lectures</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<link href="../../../../assets/favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-58977f975ce4fb9f23873c5269492814.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"interstitial",
  "consent_type":"express",
  "palette":"dark",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  
<meta name="mermaid-theme" content="neutral">
<script src="../../../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<meta name="robots" content="noindex">   

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Bayesian Networks – awe.lectures">
<meta property="og:description" content="Introduction to AI (I2AI)">
<meta property="og:image" content="https://awe-hnu.github.io/lectures/I2AI/26ST/bayes-net/images/knowledgeBasedAgents.svg">
<meta property="og:site_name" content="awe.lectures">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">awe — Lecture Notes</span>
    </a>
  </div>
          <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../../index.html" aria-current="page"> 
<span class="menu-text">Start</span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title">Bayesian Networks</h1>
        </a>     
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block">Bayesian Networks</h1>
            <p class="subtitle lead">Introduction to AI (I2AI)</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Lecture Notes</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-body">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Andy Weeger </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Apr 14, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">Feb 9, 2026</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../../../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Admin</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
 <span class="menu-text">Moodle</span>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../lectures/I2AI/26ST/admin/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Administrivia</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lecture notes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../lectures/I2AI/26ST/intro/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../lectures/I2AI/26ST/agents/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Environments &amp; Agents</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../lectures/I2AI/26ST/search/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Search &amp; Planning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../lectures/I2AI/26ST/knowledge/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Knowledge &amp; Inference</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../lectures/I2AI/26ST/probability-theory/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Theory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../lectures/I2AI/26ST/bayes-net/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Bayesian Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../lectures/I2AI/26ST/learning/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to ML</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../lectures/I2AI/26ST/decision-trees/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Decision Trees</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../lectures/I2AI/26ST/neural-networks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural Networks</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#bayesian-networks" id="toc-bayesian-networks" class="nav-link" data-scroll-target="#bayesian-networks">Bayesian Networks</a></li>
  <li><a href="#structure" id="toc-structure" class="nav-link" data-scroll-target="#structure">Structure</a></li>
  <li><a href="#pacmanexample" id="toc-pacmanexample" class="nav-link" data-scroll-target="#pacmanexample">Example: Pacman</a></li>
  <li><a href="#building-a-bayes-net" id="toc-building-a-bayes-net" class="nav-link" data-scroll-target="#building-a-bayes-net">Building a Bayes net</a></li>
  <li><a href="#the-markov-blanket" id="toc-the-markov-blanket" class="nav-link" data-scroll-target="#the-markov-blanket">The Markov blanket</a></li>
  <li><a href="#inference-in-bayes-nets" id="toc-inference-in-bayes-nets" class="nav-link" data-scroll-target="#inference-in-bayes-nets">Inference in Bayes nets</a></li>
  <li><a href="#common-misconceptions" id="toc-common-misconceptions" class="nav-link" data-scroll-target="#common-misconceptions">Common misconceptions</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  <li><a href="#literature" id="toc-literature" class="nav-link" data-scroll-target="#literature">Literature</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-body" id="quarto-document-content">





<section id="introduction" class="level1">
<h1>Introduction</h1>
<section id="knowledge-based-agents" class="level2">
<h2 data-anchor-id="knowledge-based-agents">Knowledge-based agents</h2>
<p>As discussed in the section <a href="../knowledge/">Knowledge &amp; Inference</a>, knowledge-based agents combine two essential components:</p>
<ul>
<li>A <strong>knowledge base</strong> (KB) that stores information about the environment in a formal, structured representation</li>
<li>An <strong>inference engine</strong> that applies logical rules to answer questions and derive new insights</li>
</ul>
<p>When knowledge-based agents interact with their environment, they</p>
<ul>
<li>process new observations to update their knowledge,</li>
<li>answer queries based on what they know, and</li>
<li>make decisions by applying reasoning to their knowledge base</li>
</ul>
<div id="fig-kba" class="quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kba-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/knowledgeBasedAgents.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kba-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A simplified illustration of a knowledge-based agent
</figcaption>
</figure>
</div>
</section>
<section id="traditional-knowledge-based-agents" class="level2">
<h2 data-anchor-id="traditional-knowledge-based-agents">Traditional knowledge-based agents</h2>
<p>Traditional knowledge-based agents use logical frameworks like propositional or first-order logic. While powerful in certain domains (such as medical diagnosis), they face significant challenges:</p>
<ul>
<li><strong>Binary reasoning only</strong> — they can only represent <em>True</em> or <em>False</em> statements, with no ability to handle uncertainty or probability</li>
<li><strong>Complexity barriers</strong> — as knowledge bases grow, both representation and reasoning become increasingly difficult</li>
<li><strong>Limited expressiveness</strong> — more formal languages often sacrifice the ability to represent nuanced knowledge</li>
</ul>
</section>
<section id="from-logic-to-probability" class="level2">
<h2 data-anchor-id="from-logic-to-probability">From logic to probability</h2>
<p>The limitations of traditional knowledge-based systems highlight a fundamental challenge: real-world reasoning rarely operates in absolutes of true and false. Instead, our knowledge is often incomplete, uncertain, or subject to change as new evidence emerges.</p>
<p>By incorporating probability theory, we can represent knowledge in a structured form while replacing rigid logical rules with probabilistic relationships.</p>
<p>The core of probabilistic reasoning is the <strong>joint probability distribution</strong> (JPD), which specifies the probability of every possible combination of values for all variables in our domain. For <span class="math inline">\(N\)</span> variables, this distribution represents all possible combinations of values and their probabilities:</p>
<p><span class="math display">\[
P(X_1=x_{i_1},X_2=x_{i_2}, \ldots, X_N=x_{i_N}) \quad \text{or simply} \quad P(x_{i_1},x_{i_2}, \ldots, x_{i_N})
\]</span></p>
<p>With a complete JPD, an agent can answer two fundamental types of questions:</p>
<ol type="1">
<li><p><strong>Joint probability queries</strong>:<br>
What’s the likelihood of multiple events occurring together?<br>
<span class="math inline">\(P(X_1=a,X_2=b)\)</span></p></li>
<li><p><strong>Conditional probability queries</strong>:<br>
Given that one event has occurred, what’s the probability of another?<br>
<span class="math inline">\(P(X_1=a \mid X_2=b)\)</span></p></li>
</ol>
<p>The power of a complete JPD is that it contains all information needed to answer any probabilistic query about the modeled variables.</p>
<section id="example-pacman" class="level3">
<h3 data-anchor-id="example-pacman">Example: Pacman</h3>
<p>Consider a Pacman game with three key events:</p>
<ul>
<li>encountering a ghost (<span class="math inline">\(G\)</span>),</li>
<li>eating a power pellet (<span class="math inline">\(P\)</span>), and</li>
<li>successfully completing the level (<span class="math inline">\(L\)</span>).</li>
</ul>
<p>We’ve analyzed game data to create the following joint probability distribution (JPD):</p>
<div id="tbl-pacman" class="quarto-float quarto-figure quarto-figure-left">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-pacman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 19%">
<col style="width: 29%">
<col style="width: 32%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Ghost (<span class="math inline">\(G\)</span>)</th>
<th>Power pellet (<span class="math inline">\(P\)</span>)</th>
<th>Level complete (<span class="math inline">\(L\)</span>)</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>True</td>
<td>True</td>
<td>True</td>
<td>0.15</td>
</tr>
<tr class="even">
<td>True</td>
<td>True</td>
<td>False</td>
<td>0.05</td>
</tr>
<tr class="odd">
<td>True</td>
<td>False</td>
<td>True</td>
<td>0.05</td>
</tr>
<tr class="even">
<td>True</td>
<td>False</td>
<td>False</td>
<td>0.25</td>
</tr>
<tr class="odd">
<td>False</td>
<td>True</td>
<td>True</td>
<td>0.20</td>
</tr>
<tr class="even">
<td>False</td>
<td>True</td>
<td>False</td>
<td>0.10</td>
</tr>
<tr class="odd">
<td>False</td>
<td>False</td>
<td>True</td>
<td>0.10</td>
</tr>
<tr class="even">
<td>False</td>
<td>False</td>
<td>False</td>
<td>0.10</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-pacman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Joint probability distribution of the Pacman scenarios
</figcaption>
</figure>
</div>
<p>This knowledge-base tells us, for example:</p>
<ul>
<li>The probability that a player encounters a ghost, eats a power pellet, and completes the level is <span class="math inline">\(P(G,P,L)=0.15\)</span></li>
<li>The probability that a player encounters no ghost, no power pellet, and fails to complete the level is <span class="math inline">\(P(\neg G, \neg P, \neg L) = 0.10\)</span></li>
</ul>
<p>With this complete JPD, we can answer questions like:</p>
<ul>
<li>What’s the probability of encountering a ghost during gameplay? <span class="math inline">\(P(G)\)</span>?</li>
<li>What’s the probability of both encountering a ghost and eating a power pellet? <span class="math inline">\(P(G,P)\)</span>?</li>
<li>If a player encounters a ghost, what’s the probability they’ll complete the level? <span class="math inline">\(P(L \mid G)\)</span>?</li>
<li>If a player eats a power pellet, what’s the probability they’ll encounter a ghost? <span class="math inline">\(P(G \mid P)\)</span>?</li>
<li>If a player encounters a ghost and eats a power pellet, what’s the probability they’ll complete the level? <span class="math inline">\(P(L \mid G, P)\)</span>?</li>
</ul>
</section>
</section>
<section id="the-scalability-problem" class="level2">
<h2 data-anchor-id="the-scalability-problem">The scalability problem</h2>
<p>While a complete JPD is theoretically powerful, it quickly becomes impractical. With <span class="math inline">\(d\)</span> variables that each take <span class="math inline">\(n\)</span> values, the JPD requires storing <span class="math inline">\(n^d\)</span> probabilities. For example, with just 10 binary variables, we need to store 1,024 values; with 20 variables, over a million!</p>
<p><strong>Bayesian networks</strong> address these limitations by exploiting independence relationships between variables to create a more compact representation.</p>
</section>
</section>
<section id="bayesian-networks" class="level1">
<h1>Bayesian Networks</h1>
<p>By the use of <strong>Bayesian networks</strong>, complexity can be dramatically reduced by exploiting:</p>
<ul>
<li>Variable <strong>independence</strong>: When variables don’t influence each other</li>
<li><strong>Conditional independence</strong>: When variables are unrelated once we know the value of other variables</li>
</ul>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Unconditional indendence and conditional independence
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="h4"><strong>Unconditional indendence</strong></span></p>
<p>When two variables are independent, knowing the value of one tells you nothing about the value of the other. Mathematically, we say A and B are independent if <span class="math inline">\(P(A,B) = P(A) × P(B)\)</span>.</p>
<p>The terms unconditional independence, variable independence, and marginal independence describe the same concept: two variables are independent without conditioning on any other variables, meaning the probability distribution of one variable is not affected by the value of the other.</p>
<p>For example, in the <a href="#pacmanexample">Pacman example</a>, if encountering a ghost were completely independent of eating a power pellet, then knowing a player ate a power pellet wouldn’t change our estimate of whether they’ll encounter a ghost.</p>
<p><span class="h4"><strong>Conditional independence</strong></span></p>
<p>This is more subtle and powerful. Variables A and B are conditionally independent given C if, once you know C, learning A gives you no additional information about B.</p>
<p>Mathematically:<br>
<span class="math inline">\(P(A,B \mid C) = P(A \mid C) \cdot P(B \mid C)\)</span></p>
<p>Or equivalently:<br>
<span class="math inline">\(P(A \mid C,B) = P(A \mid C) \quad \text{and} \quad P(B \mid C,A) = P(B \mid C)\)</span></p>
<p>For example, imagine that in Pacman:</p>
<ul>
<li>Ghost encounters (<span class="math inline">\(G\)</span>) influence level completion (<span class="math inline">\(L\)</span>)</li>
<li>Power pellets (<span class="math inline">\(P\)</span>) influence level completion (<span class="math inline">\(L\)</span>)</li>
<li>But once we know whether the level was completed (<span class="math inline">\(L\)</span>), knowing about ghost encounters (<span class="math inline">\(G\)</span>) tells us nothing new about power pellets (<span class="math inline">\(P\)</span>)</li>
</ul>
<p>In this case, <span class="math inline">\(G\)</span> and <span class="math inline">\(P\)</span> would be conditionally independent given <span class="math inline">\(L\)</span> (see <a href="#path-blocking-rules">fork structure</a>.</p>
</div>
</div>
<section id="reasoning" class="level2">
<h2 data-anchor-id="reasoning">Reasoning</h2>
<p>Bayesian networks excel at two types of reasoning:</p>
<ul>
<li><strong>Causal reasoning</strong> (predicting effects):<br>
<span class="math inline">\(P(\text{effects} \mid \text{causes})\)</span><br>
Example: “Given a patient’s symptoms, what disease do they likely have?”</li>
<li><strong>Diagnostic reasoning</strong> (inferring causes):<br>
<span class="math inline">\(P(\text{causes} \mid \text{evidence})\)</span><br>
Example: “Given a patient has disease X, what symptoms might they develop?”</li>
</ul>
</section>
</section>
<section id="structure" class="level1">
<h1>Structure</h1>
<p>Bayesian networks exploit these independencies by organizing variables in a directed acyclic graphs structure where:</p>
<ul>
<li>Each node only depends directly on its parent nodes</li>
<li>Nodes are conditionally independent of their non-descendants, given their parents</li>
</ul>
<p>This structure dramatically reduces the number of probabilities that need to stored. Instead of storing the full JPD with <span class="math inline">\(2^n\)</span> values (where <span class="math inline">\(n\)</span> is the number of variables), we only need to store conditional probabilities for each node given its parents.</p>
<p>For instance, with 20 binary variables, a full JPD would require <span class="math inline">\(2^{20}\)</span> ≈ 1 million probabilities, but a Bayesian network might need only a few thousand, making probabilistic reasoning computationally feasible.</p>
<p>Bayesian networks are directed acyclic graphs where:</p>
<ul>
<li><strong>Nodes</strong> represent random variables (i.e., events or propositions about the world)</li>
<li><strong>Edges</strong> represent direct causal or influential relationships (i.e., they show which variables directly influence each other)</li>
<li>Each node contains a <strong>probability distribution</strong> conditioned on its parent nodes (i.e., a <strong>conditional probability table</strong> (CPT) showing how likely its values are, based on its parent nodes’ values; <span class="math inline">\(P(X \mid parents(X))\)</span>)</li>
</ul>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Directed acyclic graphs
</div>
</div>
<div class="callout-body-container callout-body">
<p>Directed acyclic graphs (DAGs) are mathematical structures with:</p>
<ul>
<li>Nodes (vertices) connected by directed edges (arrows)</li>
<li>No cycles or loops (you cannot follow arrows and return to a starting node)</li>
<li>A partial ordering of nodes (some nodes come “before” others)</li>
</ul>
<p>DAGs are essential in Bayesian networks because they encode conditional independence relationships and support efficient probabilistic inference algorithms.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A[Disease] --&gt; B[Symptom 1]
    A --&gt; C[Symptom 2]
    D[Risk Factor] --&gt; A
    D --&gt; E[Comorbidity]
    E --&gt; C
</pre>
</div>
<p></p><figcaption> Example of a Directed Acyclic Graph (DAG)</figcaption> </figure><p></p>
</div>
</div>
</div>
<p>This DAG example shows how a risk factor might influence a disease, which causes symptoms, while a comorbidity influenced by the same risk factor affects one of the symptoms.</p>
<div class="small">
<p>In contrast, knowledge graphs:</p>
<ul>
<li>allow cycles (bidirectional relationships),</li>
<li>use labeled edges to represent different types of relationships, _ primarily represent factual knowledge rather than probabilistic dependencies,</li>
<li>have no acyclicity constraint, and</li>
<li>focus on representing semantic connections between entities.</li>
</ul>
<p>While both structures organize information as nodes and edges, they serve different purposes: DAGs for probabilistic reasoning and knowledge graphs for representing interconnected factual knowledge.</p>
</div>
</div>
</div>
<section id="d-separation" class="level2">
<h2 data-anchor-id="d-separation">D-Separation</h2>
<p>To systematically determine conditional independence relationships in Bayesian networks, we use the concept of <strong>d-separation</strong> (directional separation). D-separation provides formal criteria for identifying whether variables are conditionally independent given a set of evidence variables.</p>
<p>Two variables <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are d-separated given a set of evidence variables <span class="math inline">\(E\)</span> if all paths between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are “blocked” by variables in <span class="math inline">\(E\)</span>. When variables are d-separated, they are conditionally independent.</p>
<p>In the context of d-separation, a path being “blocked” means that information cannot flow along that path between the variables, resulting in conditional independence. When all paths between two variables are blocked, those variables are conditionally independent.</p>
<p>Think of a Bayesian network as a system of information channels. A path is “open” if it allows probabilistic information to flow from one variable to another. A path is “blocked” if something prevents this information flow, making the connected variables conditionally independent.</p>
<section id="path-blocking-rules" class="level3">
<h3 data-anchor-id="path-blocking-rules">Path blocking rules</h3>
<p>Whether a path is blocked depends on both the structure of the path and which variables we already know (our evidence variables <span class="math inline">\(E\)</span>):</p>
<p><span class="h4"><strong>Chain structure</strong></span></p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
  A--&gt;C
  C--&gt;B
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ul>
<li>The path is blocked when <span class="math inline">\(C\)</span> is observed (when <span class="math inline">\(C\)</span> is in our evidence set <span class="math inline">\(E\)</span>)</li>
<li><strong>Result</strong>: <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are conditionally independent given <span class="math inline">\(C\)</span></li>
<li><strong>Formally</strong>: <span class="math inline">\(P(B|A,C) = P(B|C)\)</span> and <span class="math inline">\(P(A|B,C) = P(A|C)\)</span></li>
<li><strong>Example</strong>: If “Rain” causes “Wet Ground” which causes “Slippery Road”, then once we know the ground is wet, learning about rain gives us no additional information about road slipperiness</li>
<li><strong>Intuition</strong>: Once we know the middle variable’s value, the first variable can’t tell us anything new about the third variable</li>
</ul>
<p><span class="h4"><strong>Fork structure</strong></span></p>
<p><span class="math inline">\(A \leftarrow C \rightarrow B\)</span></p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
  C--&gt;A
  C--&gt;B
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ul>
<li>The path is blocked when <span class="math inline">\(C\)</span> is observed (when <span class="math inline">\(C\)</span> is in our evidence set <span class="math inline">\(E\)</span>)</li>
<li><strong>Result</strong>: <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are conditionally independent given <span class="math inline">\(C\)</span></li>
<li><strong>Formally</strong>: <span class="math inline">\(P(A|B,C) = P(A|C)\)</span> and <span class="math inline">\(P(B|A,C) = P(B|C)\)</span></li>
<li><strong>Example</strong>: If “Season” influences both “Temperature” and “Daylight Hours”, then once we know the season, learning about temperature gives us no additional information about daylight hours</li>
<li><strong>Intuition</strong>: When we know the common cause, its effects become independent of each other</li>
</ul>
<p><span class="h4"><strong>Collider structure</strong> or v-structure</span></p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
  A--&gt;C
  B--&gt;C
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ul>
<li><p>The path is blocked when <span class="math inline">\(C\)</span> and all its descendants are NOT observed</p></li>
<li><p><strong>Result</strong>: <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are marginally independent (unconditionally independent)</p></li>
<li><p><strong>Formally</strong>: <span class="math inline">\(P(A|B) = P(A)\)</span> and <span class="math inline">\(P(B|A) = P(B)\)</span></p></li>
<li><p>The path becomes unblocked when <span class="math inline">\(C\)</span> OR any descendant of <span class="math inline">\(C\)</span> IS observed</p></li>
<li><p><strong>Result</strong>: <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> become conditionally dependent given <span class="math inline">\(C\)</span></p></li>
<li><p><strong>Formally</strong>: <span class="math inline">\(P(A|B,C) \neq P(A|C)\)</span> and <span class="math inline">\(P(B|A,C) \neq P(B|C)\)</span></p></li>
<li><p><strong>Example</strong>: If both “Flu” and “Allergies” can cause “Sneezing”, then initially flu and allergies are independent. However, if we observe sneezing, knowing someone doesn’t have allergies increases the probability they have the flu</p></li>
<li><p><strong>Intuition</strong>: Independent causes become dependent when we observe their common effect</p></li>
</ul>
<p>Understanding d-separation allows us to identify the conditional independence assumptions encoded in a Bayesian network structure, which is crucial for both building appropriate networks and performing efficient inference.</p>
</section>
</section>
</section>
<section id="pacmanexample" class="level1">
<h1>Example: Pacman</h1>
<p>Let’s model a Pacman video game scenario where we want to predict when the player will encounter ghosts and be defeated.</p>
<p>Our Bayesian network will model:</p>
<ul>
<li><strong>Player Skill</strong> (<span class="math inline">\(S\)</span>): We classify players into three skill levels:
<ul>
<li><span class="math inline">\(25\%\)</span> of players are beginners (<span class="math inline">\(b\)</span>)</li>
<li><span class="math inline">\(45\%\)</span> of players are intermediate (<span class="math inline">\(i\)</span>)</li>
<li><span class="math inline">\(30\%\)</span> of players are advanced (<span class="math inline">\(a\)</span>)</li>
</ul></li>
<li><strong>Level Type</strong> (<span class="math inline">\(L\)</span>):
<ul>
<li><span class="math inline">\(40\%\)</span> of levels are maze-type (<span class="math inline">\(m\)</span>)</li>
<li><span class="math inline">\(60\%\)</span> of levels are open-area type (<span class="math inline">\(o\)</span>)</li>
</ul></li>
<li><strong>Ghost Activity</strong> (<span class="math inline">\(G\)</span>): There are high activity (<span class="math inline">\(h\)</span>) and low activity (<span class="math inline">\(l\)</span>) levels:
<ul>
<li>For maze levels:
<ul>
<li>Beginners face high ghost activity with probability <span class="math inline">\(0.8\)</span></li>
<li>Intermediate players face high ghost activity with probability <span class="math inline">\(0.5\)</span></li>
<li>Advanced players face high ghost activity with probability <span class="math inline">\(0.3\)</span></li>
</ul></li>
<li>For open-area levels:
<ul>
<li>Beginners face high ghost activity with probability <span class="math inline">\(0.6\)</span></li>
<li>Intermediate players face high ghost activity with probability <span class="math inline">\(0.4\)</span></li>
<li>Advanced players face high ghost activity with probability <span class="math inline">\(0.2\)</span></li>
</ul></li>
</ul></li>
<li><strong>Power Pellet Collected</strong> (<span class="math inline">\(P\)</span>):
<ul>
<li>With high ghost activity, the probability of collecting a power pellet is <span class="math inline">\(0.7\)</span></li>
<li>With low ghost activity, the probability of collecting a power pellet is <span class="math inline">\(0.4\)</span></li>
</ul></li>
<li><strong>Player Defeated</strong> (<span class="math inline">\(D\)</span>):
<ul>
<li>With high ghost activity and no power pellet, defeat probability is <span class="math inline">\(0.9\)</span></li>
<li>With high ghost activity and a power pellet, defeat probability is <span class="math inline">\(0.3\)</span></li>
<li>With low ghost activity and no power pellet, defeat probability is <span class="math inline">\(0.4\)</span></li>
<li>With low ghost activity and a power pellet, defeat probability is <span class="math inline">\(0.1\)</span></li>
</ul></li>
</ul>
<p>The Bayesian Network for this Pacman scenario is:</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    S[Player Skill] --&gt; G[Ghost Activity]
    L[Level Type] --&gt; G
    G --&gt; P[Power Pellet Collected]
    G --&gt; D[Player Defeated]
    P --&gt; D
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>In this network:</p>
<ul>
<li><strong>Nodes</strong> represent our 5 key variables (Skill, Level Type, Ghost Activity, Power Pellet, Defeated)</li>
<li><strong>Edges</strong> connect variables that directly influence each other (e.g., Player Skill affects Ghost Activity)</li>
<li><strong>Missing edges</strong> indicate conditional independence (e.g., Level Type and Player Skill don’t directly affect whether a Power Pellet is collected - this influence flows through Ghost Activity)</li>
<li>Each node has an associated <strong>conditional probability table (CPT)</strong> showing how likely each value is given its parent nodes’ values</li>
</ul>
<section id="example-in-the-pacman-network" class="level3">
<h3 data-anchor-id="example-in-the-pacman-network">Example in the Pacman Network</h3>
<p>In our Pacman network:</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    S[Player Skill] --&gt; G[Ghost Activity]
    L[Level Type] --&gt; G
    G --&gt; P[Power Pellet Collected]
    G --&gt; D[Player Defeated]
    P --&gt; D
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>We can identify the following d-separation relationships:</p>
<ul>
<li><span class="math inline">\(S\)</span> and <span class="math inline">\(L\)</span> are d-separated (marginally independent) because their only connecting path is through a collider at <span class="math inline">\(G\)</span>
<ul>
<li><strong>This means</strong>: <span class="math inline">\(P(S|L) = P(S)\)</span> and <span class="math inline">\(P(L|S) = P(L)\)</span></li>
</ul></li>
<li><span class="math inline">\(S\)</span> and <span class="math inline">\(P\)</span> are d-separated given <span class="math inline">\(G\)</span> (conditionally independent given <span class="math inline">\(G\)</span>) because knowing <span class="math inline">\(G\)</span> blocks the chain <span class="math inline">\(S \rightarrow G \rightarrow P\)</span>
<ul>
<li><strong>This means</strong>: <span class="math inline">\(P(S|P,G) = P(S|G)\)</span> and <span class="math inline">\(P(P|S,G) = P(P|G)\)</span></li>
</ul></li>
<li><span class="math inline">\(S\)</span> and <span class="math inline">\(D\)</span> are d-separated given <span class="math inline">\(G\)</span> and <span class="math inline">\(P\)</span> (conditionally independent given both <span class="math inline">\(G\)</span> and <span class="math inline">\(P\)</span>) because all paths between them are blocked
<ul>
<li><strong>This means</strong>: <span class="math inline">\(P(S|D,G,P) = P(S|G,P)\)</span> and <span class="math inline">\(P(D|S,G,P) = P(D|G,P)\)</span></li>
</ul></li>
<li><span class="math inline">\(P\)</span> and <span class="math inline">\(L\)</span> are d-separated given <span class="math inline">\(G\)</span> (conditionally independent given <span class="math inline">\(G\)</span>) because knowing <span class="math inline">\(G\)</span> blocks the fork <span class="math inline">\(P \leftarrow G \leftarrow L\)</span>
<ul>
<li><strong>This means</strong>: <span class="math inline">\(P(P|L,G) = P(P|G)\)</span> and <span class="math inline">\(P(L|P,G) = P(L|G)\)</span></li>
</ul></li>
</ul>
<p>However, note that:</p>
<ul>
<li><span class="math inline">\(S\)</span> and <span class="math inline">\(L\)</span> are not d-separated given <span class="math inline">\(G\)</span> (they become dependent when conditioning on their common effect)
<ul>
<li><strong>This means</strong>: <span class="math inline">\(P(S|L,G) \neq P(S|G)\)</span> and <span class="math inline">\(P(L|S,G) \neq P(L|G)\)</span></li>
</ul></li>
<li><span class="math inline">\(G\)</span> and <span class="math inline">\(D\)</span> are not d-separated given <span class="math inline">\(P\)</span> (they have a direct link)
<ul>
<li><strong>This means</strong>: <span class="math inline">\(P(G|D,P) \neq P(G|P)\)</span> and <span class="math inline">\(P(D|G,P) \neq P(D|P)\)</span></li>
</ul></li>
</ul>
</section>
</section>
<section id="building-a-bayes-net" class="level1">
<h1>Building a Bayes net</h1>
<p>To construct a Bayesian network that accurately models a domain:</p>
<ol type="1">
<li><p><strong>Identify root nodes</strong>:<br>
Start with variables that don’t depend on any others (no parents).<br>
<span class="small">In our Pacman example, these are Player Skill (<span class="math inline">\(S\)</span>) and Level Type (<span class="math inline">\(L\)</span>).</span></p></li>
<li><p><strong>Add dependent variables in layers</strong>:<br>
Connect variables to their direct influences.<br>
<span class="small">Ghost Activity (<span class="math inline">\(G\)</span>) depends on both Player Skill and Level Type, so it forms the second layer.</span></p></li>
<li><p><strong>Continue by causal relationship</strong>:<br>
Add variables that depend on the previous layer.<br>
<span class="small">Power Pellet (<span class="math inline">\(P\)</span>) and part of the Player Defeated (<span class="math inline">\(D\)</span>) probability depend on Ghost Activity.</span></p></li>
<li><p><strong>Complete all dependencies</strong>:<br>
Ensure all influence relationships are captured.<br>
<span class="small">Player Defeated (<span class="math inline">\(D\)</span>) depends on both Ghost Activity and Power Pellet status.</span></p></li>
</ol>
</section>
<section id="the-markov-blanket" class="level1">
<h1>The Markov blanket</h1>
<p>The <strong>Markov blanket</strong> of a node is the minimal set of variables that shield it from the rest of the network. For any node <span class="math inline">\(X\)</span>, once you know its Markov blanket, no other variable provides additional information about <span class="math inline">\(X\)</span>.</p>
<p>A node’s Markov blanket consists of:</p>
<ul>
<li>Its parents</li>
<li>Its children</li>
<li>The parents of its children (other than itself)</li>
</ul>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    subgraph "Markov Blanket of X"
        P1[Parent 1] --&gt; X
        P2[Parent 2] --&gt; X
        X --&gt; C1[Child 1]
        X --&gt; C2[Child 2]
        CP[Co-Parent] --&gt; C2
    end
    Other1[Other Variable] --- Other2[Other Variable]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This property allows us to perform <strong>localized reasoning</strong>, making Bayesian networks computationally efficient.</p>
</section>
<section id="inference-in-bayes-nets" class="level1">
<h1>Inference in Bayes nets</h1>
<p>With a complete Bayesian network, we can calculate three types of probabilities:</p>
<ol type="1">
<li><strong>Marginal probabilities</strong> for single variables:<br>
<span class="math inline">\(P(X=x)\)</span></li>
<li><strong>Joint probabilities</strong> for multiple variables:<br>
<span class="math inline">\(P(X=x,Y=y)\)</span></li>
<li><strong>Conditional probabilities</strong> between variables:<br>
<span class="math inline">\(P(Y=y \mid X=x)\)</span></li>
</ol>
<section id="calculating-marginal-probabilities" class="level3">
<h3 data-anchor-id="calculating-marginal-probabilities">Calculating marginal probabilities</h3>
<section id="for-root-nodes" class="level4">
<h4 data-anchor-id="for-root-nodes">For root nodes</h4>
<p>For variables at the top of the network (like Player Skill in our Pacman example), we can directly read their probabilities from their CPT:</p>
<p><span class="math display">\[P(S=\text{advanced}) = 0.30\]</span></p>
</section>
<section id="for-other-nodes" class="level4">
<h4 data-anchor-id="for-other-nodes">For other nodes</h4>
<p>For variables with parents, we must use <strong>marginalization</strong> — summing over all possible parent values. For example, to find the probability of high Ghost Activity:</p>
<p><span class="math display">\[
\begin{align}
&amp; P(G=h) \\
&amp;= \sum_{s \in \{b,i,a\}} \sum_{l \in \{m,o\}} P(G=h \mid S=s, L=l) \cdot P(S=s) \cdot P(L=l)
\end{align}
\]</span></p>
<div class="small">
<p>In plain language: To find the overall probability of high ghost activity, consider every possible combination of player skill and level type, multiply the probability of that combination by the probability of high ghost activity given that combination, then sum all these products.</p>
</div>
</section>
</section>
<section id="calculating-joint-probabilities" class="level3">
<h3 data-anchor-id="calculating-joint-probabilities">Calculating joint probabilities</h3>
<p>To calculate the joint probability of multiple variables, we use the <strong>chain rule of probability</strong> applied to the structure of the Bayesian network.</p>
<p>For any Bayesian network with variables <span class="math inline">\(X_1, X_2, ..., X_n\)</span>, the joint probability is:</p>
<p><span class="math display">\[P(X_1=x_1, X_2=x_2, ..., X_n=x_n) = \prod_{i=1}^{n} P(X_i=x_i \mid \text{Parents}(X_i))\]</span></p>
<div class="small">
<p>In our Pacman example, to calculate the probability of advanced player skill, original level, and high ghost activity:</p>
<p><span class="math display">\[P(S=a, L=o, G=h) = P(S=a) \cdot P(L=o) \cdot P(G=h \mid S=a, L=o)\]</span></p>
<p>If <span class="math inline">\(P(S=a) = 0.30\)</span>, <span class="math inline">\(P(L=o) = 0.40\)</span>, and <span class="math inline">\(P(G=h \mid S=a, L=o) = 0.70\)</span></p>
<p>then <span class="math inline">\(P(S=a, L=o, G=h) = 0.30 \cdot 0.40 \cdot 0.70 = 0.084\)</span></p>
<p>This calculation works because the Bayesian network structure encodes conditional independence assumptions, allowing us to decompose the joint probability into this product.</p>
</div>
</section>
<section id="calculating-conditional-probabilities" class="level3">
<h3 data-anchor-id="calculating-conditional-probabilities">Calculating conditional probabilities</h3>
<p>Conditional probabilities ask questions like “given that X has occurred, what is the probability of Y?”</p>
<section id="using-bayes-rule-directly" class="level4">
<h4 data-anchor-id="using-bayes-rule-directly">Using Bayes’ rule directly</h4>
<p>For simple queries, we can apply Bayes’ rule:</p>
<p><span class="math display">\[P(Y=y \mid X=x) = \frac{P(X=x \mid Y=y) \cdot P(Y=y)}{P(X=x)}\]</span></p>
</section>
<section id="using-joint-probabilities" class="level4">
<h4 data-anchor-id="using-joint-probabilities">Using joint probabilities</h4>
<p>More commonly in Bayesian networks, we calculate conditional probabilities using:</p>
<p><span class="math display">\[P(Y=y \mid X=x) = \frac{P(Y=y, X=x)}{P(X=x)}\]</span></p>
<div class="small">
<p>In our Pacman example we want to find the probability of player skill given ghost activity. To find <span class="math inline">\(P(S=a \mid G=h)\)</span> (the probability the player is advanced given high ghost activity):</p>
<ol type="1">
<li><p>Calculate the joint probability <span class="math inline">\(P(S=a, G=h)\)</span>: <span class="math display">\[P(S=a, G=h) = \sum_{l \in \{m,o\}} P(S=a) \cdot P(L=l) \cdot P(G=h \mid S=a, L=l)\]</span></p></li>
<li><p>Calculate the marginal probability <span class="math inline">\(P(G=h)\)</span> using marginalization as shown earlier.</p></li>
<li><p>Apply the conditional probability formula: <span class="math display">\[P(S=a \mid G=h) = \frac{P(S=a, G=h)}{P(G=h)}\]</span></p></li>
</ol>
</div>
<p>When we have evidence on multiple variables, the calculation extends naturally.</p>
<p>For example, to find <span class="math inline">\(P(S=a \mid G=h, L=o)\)</span>:</p>
<p><span class="math display">\[
\begin{align}
&amp; P(S=a \mid G=h, L=o) = \frac{P(S=a, G=h, L=o)}{P(G=h, L=o)} \\
&amp; = \frac{P(S=a) \cdot P(L=o) \cdot P(G=h \mid S=a, L=o)}{P(G=h, L=o)}
\end{align}
\]</span></p>
<p>The denominator <span class="math inline">\(P(G=h, L=o)\)</span> can be calculated by summing over all possible values of <span class="math inline">\(S\)</span>:</p>
<p><span class="math display">\[
\begin{align}
&amp; P(G=h, L=o) \\
&amp; = \sum_{s \in \{b,i,a\}} P(S=s) \cdot P(L=o) \cdot P(G=h \mid S=s, L=o)
\end{align}
\]</span></p>
</section>
</section>
<section id="law-of-total-probability" class="level3">
<h3 data-anchor-id="law-of-total-probability">Law of Total Probability</h3>
<p>The <strong>law of total probability</strong> is a fundamental rule that allows us to calculate the probability of an event by considering all the ways that event can occur. For any variable <span class="math inline">\(Y\)</span> and another variable <span class="math inline">\(X\)</span> with possible values <span class="math inline">\(x_1, x_2, ..., x_n\)</span>:</p>
<p><span class="math display">\[P(Y=y) = \sum_{i=1}^{n} P(Y=y \mid X=x_i) \cdot P(X=x_i)\]</span></p>
<p>This law naturally emerges from the marginalization process and is particularly useful when calculating probabilities of variables that are not directly connected in a Bayesian network.</p>
<div class="small">
<p>For example, to calculate <span class="math inline">\(P(R=True)\)</span> (the probability a patient requires insulin treatment) in our diabetes model, we can apply the law of total probability using HbA1c levels:</p>
<p><span class="math display">\[\begin{align}
P(R=True) &amp;= P(R=True \mid B=h) \cdot P(B=h) \\
&amp; + P(R=True \mid B=l) \cdot P(B=l) \\
&amp;= 0.7 \cdot P(B=h) + 0.1 \cdot P(B=l)
\end{align}\]</span></p>
<p>Similarly, when calculating conditional probabilities between variables that are not directly connected, like <span class="math inline">\(P(R=True \mid T=t1)\)</span>, we can use an expanded form:</p>
<p><span class="math display">\[\begin{align}
P(R=True \mid T=t1) &amp;= P(R=True \mid B=h) \cdot P(B=h \mid T=t1) \\
&amp; + P(R=True \mid B=l) \cdot P(B=l \mid T=t1)
\end{align}\]</span></p>
<p>This approach explicitly accounts for how information flows through the intermediate variable <span class="math inline">\(B\)</span> in our network.</p>
</div>
<div class="small">
<p>Note: For more complex Bayesian networks, exact inference can become computationally expensive. In such cases, we often use approximation algorithms like variable elimination<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, belief propagation<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, or sampling methods<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> to perform efficient inference.</p>
</div>
</section>
</section>
<section id="common-misconceptions" class="level1">
<h1>Common misconceptions</h1>
<p>When working with Bayesian networks, students often encounter several conceptual hurdles. Being aware of these common misconceptions can help you avoid errors in both understanding and application.</p>
<p><span class="h4"><strong>Confusing causality with conditional dependence</strong></span></p>
<ul>
<li><strong>Misconception</strong>: Assuming that an edge <span class="math inline">\(A \rightarrow B\)</span> in a Bayesian network means that <span class="math inline">\(A\)</span> is always the cause of <span class="math inline">\(B\)</span> in the real world.</li>
<li><strong>Reality</strong>: Edges represent statistical dependencies, not necessarily causation. While Bayesian networks are often built to reflect causal relationships, the arrows formally indicate conditional probability relationships.</li>
<li><strong>Example</strong>: In a medical diagnostic network, a disease node might point to a symptom node, reflecting causality. However, the same probabilistic relationship could be modeled in reverse for diagnostic reasoning.</li>
</ul>
<p><span class="h4"><strong>Misinterpreting missing edges</strong></span></p>
<ul>
<li><strong>Misconception</strong>: Assuming that if there’s no edge between nodes <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, they must be completely independent.</li>
<li><strong>Reality</strong>: Nodes without direct connections might still be dependent through other paths. Only specific independence relationships are guaranteed by the network structure (as defined by d-separation).</li>
<li><strong>Example</strong>: In our Pacman example, there’s no direct edge from Player Skill to Power Pellet, but they are still statistically dependent because Player Skill influences Ghost Activity, which influences Power Pellet collection.</li>
</ul>
<p><span class="h4"><strong>Confusing joint and conditional probabilities</strong></span></p>
<ul>
<li><strong>Misconception</strong>: Mistaking <span class="math inline">\(P(A, B)\)</span> for <span class="math inline">\(P(A \mid B)\)</span> or vice versa in calculations.</li>
<li><strong>Reality</strong>: These are fundamentally different quantities. The joint probability <span class="math inline">\(P(A, B)\)</span> represents the probability of both events occurring, while the conditional probability <span class="math inline">\(P(A \mid B)\)</span> represents the probability of <span class="math inline">\(A\)</span> occurring given that <span class="math inline">\(B\)</span> has occurred.</li>
<li><strong>Example</strong>: In the Pacman network, <span class="math inline">\(P(G=h, L=m)\)</span> is the probability of high ghost activity in a maze level, while <span class="math inline">\(P(G=h \mid L=m)\)</span> is the probability of high ghost activity given that we know we’re in a maze level.</li>
</ul>
<p><span class="h4"><strong>Neglecting the chain rule factorization</strong></span></p>
<ul>
<li><strong>Misconception</strong>: Calculating the joint probability of all variables incorrectly, often by multiplying probabilities without respecting the network structure.</li>
<li><strong>Reality</strong>: In a Bayesian network, the joint probability factorizes according to the network structure: <span class="math inline">\(P(X_1, X_2, ..., X_n) = \prod_{i=1}^{n} P(X_i \mid \text{Parents}(X_i))\)</span>.</li>
<li><strong>Example</strong>: In our diabetes example, <span class="math inline">\(P(A, T, B, R) = P(A) \cdot P(T) \cdot P(B \mid A, T) \cdot P(R \mid B)\)</span>, not simply the product of all marginal probabilities.</li>
</ul>
<p><span class="h4"><strong>Misapplying Bayes’ rule</strong></span></p>
<ul>
<li><strong>Misconception</strong>: Incorrectly applying Bayes’ rule, especially when dealing with multiple variables.</li>
<li><strong>Reality</strong>: Bayes’ rule states that <span class="math inline">\(P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}\)</span>. When multiple variables are involved, careful attention must be paid to which variables are being conditioned on.</li>
<li><strong>Example</strong>: To find <span class="math inline">\(P(S=a \mid G=h)\)</span> in the Pacman example, we need <span class="math inline">\(P(G=h \mid S=a) \cdot P(S=a) / P(G=h)\)</span>, where <span class="math inline">\(P(G=h)\)</span> must be calculated by marginalizing over both <span class="math inline">\(S\)</span> and <span class="math inline">\(L\)</span>.</li>
</ul>
<p>Remember that these misconceptions often arise from the complexity of probabilistic thinking rather than simple carelessness. Taking time to clearly write out the appropriate formulas and checking each step of your calculations can help avoid many of these common errors.</p>
</section>
<section id="exercises" class="level1 headline-only">
<h1 class="headline-only">Exercises</h1>
<section id="basic-probabilities" class="level2">
<h2 data-anchor-id="basic-probabilities">Basic probabilities</h2>
<p>Consider a simple Bayesian network with two binary variables: Cloudy (<span class="math inline">\(C\)</span>) and Rain (<span class="math inline">\(R\)</span>), where Cloudy is a parent of Rain.</p>
<p>The conditional probability tables are:</p>
<ul>
<li><span class="math inline">\(P(C=\text{true}) = 0.3\)</span></li>
<li><span class="math inline">\(P(R=\text{true} \mid C=\text{true}) = 0.8\)</span></li>
<li><span class="math inline">\(P(R=\text{true} \mid C=\text{false}) = 0.1\)</span></li>
</ul>
<p>What is the joint probability <span class="math inline">\(P(C=\text{true}, R=\text{true})\)</span>?</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Solution notes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Using the chain rule for Bayesian networks:</p>
<p><span class="math display">\[
\begin{flalign}
P(C=\text{true}, R=\text{true}) &amp;= P(R=\text{true} \mid C=\text{true}) \cdot P(C=\text{true}) \\
&amp;= 0.8 \cdot 0.3 = 0.24
\end{flalign}
\]</span></p>
<p>The probability of it being both cloudy and rainy is 0.24 or 24%.</p>
</div>
</div>
</div>
<p>What is the marginal probability <span class="math inline">\(P(R=\text{true})\)</span>?</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Solution notes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To find the marginal probability, we need to marginalize over all possible values of <span class="math inline">\(C\)</span>:</p>
<p><span class="math display">\[
\begin{flalign}
P(R=\text{true}) &amp;= P(R=\text{true} \mid C=\text{true}) \cdot P(C=\text{true}) \\
&amp; + P(R=\text{true} \mid C=\text{false}) \cdot P(C=\text{false})
\end{flalign}
\]</span> <span class="math display">\[
\begin{flalign}
P(R=\text{true}) &amp;= 0.8 \cdot 0.3 + 0.1 \cdot 0.7 \\
&amp;= 0.24 + 0.07 = 0.31
\end{flalign}
\]</span></p>
<p>The probability of rain, regardless of cloudiness, is 0.31 or 31%.</p>
</div>
</div>
</div>
<p>Using the weather network from Questions 1 and 2, what is the probability that it’s cloudy given that it’s raining? That is, calculate <span class="math inline">\(P(C=\text{true} \mid R=\text{true})\)</span>.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Solution notes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We use Bayes’ rule:</p>
<p><span class="math inline">\(P(C=\text{true} \mid R=\text{true}) = \frac{P(R=\text{true} \mid C=\text{true}) \cdot P(C=\text{true})}{P(R=\text{true})}\)</span></p>
<p>We already calculated:</p>
<p><span class="math inline">\(P(R=\text{true} \mid C=\text{true}) = 0.8\)</span> <span class="math inline">\(P(C=\text{true}) = 0.3\)</span> <span class="math inline">\(P(R=\text{true}) = 0.31\)</span></p>
<p>So: <span class="math inline">\(P(C=\text{true} \mid R=\text{true}) = \frac{0.8 \cdot 0.3}{0.31} = \frac{0.24}{0.31} \approx 0.774\)</span></p>
<p>There’s about a 77.4% chance it’s cloudy, given that it’s raining.</p>
</div>
</div>
</div>
</section>
<section id="conditional-independence" class="level2">
<h2 data-anchor-id="conditional-independence">Conditional independence</h2>
<p>Consider a Bayesian network with three binary variables: Season (<span class="math inline">\(S\)</span>), Temperature (<span class="math inline">\(T\)</span>), and Ice Cream Sales (<span class="math inline">\(I\)</span>), structured as <span class="math inline">\(S \rightarrow T \rightarrow I\)</span> (Season affects Temperature, which affects Ice Cream Sales).</p>
<p>Are Season (<span class="math inline">\(S\)</span>) and Ice Cream Sales (<span class="math inline">\(I\)</span>) conditionally independent given Temperature (<span class="math inline">\(T\)</span>)? Explain your answer.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Solution notes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Yes, Season (<span class="math inline">\(S\)</span>) and Ice Cream Sales (<span class="math inline">\(I\)</span>) are conditionally independent given Temperature (<span class="math inline">\(T\)</span>).</p>
<p>In a Bayesian network, a variable is conditionally independent of its non-descendants given its parents. Looking at the structure <span class="math inline">\(S \rightarrow T \rightarrow I\)</span>, when we know the value of Temperature (<span class="math inline">\(T\)</span>), the value of Season (<span class="math inline">\(S\)</span>) provides no additional information about Ice Cream Sales (<span class="math inline">\(I\)</span>). This is because all the influence of Season on Ice Cream Sales flows through Temperature.</p>
<p>Mathematically, this means: <span class="math inline">\(P(I \mid T, S) = P(I \mid T)\)</span></p>
<p>For example, if we know it’s hot (high Temperature), then Ice Cream Sales are likely high regardless of whether it’s summer or winter. The season only affects ice cream sales by affecting temperature.</p>
</div>
</div>
</div>
</section>
<section id="d-separation-1" class="level2">
<h2 data-anchor-id="d-separation-1">D-Separation</h2>
<p>Consider a Bayesian network with the following structure:</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A--&gt;C
    B--&gt;C
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Are variables <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> independent? What about if we condition on <span class="math inline">\(C\)</span>?</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Solution notes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are marginally independent (unconditionally independent) because there’s no direct path between them and no common cause. This means <span class="math inline">\(P(A,B) = P(A) \cdot P(B)\)</span>.</p>
<p>However, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> become conditionally dependent when we condition on their common effect <span class="math inline">\(C\)</span>. This is known as “explaining away” - once we know the value of the effect <span class="math inline">\(C\)</span>, knowing the value of one cause provides information about the other cause. This means <span class="math inline">\(P(A \mid B, C) \neq P(A \mid C)\)</span>.</p>
<p>For example, if <span class="math inline">\(A\)</span> represents “Sprinkler On”, <span class="math inline">\(B\)</span> represents “Rain”, and <span class="math inline">\(C\)</span> represents “Grass is Wet”, then Sprinkler and Rain are independent events. But if we know the grass is wet, learning the sprinkler was off increases the probability that it rained (explaining away).</p>
</div>
</div>
</div>
</section>
<section id="pacman" class="level2">
<h2 data-anchor-id="pacman">Pacman</h2>
<p>Please use the given <a href="#tbl-pacman">JPD</a> to answer the questions raised <a href="#example-pacman">here</a>.</p>
<div class="notes">
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Solution notes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>What’s the probability of encountering a ghost during gameplay? <span class="math inline">\(P(G)\)</span>?</strong></p>
<p>To find <span class="math inline">\(P(G)\)</span>, we need to sum all probabilities where <span class="math inline">\(G\)</span> is True:</p>
<p><span class="math display">\[P(G) = P(G,P,L) + P(G,P,\neg L) + P(G,\neg P,L) + P(G,\neg P,\neg L)\]</span> <span class="math display">\[P(G) = 0.15 + 0.05 + 0.05 + 0.25\]</span> <span class="math display">\[P(G) = 0.50\]</span></p>
<p>So there’s a 50% chance of encountering a ghost during gameplay.</p>
<p><strong>What’s the probability of both encountering a ghost and eating a power pellet? <span class="math inline">\(P(G,P)\)</span>?</strong></p>
<p>To find <span class="math inline">\(P(G,P)\)</span>, we need to sum all probabilities where both <span class="math inline">\(G\)</span> and <span class="math inline">\(P\)</span> are True:</p>
<p><span class="math display">\[P(G,P) = P(G,P,L) + P(G,P,\neg L)\]</span> <span class="math display">\[P(G,P) = 0.15 + 0.05\]</span> <span class="math display">\[P(G,P) = 0.20\]</span></p>
<p>So there’s a 20% chance of both encountering a ghost and eating a power pellet.</p>
<p><strong>If a player encounters a ghost, what’s the probability they’ll complete the level? <span class="math inline">\(P(L|G)\)</span>?</strong></p>
<p>This is a conditional probability, calculated as:</p>
<p><span class="math display">\[P(L|G) = \frac{P(L,G)}{P(G)}\]</span></p>
<p><span class="math display">\[P(L,G) = P(G,P,L) + P(G,\neg P,L) = 0.15 + 0.05 = 0.20\]</span> <span class="math display">\[P(G) = 0.50\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[P(L|G) = \frac{0.20}{0.50} = 0.40\]</span></p>
<p>So if a player encounters a ghost, there’s a 40% chance they’ll complete the level.</p>
<p><strong>If a player eats a power pellet, what’s the probability they’ll encounter a ghost? <span class="math inline">\(P(G|P)\)</span>?</strong></p>
<p>This conditional probability is:</p>
<p><span class="math display">\[P(G|P) = \frac{P(G,P)}{P(P)}\]</span></p>
<p><span class="math display">\[P(G,P) = 0.20\]</span></p>
<p><span class="math display">\[
\begin{flalign}
P(P) &amp;= P(G,P,L) + P(G,P,\neg L) + P(\neg G,P,L) + P(\neg G,P,\neg L) \\
&amp;= 0.15 + 0.05 + 0.20 + 0.10 = 0.50
\end{flalign}
\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[P(G|P) = \frac{0.20}{0.50} = 0.40\]</span></p>
<p>So if a player eats a power pellet, there’s a 40% chance they’ll encounter a ghost.</p>
<hr>
<p><strong>If a player encounters a ghost and eats a power pellet, what’s the probability they’ll complete the level? <span class="math inline">\(P(L|G,P)\)</span>?</strong></p>
<p>This conditional probability is:</p>
<p><span class="math display">\[P(L|G,P) = \frac{P(L,G,P)}{P(G,P)}\]</span></p>
<p><span class="math display">\[P(L,G,P) = 0.15\]</span> <span class="math display">\[P(G,P) = 0.20\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[P(L|G,P) = \frac{0.15}{0.20} = 0.75\]</span></p>
<p>So if a player both encounters a ghost and eats a power pellet, there’s a 75% chance they’ll complete the level.</p>
</div>
</div>
</div>
</div>
</section>
<section id="medical-diagnosis" class="level2">
<h2 data-anchor-id="medical-diagnosis">Medical diagnosis</h2>
<p>Consider a diagnostic model for diabetes with the following variables and conditional probability tables (CPTs):</p>
<ul>
<li><strong>Age (<span class="math inline">\(A\)</span>)</strong>: Patient’s age group
<ul>
<li>Young (<span class="math inline">\(y\)</span>): 20% (under 40 years)</li>
<li>Middle-aged (<span class="math inline">\(m\)</span>): 40% (40-60 years)</li>
<li>Older (<span class="math inline">\(o\)</span>): 40% (over 60 years)</li>
</ul></li>
<li><strong>Diabetes Type (<span class="math inline">\(T\)</span>)</strong>: Classification of diabetes
<ul>
<li>Type 1 (<span class="math inline">\(t1\)</span>): 30% (autoimmune form, typically insulin-dependent)</li>
<li>Type 2 (<span class="math inline">\(t2\)</span>): 70% (insulin resistance form, often lifestyle-related)</li>
</ul></li>
<li><strong>HbA1c Level (<span class="math inline">\(B\)</span>)</strong>: Glycated hemoglobin blood test result
<ul>
<li>Low (<span class="math inline">\(l\)</span>): Within normal or slightly elevated range (below 7.0%)</li>
<li>High (<span class="math inline">\(h\)</span>): Significantly elevated (7.0% or higher)</li>
<li>Probabilities depend on Age and Diabetes Type as follows:
<ul>
<li>Young patient with Type 1 diabetes: <span class="math inline">\(P(B=h|A=y,T=t1) = 0.1\)</span></li>
<li>Young patient with Type 2 diabetes: <span class="math inline">\(P(B=h|A=y,T=t2) = 0.3\)</span></li>
<li>Middle-aged patient with Type 1 diabetes: <span class="math inline">\(P(B=h|A=m,T=t1) = 0.6\)</span></li>
<li>Middle-aged patient with Type 2 diabetes: <span class="math inline">\(P(B=h|A=m,T=t2) = 0.5\)</span></li>
<li>Older patient with Type 1 diabetes: <span class="math inline">\(P(B=h|A=o,T=t1) = 0.7\)</span></li>
<li>Older patient with Type 2 diabetes: <span class="math inline">\(P(B=h|A=o,T=t2) = 0.9\)</span></li>
</ul></li>
</ul></li>
<li><strong>Insulin Treatment (<span class="math inline">\(R\)</span>)</strong>: Whether insulin therapy is required
<ul>
<li>Required (True): Probability depends on HbA1c level
<ul>
<li>With high HbA1c: <span class="math inline">\(P(R=True|B=h) = 0.7\)</span></li>
<li>With low HbA1c: <span class="math inline">\(P(R=True|B=l) = 0.1\)</span></li>
</ul></li>
</ul></li>
</ul>
<p>This Bayesian network is structured such that Age and Diabetes Type are parent nodes for HbA1c Level, and HbA1c Level is the parent node for Insulin Treatment.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A["Age (A)"] --&gt; B["HbA1c Level (B)"]
    T["Diabetes Type (T)"] --&gt; B
    B --&gt; R["Insulin Treatment (R)"]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p><span class="h4"><strong>Tasks</strong></span></p>
<ol type="1">
<li>Calculate the marginal probability <span class="math inline">\(P(B=h)\)</span>, i.e., the probability that a patient has a high HbA1c level.</li>
<li>Calculate the joint probabilitySo <span class="math inline">\(P(A=o,T=t1,B=h,R=True)\)</span>, i.e., the probability of an older patient with Type 1 diabetes who has a high HbA1c level and requires insulin treatment.</li>
<li>Calculate the joint probability <span class="math inline">\(P(A=o,B=h,R=True)\)</span>, i.e., the probability of an older patient with a high HbA1c level who requires insulin treatment.</li>
<li>Calculate the conditional probability <span class="math inline">\(P(R=True|T=t1)\)</span>, i.e., the probability that a patient with Type 1 diabetes requires insulin treatment.</li>
</ol>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Hint for Task 1
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Remember to marginalize over all possible combinations of parent variables (Age and Diabetes Type). Start by writing out the full marginalization equation, then convert joint probabilities to conditional probabilities using the chain rule.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Hint for Task 4
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This requires application of the total probability theorem. Since <span class="math inline">\(R\)</span> and <span class="math inline">\(T\)</span> are conditionally independent given <span class="math inline">\(B\)</span>, you’ll need to compute:</p>
<p><span class="math display">\[
\begin{flalign}
P(R=True|T=t1) &amp;= P(R=True|B=l) \cdot P(B=l|T=t1) \\
&amp; + P(R=True|B=h) \cdot P(B=h|T=t1)
\end{flalign}
\]</span></p>
<p>But <span class="math inline">\(P(B=l|T=t1)\)</span> and <span class="math inline">\(P(B=h|T=t1)\)</span> are not directly available in the CPTs and require marginalization over Age.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Solution notes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="h4"><strong>Task 1: Calculate <span class="math inline">\(P(B=h)\)</span></strong></span></p>
<p>The parents of variable <em>HbA1c Level (B)</em> are <em>Age (A)</em> and <em>Diabetes Type (T)</em>. Therefore we must marginalize over the values of these two parent variables:</p>
<p><span class="math display">\[\begin{align}
P(B=h) &amp;= P(B=h,T=t1,A=y) + P(B=h,T=t2,A=y) \\
       &amp;+ P(B=h,T=t1,A=m) + P(B=h,T=t2,A=m) \\
       &amp;+ P(B=h,T=t1,A=o) + P(B=h,T=t2,A=o)
\end{align}\]</span></p>
<p>To simplify notation, we’ll omit the variable names and keep only the values:</p>
<p><span class="math display">\[\begin{align}
P(h) &amp;= P(h,t1,y) + P(h,t2,y) \\
     &amp;+ P(h,t1,m) + P(h,t2,m) \\
     &amp;+ P(h,t1,o) + P(h,t2,o)
\end{align}\]</span></p>
<p>Converting joint probabilities to conditional probabilities:</p>
<p><span class="math display">\[\begin{align}
P(h) &amp;= P(h \mid t1,y) \cdot P(t1,y) + P(h \mid t2,y) \cdot P(t2,y) \\
     &amp;+ P(h \mid t1,m) \cdot P(t1,m) + P(h \mid t2,m) \cdot P(t2,m) \\
     &amp;+ P(h \mid t1,o) \cdot P(t1,o) + P(h \mid t2,o) \cdot P(t2,o)
\end{align}\]</span></p>
<p>Since Age and Diabetes Type are independent:</p>
<p><span class="math display">\[\begin{align}
P(h) &amp;= P(h \mid t1,y) \cdot P(t1) \cdot P(y) + P(h \mid t2,y) \cdot P(t2) \cdot P(y) \\
     &amp;+ P(h \mid t1,m) \cdot P(t1) \cdot P(m) + P(h \mid t2,m) \cdot P(t2) \cdot P(m) \\
     &amp;+ P(h \mid t1,o) \cdot P(t1) \cdot P(o) + P(h \mid t2,o) \cdot P(t2) \cdot P(o)
\end{align}\]</span></p>
<p>Substituting the values from the CPTs:</p>
<p><span class="math display">\[\begin{align}
P(h) &amp;= 0.1 \cdot 0.3 \cdot 0.2 + 0.3 \cdot 0.7 \cdot 0.2 \\
     &amp;+ 0.6 \cdot 0.3 \cdot 0.4 + 0.5 \cdot 0.7 \cdot 0.4 \\
     &amp;+ 0.7 \cdot 0.3 \cdot 0.4 + 0.9 \cdot 0.7 \cdot 0.4 \\
     &amp;= 0.006 + 0.042 + 0.072 + 0.14 + 0.084 + 0.252 \\
     &amp;= 0.596
\end{align}\]</span></p>
<p>Therefore, the probability of a patient having a high HbA1c level is 59.6%.</p>
<p><span class="h4"><strong>Task 2: Calculate <span class="math inline">\(P(A=o,T=t1,B=h,R=True)\)</span></strong></span></p>
<p>According to the chain rule of probability applied to the Bayesian network structure (as introduced in the section <a href="#calculating-joint-probabilities">calculating joint probabilities</a>), we can calculate this joint probability as:</p>
<p><span class="math display">\[\begin{align}
P(o,t1,h,r) &amp;= P(r \mid h) \cdot P(h \mid o,t1) \cdot P(o) \cdot P(t1) \\
&amp;= 0.7 \cdot 0.7 \cdot 0.4 \cdot 0.3 \\
&amp;= 0.0588
\end{align}\]</span></p>
<p>Therefore, the probability of an older patient with Type 1 diabetes who has a high HbA1c level and requires insulin treatment is 5.88%.</p>
<p><span class="h4"><strong>Task 3: Calculate <span class="math inline">\(P(A=o,B=h,R=True)\)</span></strong></span></p>
<p>In this case, we don’t know the value of Diabetes Type, so we need to marginalize over it:</p>
<p><span class="math display">\[\begin{align}
P(o,h,r) &amp;= P(r \mid h) \cdot \left[ P(h \mid o,t1) \cdot P(o) \cdot P(t1) + P(h \mid o,t2) \cdot P(o) \cdot P(t2)\right] \\
&amp;= 0.7 \cdot \left[ 0.7 \cdot 0.4 \cdot 0.3 + 0.9 \cdot 0.4 \cdot 0.7 \right] \\
&amp;= 0.7 \cdot [0.084 + 0.252] \\
&amp;= 0.7 \cdot 0.336 \\
&amp;= 0.2352
\end{align}\]</span></p>
<p>Therefore, the probability of an older patient with a high HbA1c level who requires insulin treatment is 23.52%.</p>
<p><span class="h4"><strong>Task 4: Calculate <span class="math inline">\(P(R=True \mid T=t1)\)</span></strong></span></p>
<p>Looking at the Bayesian network structure, we can determine that <span class="math inline">\(R\)</span> and <span class="math inline">\(T\)</span> are conditionally independent given <span class="math inline">\(B\)</span>, as explained in the conditional independence section of the lecture. Therefore, we can apply the law of total probability (which follows from the marginalization process we’ve learned):</p>
<p><span class="math display">\[P(r \mid t1) = P(r\mid l) \cdot P(l \mid t1) + P(r \mid h) \cdot P(h \mid t1)\]</span></p>
<p>We know <span class="math inline">\(P(r\mid l) = 0.1\)</span> and <span class="math inline">\(P(r \mid h) = 0.7\)</span> from the CPTs, but we need to calculate <span class="math inline">\(P(l \mid t1)\)</span> and <span class="math inline">\(P(h \mid t1)\)</span>.</p>
<p>Since <span class="math inline">\(B\)</span> depends on both <span class="math inline">\(T\)</span> and <span class="math inline">\(A\)</span>, we need to marginalize over <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[\begin{align}
P(h \mid t1) &amp;= P(h\mid t1,y) \cdot P(y) + P(h\mid t1,m) \cdot P(m) + P(h\mid t1,o) \cdot P(o) \\
&amp;= 0.1 \cdot 0.2 + 0.6 \cdot 0.4 + 0.7 \cdot 0.4 \\
&amp;= 0.02 + 0.24 + 0.28 \\
&amp;= 0.54
\end{align}\]</span></p>
<p>Since <span class="math inline">\(P(l \mid t1) = 1 - P(h \mid t1) = 1 - 0.54 = 0.46\)</span></p>
<p>Now we can calculate <span class="math inline">\(P(r \mid t1)\)</span>:</p>
<p><span class="math display">\[\begin{align}
P(r \mid t1) &amp;= 0.1 \cdot 0.46 + 0.7 \cdot 0.54 \\
&amp;= 0.046 + 0.378 \\
&amp;= 0.424
\end{align}\]</span></p>
<p>Therefore, the probability that a patient with Type 1 diabetes requires insulin treatment is 42.4%.</p>
</div>
</div>
</div>
</section>
</section>
<section id="literature" class="level1">
<h1>Literature</h1>
<div id="refs" role="list">

</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Variable elimination is an algorithm that computes exact marginal probabilities by systematically “eliminating” variables through summation, reducing computational complexity by exploiting conditional independence relationships.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Belief propagation (also known as message passing) works by having nodes in the network exchange “messages” containing probability information, enabling efficient inference even in networks with loops.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Sampling methods like Markov Chain Monte Carlo (MCMC) and Gibbs sampling approximate probability distributions by generating random samples, making them useful for very large networks where exact inference is intractable.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/awe-hnu\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2026, Andy Weeger
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">

<div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../../index.html">
<p>Start</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../../../about.html">
<p>About</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://www.hnu.de" target="_blank">
<p>HNU</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../../../imprint.html">
<p>Imprint</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>