<!DOCTYPE html>
<html lang="en"><head>
<link href="../../../../assets/favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/quarto-contrib/countdown-0.5.0/countdown.js"></script>
<link href="../../../../site_libs/quarto-contrib/countdown-0.5.0/countdown.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Andy Weeger">
  <meta name="dcterms.date" content="2025-09-23">
  <title>awe.lectures – Intelligence and Affection</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../../../site_libs/revealjs/dist/theme/quarto-a4ff407c4868b4614656cee7a316f8fb.css">
  
  <script type="text/javascript" charset="UTF-8">
  document.addEventListener('DOMContentLoaded', function () {
  cookieconsent.run({
    "notice_banner_type":"interstitial",
    "consent_type":"express",
    "palette":"dark",
    "language":"en",
    "page_load_consent_levels":["strictly-necessary"],
    "notice_banner_reject_button_hide":false,
    "preferences_center_close_button_hide":false,
    "website_name":""
    ,
  "language":"en"
    });
  });
  </script> 
    
  <link href="../../../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <!-- Countdown Global CSS -->
  <style text="text/css">:root {--countdown-font-size: 2rem;--countdown-margin: 30px;--countdown-border-width: 1px;--countdown-border-radius: 0;}</style>
<meta property="og:title" content="Intelligence and Affection – awe.lectures">
<meta property="og:description" content="Future Technologies &amp; Media (FTM)">
<meta property="og:site_name" content="awe.lectures">
</head>
<body class="quarto-light">
<div class="footer custom">

    <div class="version">
        V1.1
    </div>

    <div class="footnote">
        Andy Weeger / Neu-Ulm University of Applied Sciences Neu-Ulm / Future Technologies and Media (FTM)
   </div>

    <div class="logo">
        <svg viewbox="0 0 125 39" xmlns="http://www.w3.org/2000/svg" version="1.1" space="preserve">
            <g>
             <title>HNU</title>
             <path d="m8.58222,0l7.5,0l0,12.7l-7.5,0l0,-12.7zm7.4,24.1l6.5,0l0,-3l-13.9,0l0,15.4l-3.9,0l0,2.3l11.3,0l0,-14.7zm24,-24.1l-7.4,0l0,36.5l-3.9,0l0,2.3l11.3,0l0,-38.8zm60.2,28.4c1.1,0 2.1,-0.1 2.9,-0.4c-1.3,-1.2 -1.9,-3.2 -1.9,-6.4l0,-21.6l-7.4,0l0,19.3c0,5.9 1.5,9.1 6.4,9.1z" id="svg_1"></path>
             <path d="m96.78222,37c2.8,1.6 6.4,2.5 10.6,2.5c11.2,0 17.7,-6 17.7,-16.3l0,-23.2l-7.4,0l0,21.1c0,11.3 -7.7,16.1 -17.6,16.1c-1.1,0 -2.2,-0.1 -3.3,-0.2zm-30.8,-27l-7.1,-10l-7.3,0l14.4,20.4l0,-10.4zm17,28.8l0,-38.8l-7.5,0l0,36.5l-3.5,0l1.7,2.3l9.3,0zm-24,-14.8l-8,-10.8l0,23.3l-3,0l0,2.3l11,0l0,-14.8z" id="svg_2"></path>
            </g>   
        </svg>
    </div>

</div>
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-color="#0333ff" data-background-image="../assets/bg.jpg" data-background-opacity="1" data-background-size="cover" class="quarto-title-block">
  <h1 class="title">Intelligence and Affection</h1>
  <p class="subtitle">Future Technologies &amp; Media (FTM)</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Andy Weeger 
</div>
        <p class="quarto-title-affiliation">
            Neu-Ulm University of Applied Sciences
          </p>
    </div>
</div>

  <p class="date">September 23, 2025</p>
</section>
<section>
<section id="revision" class="title-slide slide level1 headline-only vertical-center" data-background-color="#0333ff">
<h1>Revision</h1>

</section>
<section id="hypothesis-1" class="title-slide slide level2" data-background-color="#0333ff">
<h2>Hypothesis 1</h2>
<div class="large">
<p>Emerging information technologies enable <strong>multimodal</strong> and <strong>immersive</strong> systems.</p>
</div>
</section>

<section id="multimodality" class="title-slide slide level2">
<h2>Multimodality</h2>
<div class="large">
<p>Multimodality refers to the use of <span class="link-color">multiple modes of communication</span> to to create meaning.</p>
</div>
<div class="fragment">
<p>Multimodality implies that the use of several means of communication contributes to a better overall understanding of a message.</p>

</div>
<aside><div>
<p><span class="citation" data-cites="adami2016introducing">Adami (<a href="#/literature" role="doc-biblioref" onclick="">2016</a>)</span></p>
</div></aside></section>

<section id="immersion" class="title-slide slide level2">
<h2>Immersion</h2>
<div class="medium">
<p>Immersion refers to the state of <span class="link-color">being deeply engaged, absorbed, or submerged in an environment</span>, either physically or mentally.</p>
</div>
<div class="fragment">
<p>Immersion implies that the consciousness of the immersed person is detached from their physical self. Immersiveness is the quality or degree of being immersive.</p>

</div>
<aside><div>
<p><span class="citation" data-cites="suh2018state">Suh &amp; Prophet (<a href="#/literature" role="doc-biblioref" onclick="">2018</a>)</span>, <span class="citation" data-cites="lee2013presence">Lee et al. (<a href="#/literature" role="doc-biblioref" onclick="">2013</a>)</span></p>
</div></aside></section>

<section id="interdependency" class="title-slide slide level2">
<h2>Interdependency</h2>
<div class="medium">
<p>Stimuli that determine the <span class="link-color">immersiveness</span> of environments created by technology are <span class="link-color">multimodal</span>.</p>
</div>
<div class="medium">
<p><span class="fragment fade-in-then-semi-out">Visual,</span> <span class="fragment fade-in-then-semi-out">auditory,</span> <span class="fragment fade-in-then-semi-out">tactile,</span> <span class="fragment fade-in-then-semi-out">olfactory, and</span> <span class="fragment fade-in-then-semi-out">interactive.</span></p>
</div>
</section>
</section>
<section id="hypothesis-2" class="title-slide slide level1" data-background-color="#0333ff">
<h1>Hypothesis 2</h1>
<div class="large">
<p>Emerging information technologies enable <strong>intelligent</strong> and <strong>affective</strong> systems.</p>
</div>
</section>

<section>
<section id="intelligence" class="title-slide slide level1 headline-only">
<h1>Intelligence</h1>

</section>
<section id="discussion" class="title-slide slide level2 discussion-slide" data-background-color="#000">
<h2>Discussion</h2>
<div class="medium">
<p>What do we mean<br>
by <strong>intelligence</strong>?</p>
</div>
<p>Provide a description that outlines what intelligence could mean.</p>
</section>

<section id="human-intelligence" class="title-slide slide level2">
<h2>Human intelligence</h2>
<blockquote>
<p><strong>Human intelligence</strong> “covers the capacity to learn, reason, and adaptively perform effective actions within an environment, based on existing knowledge. This allows humans to adapt to changing environments and act towards achieving their goals.” <em><span class="citation" data-cites="dellermann2019hybrid">Dellermann et al. (<a href="#/literature" role="doc-biblioref" onclick="">2019, p. 632</a>)</span></em></p>
</blockquote>
<div class="fragment">
<p><span class="citation" data-cites="sternberg1985beyond">Sternberg et al. (<a href="#/literature" role="doc-biblioref" onclick="">1985</a>)</span> proposes three distinctive dimensions:</p>
<ul>
<li class="fragment"><strong>Componential intelligence</strong><br>
the ability to take apart problems and being able to see solutions not often seen</li>
<li class="fragment"><strong>Experiential intelligence</strong><br>
the ability to learn and adapt through evolutionary experience</li>
<li class="fragment"><strong>Contextual intelligence</strong><br>
the ability to create an ideal fit between the environment by adaptation, shaping, and selection</li>
</ul>
</div>
</section>

<section id="artificial-intelligence" class="title-slide slide level2">
<h2>Artificial intelligence</h2>
<blockquote>
<p>‘AI system’ means a machine-based systems designed to <span class="fragment highlight-current-blue">operate with varying levels of autonomy</span> and that may <span class="fragment highlight-current-blue">exhibit adaptiveness after deployment</span> and that, <span class="fragment highlight-current-blue">for explicit or implicit objectives, infers, from the input it received</span>, how to <span class="fragment highlight-current-blue">generate output such as content, predictions, recommendations, or decisions</span>, that can <span class="fragment highlight-current-blue">influence physical or virtual environment</span> <span class="citation" data-cites="euAIAct2024">(<a href="#/literature" role="doc-biblioref" onclick="">European Commission, 2024</a>)</span>.</p>
</blockquote>
<div class="fragment">
<p>Based on this definition, three main properties of intelligent agents can be distinguished:</p>
</div>
<div class="medium">
<p><span class="fragment fade-in-then-semi-out">Capacity to work in a complex environment<a href="#/footnotes" class="footnote-ref" id="fnref1" role="doc-noteref" data-footnote-href="#/fn1" onclick=""><sup>1</sup></a>,</span> <span class="fragment fade-in-then-semi-out">cognitive abilities<a href="#/footnotes" class="footnote-ref" id="fnref2" role="doc-noteref" data-footnote-href="#/fn2" onclick=""><sup>2</sup></a>, and</span> <span class="fragment fade-in-then-semi-out">complex behavior<a href="#/footnotes" class="footnote-ref" id="fnref3" role="doc-noteref" data-footnote-href="#/fn3" onclick=""><sup>3</sup></a>.</span></p>
</div>
<aside></aside></section>
</section>
<section>
<section id="complex-environments" class="title-slide slide level1 headline-only">
<h1>Complex environments</h1>

</section>
<section id="agents" class="title-slide slide level2">
<h2>Agents</h2>
<div class="medium">
<p>An agent is anything that can be viewed as perceiving its <span class="fragment custom bold">environment</span> through <span class="fragment custom bold">sensors</span> and acting upon that environment through <span class="fragment custom bold">actuators.</span></p>
</div>
</section>

<section id="agents-and-environments" class="title-slide slide level2">
<h2>Agents and environments</h2>
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/actor.svg" class="img-fluid"></p>
<figcaption>Necessary components to interact with complex environments</figcaption>
</figure>
</div>
</section>

<section id="example" class="title-slide slide level2">
<h2>Example</h2>
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/thermostat.svg" class="img-fluid"></p>
<figcaption>Example for an intelligent systems</figcaption>
</figure>
</div>
</section>

<section id="task-environment" class="title-slide slide level2">
<h2>Task environment</h2>
<p>When designing an intelligent system, the <strong>task environment</strong> (i.e., the problem) must be specified as fully as possible, including</p>
<div class="medium">
<p><span class="fragment">The <span class="link-color">p</span>erformance measure</span><br>
<span class="fragment">the task <span class="link-color">e</span>nvironment,</span><br>
<span class="fragment">the <span class="link-color">a</span>ctuators,</span><br>
<span class="fragment">and the <span class="link-color">s</span>ensors</span></p>
</div>
<div class="fragment">
<p><span class="citation" data-cites="RusselNorvig2022AIMA">Russel &amp; Norvig (<a href="#/literature" role="doc-biblioref" onclick="">2022</a>)</span> call the task environment PEAS.</p>
</div>
<aside class="notes">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Example of an PEAS description</strong></p>
</div>
<div class="callout-content">
<p>Task environment of a taxi driver agent</p>
<ul>
<li><strong>P</strong>: Safe, fast, legal, comfortable, maximize profits, minimize impact on other road users</li>
<li><strong>E</strong>: Roads, other road users, police, pedestrians, customers, weather</li>
<li><strong>A</strong>: Steering, accelerator, brake, signal horn, display, speech</li>
<li><strong>S</strong>: Cameras, radar, speedometer, GPS, engine, sensors, accelerometer, microphones, touchscreen</li>
</ul>
<p>Source: <span class="citation" data-cites="RusselNorvig2022AIMA">Russel &amp; Norvig (<a href="#/literature" role="doc-biblioref" onclick="">2022, p. 61</a>)</span></p>
</div>
</div>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="properties" class="title-slide slide level2">
<h2>Properties</h2>
<p>Task environments can be categorized along following dimensions <span class="citation" data-cites="RusselNorvig2022AIMA">(<a href="#/literature" role="doc-biblioref" onclick="">Russel &amp; Norvig, 2022, pp. 62–64</a>)</span>:</p>
<ul>
<li class="fragment">Fully observable ⇠⇢ partially observable</li>
<li class="fragment">Single agent ⇠⇢ multi-agent</li>
<li class="fragment">Deterministic ⇠⇢ nondeterministic</li>
<li class="fragment">Episodic ⇠⇢ sequential</li>
<li class="fragment">Static ⇠⇢ dynamic</li>
<li class="fragment">Discrete ⇠⇢ continuous</li>
<li class="fragment">Known ⇠⇢ unknown</li>
</ul>
<aside class="notes">
<p><strong>Explanations</strong></p>
<ul>
<li>If an agent’s sensors give it access to the full state of the environment at any point in time, then we say that the task environment is <em>fully observable</em> (e.g., image analysis).</li>
<li>When multiple agents intend to maximize a performance measure that depends on the behavior of other agents, we say the environment is <em>multi-agent</em> (e.g., chess).</li>
<li>When the environment is completely determined by the current state and the actions performed by the agent(s), it is called a <em>deterministic</em> environment (e.g., crossword puzzle). When a model of the environment explicitly uses probabilities, it is called a <em>stochastic</em> environment (e.g., poker).</li>
<li>If an agent’s experience is divided into atomic episodes in which the agent receives a perception and then performs a single action, and if the next episode does not depend on the actions performed in the previous episodes, then we say that the task environment is <em>episodic</em> (e.g., image analysis).</li>
<li>If the environment changes while an agent is deliberating, then the environment is <em>dynamic</em> (e.g., taxi driving).</li>
<li>If the environment has a finite number of different states, we speak of <em>discrete</em> environments (e.g., chess).</li>
<li>If the outcomes (or outcome probabilities) for all actions are given, then the environment is <em>known</em> (e.g., solitaire card game).</li>
</ul>
<p>Source: <span class="citation" data-cites="RusselNorvig2022AIMA">Russel &amp; Norvig (<a href="#/literature" role="doc-biblioref" onclick="">2022</a>)</span>, p.62-64</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside class="notes">
<p>The hardest case is <em>partially observable, multi-agent, nondeterministic, sequential, dynamic, and continuous.</em></p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="exercise" class="title-slide slide level2 discussion-slide">
<h2>Exercise</h2>
<div class="medium">
<p>Describe the task environment of a <span class="link-color">chess player</span> and an <span class="link-color">autonomous car</span>.</p>
</div>
<div id="timer_c2c4e0e48602591c578e9e15881e36516c7ceaba" class="countdown" data-warn-when="0" data-update-every="1" data-play-sound="false" data-blink-colon="false" data-start-immediately="false" tabindex="0" style="top: 0;right: 0;">
  <div class="countdown-controls">
    <button class="countdown-bump-down">−</button>
    <button class="countdown-bump-up">+</button>
 </div>
 <code class="countdown-time"><span class="countdown-digits minutes">15</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
 </div>
<aside class="notes">
<p><strong>Chess player:</strong></p>
<ul>
<li>static</li>
<li>discrete</li>
<li>fully-observable</li>
<li>deterministic</li>
<li>sequential</li>
<li>known</li>
</ul>
<p><strong>Autonomous car</strong></p>
<ul>
<li>dynamic</li>
<li>continuos</li>
<li>partial-observable</li>
<li>stochastic</li>
<li>sequential</li>
<li>known</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="autonomous-vs.-advisor-system" class="title-slide slide level2">
<h2>Autonomous vs.&nbsp;advisor system</h2>
<div class="r-stack">
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/systemTypes-1.svg" height="400"></p>
<figcaption>Types of intelligent systems in terms of their interaction with the environment <span class="citation" data-cites="Molina2020Intelligent">(<a href="#/literature" role="doc-biblioref" onclick="">Molina, 2020</a>)</span></figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/systemTypes.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section>
<section id="cognitive-abilities" class="title-slide slide level1 headline-only">
<h1>Cognitive abilities</h1>

</section>
<section id="definition" class="title-slide slide level2">
<h2>Definition</h2>
<p>Cognitive abilities—or <strong>“thinking skills”</strong>—are mental capacities that enable us to acquire knowledge, process information, and solve problems. They involve processing mental information through <span class="citation" data-cites="carroll1993human">(<a href="#/literature" role="doc-biblioref" onclick="">Carroll, 1993</a>)</span>:</p>
<div class="large">
<p><span class="fragment fade-in-then-semi-out">perception,</span> <span class="fragment fade-in-then-semi-out">attention,</span> <span class="fragment fade-in-then-semi-out">memory,</span> <span class="fragment fade-in-then-semi-out">reasoning,</span> <span class="fragment fade-in-then-semi-out">language,</span> <span class="fragment fade-in-then-semi-out">and executive functions</span></p>
</div>
</section>

<section id="implementation-in-ai-systems" class="title-slide slide level2">
<h2>Implementation in AI systems</h2>
<div class="r-stack">
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/cognition-1.svg" height="400"></p>
<figcaption>Primary cognitive abilities of intelligent systems</figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/cognition-2.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/cognition-3.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/cognition-4.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/cognition.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p><strong>Perception</strong> refers to AI systems’ ability to gather and interpret data from their environment through sensors, cameras, microphones, or data inputs</p>
<p><strong>Deliberation</strong> refers to the computational processes of analyzing information, drawing conclusions, and making decisions.</p>
<p><strong>Interaction</strong> refers to the communication capabilities with AI systems, including natural language processing and generation.</p>
<p><strong>Action control</strong> refers to the mechanisms that translate decisions into actions or outputs in the environment.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="discussion-1" class="title-slide slide level2 discussion-slide">
<h2>Discussion</h2>
<div class="medium">
<p>What are the basic cognitive abilities of a <span class="link-color">chatbot</span>? How they are technically implemented?</p>
</div>
<div id="timer_c2c4e0e48602591c578e9e15881e36516c7ceaba" class="countdown" data-warn-when="0" data-update-every="1" data-play-sound="false" data-blink-colon="false" data-start-immediately="false" tabindex="0" style="top: 0;right: 0;">
  <div class="countdown-controls">
    <button class="countdown-bump-down">−</button>
    <button class="countdown-bump-up">+</button>
 </div>
 <code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
 </div>
</section>

<section id="deliberation-and-reactive-behavior" class="title-slide slide level2">
<h2>Deliberation and reactive behavior</h2>
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/behavior.svg" height="400"></p>
<figcaption>Different types of behavior require different “thinking systems” based on <span class="citation" data-cites="Molina2020Intelligent">Molina (<a href="#/literature" role="doc-biblioref" onclick="">2020</a>)</span></figcaption>
</figure>
</div>
</section>

<section id="multiagent-systems" class="title-slide slide level2">
<h2>Multiagent systems</h2>
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/multi-agent.svg" height="400"></p>
<figcaption>Meta cognitive abilities by means of multiagent systems based on <span class="citation" data-cites="Molina2020Intelligent">Molina (<a href="#/literature" role="doc-biblioref" onclick="">2020</a>)</span></figcaption>
</figure>
</div>
<aside class="notes">
<p>Metacognition is “thinking about thinking” — the ability to reflect on, monitor, and regulate one’s own cognitive processes.</p>
<p>It involves two main components:</p>
<ul>
<li><strong>Metacognitive knowledge:</strong> Understanding and awareness of your own cognitive abilities, strategies, and limitations. This includes knowing what you know and don’t know, understanding how you learn best, and recognizing the demands of different tasks.</li>
<li><strong>Metacognitive regulation:</strong> The active control and monitoring of cognitive processes during learning or problem-solving. This includes planning how to approach a task, monitoring your comprehension or progress, evaluating your performance, and adjusting strategies when needed.</li>
</ul>
<p>In traditional (single-agent) systems, metacognition is about self-monitoring: “Do I know enough to solve this problem?” But in multiagent systems, metacognition becomes distributed and “social”.</p>
<p>This architecture supports metacognitive abilities because agents can reflect on their own knowledge, recognize the limitations of their partial view, communicate to fill knowledge gaps, and reason about what other agents might know or perceive. The distributed nature of the system requires agents to be aware of their own cognitive boundaries.</p>
<p><strong>Example scenario:</strong> Imagine robots exploring a building. Each sees only their room (partial environment). A metacognitive agent recognizes “I can’t see the exit from here” (knowledge limitation), reasons “Agent 3 near the entrance likely knows” (distributed knowledge model), and initiates communication (regulatory action).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="exercise-1" class="title-slide slide level3 discussion-slide" data-background-color="#000">
<h3>Exercise</h3>
<div class="medium">
<p>Find two real-world examples of multi-agent systems (one digital, one physical)</p>
</div>
<p>For each system, document:</p>
<ul>
<li>What are the individual agents?</li>
<li>How do they jointly perceive and deliberate?</li>
<li>How do agents interact/coordinate?</li>
</ul>
<div id="timer_c2c4e0e48602591c578e9e15881e36516c7ceaba" class="countdown" data-warn-when="0" data-update-every="1" data-play-sound="false" data-blink-colon="false" data-start-immediately="false" tabindex="0" style="top: 0;right: 0;">
  <div class="countdown-controls">
    <button class="countdown-bump-down">−</button>
    <button class="countdown-bump-up">+</button>
 </div>
 <code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
 </div>
</section>

</section>
<section>
<section id="complex-behavior" class="title-slide slide level1 headline-only">
<h1>Complex behavior</h1>

</section>
<section id="properties-1" class="title-slide slide level3">
<h3>Properties</h3>
<p>Complex behavior refers to the <strong>observable patterns of actions and responses that an AI system exhibits</strong> (<em>i.e., what the system does</em>), particularly sophisticated, adaptive, or emergent conduct that goes beyond simple stimulus-response patterns.</p>
<p><span class="fragment">To realize complex behavior, the components of an intelligent system (<em>i.e., what the system can do cognitively</em>) must have the following properties (to some extent):</span></p>
<div class="large">
<p><span class="fragment">Autonomy,</span> <span class="fragment">rationality,</span><br>
<span class="fragment">learning,</span> <span class="fragment">and introspection</span></p>
</div>
</section>

<section id="rationality" class="title-slide slide level3">
<h3>Rationality</h3>
<p>A rational agent is one that does the right thing—it is goal directed.</p>
<div class="fragment">
<blockquote>
<p>For each possible percept sequence, a rational agent should select an <strong>action</strong> that is expected to maximize its <strong>performance measure</strong>, given the evidence provided by the <strong>percept sequence</strong> and whatever built-in <strong>knowledge</strong> the agent has. <em><span class="citation" data-cites="RusselNorvig2022AIMA">Russel &amp; Norvig (<a href="#/literature" role="doc-biblioref" onclick="">2022, p. 58</a>)</span></em></p>
</blockquote>
</div>
<aside class="notes">
<p>What is rational at any given time depends on four things:</p>
<ul>
<li>The performance measure that defines the criterion of success</li>
<li>The agent’s prior knowledge of the environment</li>
<li>The actions that the agent can performance</li>
<li>The agent’s percept sequence to date</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="fragment">
<p>It can be quite hard to formulate a performance measure correctly, however:</p>
<blockquote>
<p>If we use, to achieve our purposes, a mechanical agency with those operation we cannot interfere once we have started it […] we had better be quite sure that the purpose built into the machine is the purpose which we really desire <em><span class="citation" data-cites="Wiener1960Some">Wiener (<a href="#/literature" role="doc-biblioref" onclick="">1960, p. 1358</a>)</span></em></p>
</blockquote>
</div>
</section>

<section id="discussion-2" class="title-slide slide level3 discussion-slide">
<h3>Discussion</h3>
<div class="large">
<p>Under which circumstances does a <span class="link-color">chatbot</span> act rational?</p>
</div>
<aside class="notes">
<p>Under following circumstances, the vacuum cleaning agent is rational:</p>
<ul>
<li>The performance measure of the vacuum cleaner might award one point for each clean square at each time step, over a “lifetime” of 1,000 time steps (to prevent the cleaner to oscillate needlessly back and forth).</li>
<li>The “geography” of the environment is known <em>a priori</em> but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking cleans the current square. The <em>Right</em> and <em>Left</em> actions move the agent one square except when this would take the agent outside the environment in which case the agent remains where it is.</li>
<li>The only available action is <em>Right</em>, <em>Left</em>, and <em>Suck</em>.</li>
<li>The agent correctly perceives its location and whether that location contains dirt.</li>
</ul>
<p>For details such as tabulated agent functions please see <span class="citation" data-cites="RusselNorvig2022AIMA">Russel &amp; Norvig (<a href="#/literature" role="doc-biblioref" onclick="">2022</a>)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="rationality-and-perfection" class="title-slide slide level3">
<h3>Rationality and perfection</h3>
<div class="large">
<p>Rationality != perfection</p>
</div>
<ul>
<li class="fragment">Rationality maximizes <em>expected</em> performance</li>
<li class="fragment">Perfection maximizes <em>actual</em> performance</li>
<li class="fragment">Perfection requires omniscience</li>
<li class="fragment">Rational choice depends only on the percept sequence <em>to date</em></li>
</ul>
<aside class="notes">
<p>As the environment is usually not completely known <em>a priori</em> and completely predictable (or stable), information gathering and learning are important parts of rationality <span class="citation" data-cites="RusselNorvig2022AIMA">(<a href="#/literature" role="doc-biblioref" onclick="">Russel &amp; Norvig, 2022, p. 59</a>)</span>.</p>
<p><strong>Example:</strong> The vacuum cleaner needs to explore an initially unknown environment (i.e., exploration) to maximize its expected performance. In addition, a vacuum cleaner that learns to predict where and when additional dirt will appear will do better than one that does not.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="rationality-and-cognitive-abilities" class="title-slide slide level3">
<h3>Rationality and cognitive abilities</h3>
<div class="r-stack">
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/rationalQuestions-1.svg" height="400"></p>
<figcaption>Rational decisions affect different cognitive abilities <span class="citation" data-cites="Molina2020Intelligent">(<a href="#/literature" role="doc-biblioref" onclick="">Molina, 2020</a>)</span></figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/rationalQuestions-2.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/rationalQuestions-3.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/rationalQuestions-4.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/rationalQuestions.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
</div>
</section>

<section id="learning" class="title-slide slide level3">
<h3>Learning</h3>
<blockquote>
<p>Learning agents are those that can improve their behavior through diligent study of past experiences and predictions of the future <em><span class="citation" data-cites="RusselNorvig2022AIMA">Russel &amp; Norvig (<a href="#/literature" role="doc-biblioref" onclick="">2022, p. 668</a>)</span></em></p>
</blockquote>
<div class="fragment">
<p>A learning agent</p>
<ul>
<li class="fragment">uses so-called <strong>machine learning</strong> (ML), if it is a computer;</li>
<li class="fragment">improves performance based on experience (i.e., observations of the world);</li>
<li class="fragment">is required when the designer lacks omniscience (i.e., in unknown environments) and/or</li>
<li class="fragment">has no idea how to program a solution themselves (e.g., recognizing faces)</li>
</ul>
</div>
</section>

<section id="learning-types" class="title-slide slide level3">
<h3>Learning types</h3>
<p><span class="large">Supervised learning</span><br>
<span class="link-color fragment">Involves learning a function from examples <em>➞ test and training data</em></span></p>
<p><span class="large">Unsupervised learning</span><br>
<span class="link-color fragment">The agent has to learn patterns in the input <em>➞ identification of categories or classifications</em></span></p>
<p><span class="large">Reinforcement learning</span><br>
<span class="link-color fragment">The agent must learn from punishments or rewards <em>➞ learning by trial and error</em></span></p>
</section>

<section id="learning-and-cognitive-abilities" class="title-slide slide level3">
<h3>Learning and cognitive abilities</h3>
<div class="r-stack">
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/learning-1.svg" height="400"></p>
<figcaption>Adaptation through learning can affect differnt cognitive abilities <span class="citation" data-cites="Molina2020Intelligent">(<a href="#/literature" role="doc-biblioref" onclick="">Molina, 2020</a>)</span></figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/learning-2.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/learning-3.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/learning-4.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/learning.svg" height="400"></p>
<figcaption>&nbsp;</figcaption>
</figure>
</div>
</div>
</div>
</section>

<section id="introspection" class="title-slide slide level3">
<h3>Introspection</h3>
<div class="medium">
<p>Introspection refers to the capabilitiy to <strong>analyze one’s cognitive abilities</strong>.</p>
</div>
<p>The system uses an <span class="link-color">observable model of its own abilities</span>.&nbsp;<br>
<span class="fragment">This model is used to simulate self-awareness processes.</span></p>
<div class="fragment">
<p>Introspection allows the system …</p>
<ul>
<li class="fragment">… to judge its own actions and, thus, provides <span class="link-color">learning opportunities</span> <br> (e.g., analyzing past outputs ot identify errors or biases) and</li>
<li class="fragment">… to <span class="link-color">generate explanations</span> and, thus, to justify decisions to the user <br> (e.g., explainable AI — showing how a systems arrives at a solution)</li>
</ul>
</div>
</section>

<section id="summary" class="title-slide slide level2" data-background-color="#f0f0f0">
<h2>Summary</h2>
<p>The properties of an intelligent system are</p>
<p><span class="medium fragment">Capacity to work in a <br> complex environment</span><br>
<span class="fragment fade-in-then-semi-out">Interaction with the environment and other agents</span></p>
<p><span class="medium fragment">Cognitive abilities</span><br>
<span class="fragment fade-in-then-semi-out">Perception, action control, deliberation, and interaction</span></p>
<p><span class="medium fragment">Complex behavior</span><br>
<span class="fragment fade-in-then-semi-out">Acting autonomously, rationally, adaptation through learning, and introspection</span></p>
</section>
<section id="homework" class="title-slide slide level3 html-hidden unlisted discussion-slide" data-background-color="black">
<h3>Homework</h3>
<div class="medium">
<p>Select an intelligent system and analyse it using the properties outlined here.</p>
</div>
<p>Note: This is an excellent task to prepare for the exam.</p>
<div class="medium">
<p>Read <span class="citation" data-cites="dellermann2019hybrid">Dellermann et al. (<a href="#/literature" role="doc-biblioref" onclick="">2019</a>)</span>.</p>
</div>
</section>

</section>
<section>
<section id="affections" class="title-slide slide level1 headline-only">
<h1>Affections</h1>

</section>
<section id="affective-computing" class="title-slide slide level2" data-background-color="#f0f0f0">
<h2>Affective computing</h2>
<div class="large">
<p>Computing that relates to, arises from or deliberately influences emotion.</p>
</div>

<aside><div>
<p><span class="citation" data-cites="picard2000affective">Picard (<a href="#/literature" role="doc-biblioref" onclick="">2000</a>)</span></p>
</div></aside></section>

<section id="objectives" class="title-slide slide level2">
<h2>Objectives</h2>
<div class="medium">
<p>Assigning systems “the human-like capabilities of observation, interpretation and generation of affect features<a href="#/footnotes" class="footnote-ref" id="fnref4" role="doc-noteref" data-footnote-href="#/fn4" onclick=""><sup>4</sup></a>” <span class="smaller"><span class="citation" data-cites="tao2005affective">(<a href="#/literature" role="doc-biblioref" onclick="">Tao &amp; Tan, 2005, p. 981</a>)</span></span></p>
</div>
<div class="fragment">
<p>The goal is to simulate empathy — that affective systems can interpret the emotional states of humans and adapt their behavior to them, giving an appropriate response for those emotions (i.e., <em>emotion aware systems</em>).</p>
</div>
<aside></aside></section>

<section id="properties-2" class="title-slide slide level2">
<h2>Properties</h2>
<p><span class="large fragment">Emotion recognition</span><br>
<span class="fragment fade-in-then-semi-out">Interpreting the emotional states of humans</span></p>
<p><span class="large fragment">Emotion expresssion</span><br>
<span class="fragment fade-in-then-semi-out">Ability to simulate human affects (e.g.&nbsp;‘emotional modality’)</span></p>
<p><span class="large fragment">Adequate response to emotion</span><br>
<span class="fragment fade-in-then-semi-out">Linking emotion recognition and expression e.g., to reinforce the meaning of messages</span></p>
</section>

<section id="emotional-signals" class="title-slide slide level2">
<h2>Emotional signals</h2>
<div class="medium">
<p><span class="highlight-current-blue fragment">Facial expression</span>, <span class="highlight-current-blue fragment">posture</span>, <span class="highlight-current-blue fragment">speech</span>, <span class="highlight-current-blue fragment">force or rhythm of key stroke</span>, <span class="highlight-current-blue fragment">temperature change</span> (e.g., hand on mouse) can signify changes in user’s emotional state.</p>
</div>
<div class="fragment">
<p>These can be detected and interpreted by an affective system.</p>
</div>
<div class="fragment">
<p>Affective systems can use some of these to simulate emoptions.</p>
</div>
</section>

<section id="basic-emotions" class="title-slide slide level2">
<h2>Basic emotions</h2>
<p><span class="citation" data-cites="ekman1987universals">Ekman et al. (<a href="#/literature" role="doc-biblioref" onclick="">1987</a>)</span> categorized emotions into 6 groups:</p>
<div class="large">
<p><span class="fragment fade-in-then-semi-out">Fear,</span> <span class="fragment fade-in-then-semi-out">surprise,</span> <span class="fragment fade-in-then-semi-out">disgust,</span> <span class="fragment fade-in-then-semi-out">anger,</span> <span class="fragment fade-in-then-semi-out">happiness,</span> <span class="fragment fade-in-then-semi-out">and sadness</span></p>
</div>
<div class="fragment">
<p>All of these can facially expressed.</p>
</div>
</section>

<section id="examples" class="title-slide slide level2">
<h2>Examples</h2>
<div class="incremantal">
<ul>
<li><strong>Facial expression analysis</strong><br>
Using computer vision and machine learning to analyze facial expressions and determine the emotional state of a person.</li>
<li><strong>Voice analysis</strong><br>
Analyzing the tone, pitch, and other characteristics of a person’s voice to determine their emotional state.</li>
<li><strong>Physiological sensing</strong><br>
Using wearable devices to monitor physiological signals such as heart rate, skin conductance, and body temperature to detect emotional responses.</li>
<li><strong>Emotion simulation</strong><br>
Developing systems that can generate emotional responses, such as a virtual assistant that can express empathy or a chatbot that can adapt its tone based on the user’s emotional state.</li>
</ul>
</div>
</section>

<section id="exercise-2" class="title-slide slide level2 html-hidden unlisted discussion-slide" data-background-color="black">
<h2>Exercise</h2>
<div class="large">
<p>Think of/identify a real-life use case for affective computing.</p>
</div>
<p>Relate it to the basic properties of affective systems. Argue why affective computing is effective in this use case.</p>
<div id="timer_c2c4e0e48602591c578e9e15881e36516c7ceaba" class="countdown" data-warn-when="0" data-update-every="1" data-play-sound="false" data-blink-colon="false" data-start-immediately="false" tabindex="0" style="top: 0;right: 0;">
  <div class="countdown-controls">
    <button class="countdown-bump-down">−</button>
    <button class="countdown-bump-up">+</button>
 </div>
 <code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
 </div>
</section>
</section>
<section>
<section id="hybrid-intelligence" class="title-slide slide level1 headline-only">
<h1>Hybrid intelligence</h1>

</section>
<section id="exercise-3" class="title-slide slide level2">
<h2>Exercise</h2>
<p>Synthesize your findings from reading <span class="citation" data-cites="dellermann2019hybrid">Dellermann et al. (<a href="#/literature" role="doc-biblioref" onclick="">2019</a>)</span> by findings answers to following questions:</p>
<ol type="1">
<li>How can hybrid intelligence be defined?</li>
<li>What are main characteristics of hybrid intelligence?</li>
<li>What are complementary strengths of humans and machines?</li>
<li>What implications does that concept have for practice?</li>
</ol>
<div id="timer_c2c4e0e48602591c578e9e15881e36516c7ceaba" class="countdown" data-warn-when="0" data-update-every="1" data-play-sound="false" data-blink-colon="false" data-start-immediately="false" tabindex="0" style="top: 0;right: 0;">
  <div class="countdown-controls">
    <button class="countdown-bump-down">−</button>
    <button class="countdown-bump-up">+</button>
 </div>
 <code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
 </div>
</section>

<section id="concept" class="title-slide slide level2">
<h2>Concept</h2>
<div class="large">
<p>The idea is to combine the complementary capabilities of humans and computers to augment each other.</p>
</div>

<aside><div>
<p><span class="citation" data-cites="dellermann2019hybrid">Dellermann et al. (<a href="#/literature" role="doc-biblioref" onclick="">2019</a>)</span></p>
</div></aside></section>

<section id="complementary-strengths" class="title-slide slide level2">
<h2>Complementary strengths</h2>
<div class="columns">
<div class="column">
<p><span class="medium link-color">Human intelligence</span></p>
<p><span class="fragment" data-fragment-index="1">Flexibility &amp; transfer</span></p>
<p><span class="fragment" data-fragment-index="2">Empathy &amp; creativity</span></p>
<p><span class="fragment" data-fragment-index="3">Eventualities</span></p>
<p><span class="fragment" data-fragment-index="4">Common sense</span></p>
<p><span class="fragment" data-fragment-index="5"><strong>Intuition</strong></span></p>
</div><div class="column">
<p><span class="medium link-color">Artificial intelligence</span></p>
<p><span class="fragment" data-fragment-index="1">Pattern recognition</span></p>
<p><span class="fragment" data-fragment-index="2">Probabilistic</span></p>
<p><span class="fragment" data-fragment-index="3">Consistency</span></p>
<p><span class="fragment" data-fragment-index="4">Speed &amp; efficiency</span></p>
<p><span class="fragment" data-fragment-index="5"><strong>Analysis</strong></span></p>
</div></div>
<aside class="notes">
<p>Humans are flexible, creative, empathic, and can adapt to various settings. This allows, for instance, human domain experts to deal with so called ‘‘broken-leg’’ predictions that deviate from the currently known probability distribution. However, they are restricted by bound rationality that prevents them from aggregating information perfectly and drawing conclusions from that. On the other hand, machines are particularly good at solving repetitive tasks that require fast processing of huge amounts of data, at recognizing complex patterns, or weighing multiple factors following consistent rules of probability theory <span class="citation" data-cites="dellermann2019hybrid">(<a href="#/literature" role="doc-biblioref" onclick="">Dellermann et al., 2019</a>)</span>.</p>
<p>However, there is also a technology-centric perspective that assumes that true intelligence can ultimately only be found in well-developed and mature (general) AI systems. Humans are biologically limited in their information processing and reasoning abilities and exhibit many types of cognitive biases, while computers offer virtually infinite possibilities to develop rational intelligence at human levels and beyond <span class="citation" data-cites="peeters2021hybrid">(<a href="#/literature" role="doc-biblioref" onclick="">Peeters et al., 2021</a>)</span>.</p>
<p><strong>Model of human cognition</strong></p>
<p><span class="citation" data-cites="kahneman2011thinking">Kahneman (<a href="#/literature" role="doc-biblioref" onclick="">2011</a>)</span> proposed a two-system model of human cognition, which he called System 1 and System 2.</p>
<p>System 1 is an intuitive, automatic, and fast mode of thinking that operates outside of our conscious awareness. It is responsible for generating impressions, making quick judgments, and executing routine tasks with minimal effort.</p>
<p>System 2, on the other hand, is a more analytical, controlled, and deliberate mode of thinking that requires conscious effort and attention. It is responsible for problem-solving, critical thinking, and decision-making.</p>
<p>While System 1 operates quickly and automatically, it can be prone to biases and errors, particularly in complex or unfamiliar situations. System 2, though slower and more effortful, can help us avoid these biases and make more accurate decisions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>

<section id="definition-1" class="title-slide slide level2">
<h2>Definition</h2>
<blockquote>
<p><strong>Hybrid intelligence</strong> is defined as the ability to achieve complex goals by combining human and artificial intelligence, thereby reaching superior results to those each of them could have accomplished separately, and continuously improve by learning from each other. <em><span class="citation" data-cites="dellermann2019hybrid">Dellermann et al. (<a href="#/literature" role="doc-biblioref" onclick="">2019, p. 640</a>)</span></em></p>
</blockquote>
<p>Main characteristics of hybrid intelligence are:</p>
<ul>
<li class="fragment"><strong>Collectively</strong><br>
Tasks are performed collectively and activities are conditionally dependent</li>
<li class="fragment"><strong>Superior results</strong><br>
Neither AI nor humans could have achieved the outcome without the other</li>
<li class="fragment"><strong>Continuous learning</strong><br>
All components of the socio-technical system learn from each other through experience</li>
</ul>
</section>

<section id="visualization" class="title-slide slide level2">
<h2>Visualization</h2>
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/hybridIntelligence.svg" class="img-fluid"></p>
<figcaption>Distribution of roles in hybrid intelligence <span class="citation" data-cites="dellermann2019hybrid">(<a href="#/literature" role="doc-biblioref" onclick="">Dellermann et al., 2019, p. 640</a>)</span></figcaption>
</figure>
</div>
</section>

<section id="implications" class="title-slide slide level2">
<h2>Implications</h2>
<p>According to <span class="citation" data-cites="peeters2021hybrid">Peeters et al. (<a href="#/literature" role="doc-biblioref" onclick="">2021</a>)</span> following conclusions can be drawn:</p>
<ul>
<li class="fragment">Intelligence should not be studied at the level of individual humans or AI-machines, but <strong>at the group level</strong> of humans and AI-machines working together.</li>
<li class="fragment">Increasing the intelligence of a system should be achieved by <strong>increasing the quality of the interaction between its constituents</strong> rather than the intelligence of the constituents themselves.</li>
<li class="fragment">Both human as well as artificial intelligence can be regarded as very shallow when considered in isolation.</li>
<li class="fragment">No AI is an island (it’s about the system of systems).</li>
</ul>
</section>

<section id="examples-1" class="title-slide slide level2">
<h2>Examples</h2>
<dl>
<dt><a href="https://www.robotsindeklas.nl/">Robots in de klas</a></dt>
<dd>
A team consisting of a remedial teacher, an educational therapist, and a Nao robot collaborate to support a child with learning difficulties. The robot provides expertise and advice while also helping the child stay focused and engaged.
</dd>
</dl>
<div class="fragment">
<dl>
<dt><a href="https://www.thefader.com/2019/05/21/holly-herndon-proto-ai-spawn-interview">Spawn</a></dt>
<dd>
The musician Holly Herndon created “Spawn,” an AI system that generates unique music different from her usual style. By using Spawn as a tool, Holly is able to avoid creating music that repeats her previous works but to to expand the possibilities of their music.
</dd>
</dl>
</div>
<div class="fragment">
<dl>
<dt><a href="https://github.com/features/copilot">GitHub Copilot</a></dt>
<dd>
In collaborative coding, Copilot can engage in back-and-forth dialogue about software design decisions, propose implementations, and explain reasoning about technical approaches - moving beyond simply generating code.
</dd>
</dl>
<p><strong>What examples do come to your mind?</strong></p>
</div>
</section>
</section>
<section>
<section id="qa" class="title-slide slide level1 html-hidden unlisted headline-only vertical-center" data-background-color="#0333ff" data-background-image="../assets/bg.jpg">
<h1>Q&amp;A</h1>

</section>
<section id="homework-1" class="title-slide slide level2">
<h2>Homework</h2>
<p>Listen to the Decoder podcast episode with <a href="https://www.theverge.com/24158374/google-ceo-sundar-pichai-ai-search-gemini-future-of-the-internet-web-openai-decoder-interview">Google CEO Sundar Pichai on AI-powered search and the future of the web</a> and reflect on the problems with the Internet and responses discussed.</p>
</section>
</section>
<section id="literature" class="title-slide slide level1 smaller scrollable">
<h1>Literature</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-adami2016introducing" class="csl-entry" role="listitem">
Adami, E. (2016). Introducing multimodality. <em>The Oxford Handbook of Language and Society</em>, 451–472.
</div>
<div id="ref-carroll1993human" class="csl-entry" role="listitem">
Carroll, J. B. (1993). <em>Human cognitive abilities: A survey of factor-analytic studies</em>. Cambridge university press.
</div>
<div id="ref-dellermann2019hybrid" class="csl-entry" role="listitem">
Dellermann, D., Ebel, P., Söllner, M., &amp; Leimeister, J. M. (2019). Hybrid intelligence. <em>Business &amp; Information Systems Engineering</em>, <em>61</em>, 637–643.
</div>
<div id="ref-ekman1987universals" class="csl-entry" role="listitem">
Ekman, P., Friesen, W. V., O’sullivan, M., Chan, A., Diacoyanni-Tarlatzis, I., Heider, K., Krause, R., LeCompte, W. A., Pitcairn, T., Ricci-Bitti, P. E., et al. (1987). Universals and cultural differences in the judgments of facial expressions of emotion. <em>Journal of Personality and Social Psychology</em>, <em>53</em>(4), 712.
</div>
<div id="ref-euAIAct2024" class="csl-entry" role="listitem">
European Commission. (2024). <em>Artificial intelligence in the european commission — a strategic vision to foster the development and use of lawful, safe and trustworthy artificial intelligence systems in the european commission</em>. <em>C(2024) 380</em>.
</div>
<div id="ref-kahneman2011thinking" class="csl-entry" role="listitem">
Kahneman, D. (2011). <em>Thinking, fast and slow</em>. macmillan.
</div>
<div id="ref-lee2013presence" class="csl-entry" role="listitem">
Lee, H.-G., Chung, S., &amp; Lee, W.-H. (2013). Presence in virtual golf simulators: The effects of presence on perceived enjoyment, perceived value, and behavioral intention. <em>New Media &amp; Society</em>, <em>15</em>(6), 930–946.
</div>
<div id="ref-Molina2020Intelligent" class="csl-entry" role="listitem">
Molina, M. (2020). Intelligent systems. <em>Master Course (Lecture Slides)</em>.
</div>
<div id="ref-peeters2021hybrid" class="csl-entry" role="listitem">
Peeters, M. M., Diggelen, J. van, Van Den Bosch, K., Bronkhorst, A., Neerincx, M. A., Schraagen, J. M., &amp; Raaijmakers, S. (2021). Hybrid collective intelligence in a human–AI society. <em>AI &amp; Society</em>, <em>36</em>, 217–238.
</div>
<div id="ref-picard2000affective" class="csl-entry" role="listitem">
Picard, R. W. (2000). <em>Affective computing</em>. MIT press.
</div>
<div id="ref-RusselNorvig2022AIMA" class="csl-entry" role="listitem">
Russel, S., &amp; Norvig, P. (2022). <em>Artificial intelligence: A modern approach</em>. Pearson Education.
</div>
<div id="ref-sternberg1985beyond" class="csl-entry" role="listitem">
Sternberg, R. J. et al. (1985). <em>Beyond IQ: A triarchic theory of human intelligence</em>. CUP Archive.
</div>
<div id="ref-suh2018state" class="csl-entry" role="listitem">
Suh, A., &amp; Prophet, J. (2018). The state of immersive technology research: A literature analysis. <em>Computers in Human Behavior</em>, <em>86</em>, 77–90.
</div>
<div id="ref-tao2005affective" class="csl-entry" role="listitem">
Tao, J., &amp; Tan, T. (2005). Affective computing: A review. <em>International Conference on Affective Computing and Intelligent Interaction</em>, 981–995.
</div>
<div id="ref-Wiener1960Some" class="csl-entry" role="listitem">
Wiener, N. (1960). Some moral and technical consequences of automation. <em>Science</em>, <em>131</em>(3410), 1355–1358.
</div>
</div>


</section>

<section id="footnotes" class="footnotes footnotes-end-of-document smaller scrollable" role="doc-endnotes"><h5>Footnotes</h5>

<ol>
<li id="fn1"><p>The capacity to work in a complex environment is described as agency</p></li>
<li id="fn2"><p>Cognitive abilities are, for instance, perception and language</p></li>
<li id="fn3"><p>Behavior refers the observable patterns of actions and responses that an AI system exhibits. Autonomy, adaptiveness, goal-directedness, emergence, and context-sensitivity makes it complex</p></li>
<li id="fn4"><p>”Affect” is basically a synonym for emotion.</p></li>
</ol>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: false,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1080,

        height: 640,

        // Factor of the display size that should remain empty around the content
        margin: 0,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script>
    // Copy over background colors to new div
    divs = document.querySelectorAll('[data-background-color]');

    Array.from(divs).map(function (x) {
      const color = x.dataset.backgroundColor;

      var new_div = document.createElement('div');
      new_div.setAttribute("class", "background-color-div");
      new_div.style.backgroundColor = color;
      x.appendChild(new_div);
      x.removeAttribute("data-background-color");
    })

    // Remove background colors from backgrounds div
    Array.from(
      document.querySelectorAll("[data-background-hash]")
    ).map(function (x) {
      x.removeAttribute("data-background-hash");
      x.style.backgroundColor = null;
    })
    </script>

    <script>
    // Copy over background images to new div
    divs = document.querySelectorAll('[data-background-image]');

    Array.from(divs).map(function (x) {
      var new_div = document.createElement('div');
      new_div.setAttribute("class", "background-image-div");

      if (x.dataset.backgroundImage != undefined) {
        new_div.style.backgroundImage = "url(" + x.dataset.backgroundImage + ")";
        x.removeAttribute("data-background-image");
      }
      if (x.dataset.backgroundSize != undefined) {
        new_div.style.backgroundSize = x.dataset.backgroundSize;
        x.removeAttribute("data-background-size");
      }
      if (x.dataset.backgroundPosition != undefined) {
        new_div.style.backgroundPosition = x.dataset.backgroundPosition;
        x.removeAttribute("data-background-position");
      }
      if (x.dataset.backgroundRepeat != undefined) {
        new_div.style.backgroundRepeat = x.dataset.backgroundRepeat;
        x.removeAttribute("data-background-repeat");
      }
      if (x.dataset.backgroundOpacity != undefined) {
        new_div.style.backgroundOpacity = x.dataset.backgroundOpacity;
        x.removeAttribute("data-background-opacity");
      }

      x.appendChild(new_div);
    })
    </script>

    <script>
    // Copy over background vidoes

    // Run once when we load
    var video_div = document.querySelectorAll(".backgrounds video");
    var slide_div = document.querySelectorAll("[data-background-video]");

    for (let i = 0; i < video_div.length; i++) {
      video_div[i].setAttribute("class", "background-video-div");

      slide_div[i].appendChild(video_div[i]);
      slide_div[i].removeAttribute("data-background-video");
    }

    Reveal.getCurrentSlide().querySelectorAll(".background-video-div").forEach(x => x.play());

    // Each time we advance slides, as the background videos aren't loaded
    // unless they are a few slides away
    Reveal.on('slidechanged', event => {
      var video_div = document.querySelectorAll(".backgrounds video");
      var slide_div = document.querySelectorAll("[data-background-video]");

      for (let i = 0; i < video_div.length; i++) {
        video_div[i].setAttribute("class", "background-video-div");

        slide_div[i].appendChild(video_div[i]);
        slide_div[i].removeAttribute("data-background-video");
      }

      Reveal.getCurrentSlide().querySelectorAll(".background-video-div").forEach(x => x.play());
    });
    </script>

    <script>
    // Copy over background vidoes

    // Run once when we load
    var iframe_div = document.querySelectorAll(".backgrounds iframe");
    var slide_div = document.querySelectorAll("[data-background-iframe]");

    for (let i = 0; i < iframe_div.length; i++) {
      iframe_div[i].setAttribute("class", "background-iframe-div");

      slide_div[i].appendChild(iframe_div[i]);
      slide_div[i].removeAttribute("data-background-iframe");
    }

    // Each time we advance slides, as the background videos aren't loaded
    // unless they are a few slides away
    Reveal.on('slidechanged', event => {
      var iframe_div = document.querySelectorAll(".backgrounds iframe");
      var slide_div = document.querySelectorAll("[data-background-iframe]");

      for (let i = 0; i < iframe_div.length; i++) {
        iframe_div[i].setAttribute("class", "background-iframe-div");

        slide_div[i].appendChild(iframe_div[i]);
        slide_div[i].removeAttribute("data-background-iframe");
      }
    });
    </script>

    <script>
    // Clean up slide background styles
    divs = document.querySelectorAll('.slide-background-content');
    Array.from(divs).map(function (x) {
      x.style = null;
    })
    </script>

    <script>

      // Move menu button
      menu_div = document.querySelector(".slide-menu-button");
      document.querySelector(".slides").appendChild(menu_div);
      
      // Move progress bar
      menu_div = document.querySelector(".progress");
      document.querySelector(".slides").appendChild(menu_div);
      
      // Move slide number
      slide_number = document.querySelector(".slide-number");
      document.querySelector(".slides").appendChild(slide_number);
      
      // Move custom footer
      footer = document.querySelector(".footer.custom");
      document.querySelector(".slides").appendChild(footer);

    </script>

    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/awe-hnu\.github\.io");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>