---
title: "Mental Models"  
subtitle: "The latticework of mental models — a framework for digital leadership"
lang: en

bibliography: ../assets/literature.bib

date: "03.05.2025"

title-slide-attributes:
  data-background-image: ../assets/bg.jpg
  data-background-size: cover
  data-background-opacity: "1"
  data-background-color: '#0333ff'

format: 
  html:
    output-file: index.html
    margin-header: | 
      [Slides](slides.html){.btn .btn-primary target="blank"}
    format-links: false   
  presentation-revealjs:
    output-file: slides.html
    include-before-body: ../assets/footer.html
    
---


# Introduction {.headline-only}

## Opening quote {.no-headline .vertical-center}

:::large
> In life and business, the person with the fewest blind spots wins. *Shane Parrish*
:::

## The decision-making crisis

:::html-hidden
:::medium
[Leadership is challenged with]{.fragment fragment-index=0}
[[information overload]{.fragment .highlight-current-blue fragment-index=1} (2.5 quintillion bytes created daily), ]{.fragment fragment-index=1}
[[increasing complexity of systems]{.fragment .highlight-current-blue fragment-index=2}, ]{.fragment fragment-index=2} 
[[and an  VUCA environment]{.fragment .highlight-current-blue fragment-index=3} (volatile, uncertain, complex, ambigious).]{.fragment fragment-index=3}
:::
:::

:::notes
In todays world we need to deal with an unprecedented scale of information. 2.5 quintillion bytes daily means our intuitive approaches to processing information are inadequate. Digital leaders need to accompany with VUCA as the "new normal", which is not a temporary state but the permanent condition of modern leadership.

According to @parrish2020great information overload, system complexity, and VUCA environments cause significant decision-making challenges in several ways:

- When faced with overwhelming information and complexity, we tend to filter based on our existing biases, potentially missing critical variables that would lead to better decisions (i.e., multiplied blind spots).
- When information exceeds our processing capacity, we tend to rely on simplistic heuristics or familiar models even when they don't fit the situation rather than the thoughtful application of diverse mental models.
- The parable of the blind men and the elephant becomes even more relevant as in complex systems, each specialist sees only their part of the problem, but no one sees the whole picture.
- In complex systems, the consequences of decisions may be distant in time and space, making it harder to "keep our feet on the ground" and learn from experience (i.e, feedback delay).
- VUCA environments make it particularly difficult to anticipate the "second-, third-, and higher-order consequences of various choices", which are crucial for decision-making.
:::

## Paradox {.html-hidden}

:::large
> We're trained to become specialists, yet specialization itself creates blind spots. *@parrish2020great*
:::

. . .

Technical expertise alone is insufficient for effective digital leadership.

. . .

This is why we need mental models from multiple disciplines \
— they help us see beyond our specialization.


:::notes
"How many IT implementations have failed not because of technical issues but because of human factors, business process misalignments, or organizational dynamics?"
:::


## The problem with specialization

![Illustration of the blind men in the classic parable [@parrish2020great p.21]](images/elephant.jpg){#fig-elephant fig-align="left"}

:::notes
Just as the blind men each touch a different part of the elephant—perceiving it as a wall, a rope, a fan, a tree, or a spear based on their limited contact—we tend to interpret complex problems solely through our specialized expertise. Whether economists, engineers, physicists, mathematicians, biologists, or chemists, we grasp partial truths about reality while missing the complete picture that would be visible if we combined our perspectives [@parrish2020great, p. 21].

- Domain expertise creates tunnel vision
- Overuse of familiar tools ("to a hammer, everything looks like a nail")
- Disciplinary silos limit innovation

Technical expertise alone is insufficient for effective digital leadership. This is why we need mental models from multiple disciplines - they help us see beyond our specialization.

:::callout-note
#### Reading recommendation

@tetlock2015superforecasting emphasize that better decision-making comes not from specialized knowledge alone but from cultivating systematic thought processes that more accurately represent reality. For instance, their expert political judgment studies show specialists make worse predictions than generalists.

This further highlights how developing better mental models isn't just philosophical—it produces measurably superior outcomes when facing uncertainty.
:::
:::

# Latticework thinking {.headline-only}

## Mental models

@johnson1983mental shows that humans don't naturally think using the rules of formal logic (like syllogisms or propositional calculus). Instead, we construct simplified mental representations or "models" of situations and mentally simulate what might happen within those scenarios.

. . .

:::medium
A mental model is a cognitive representation such as a conceptual framework or worldview that helps us understand and interpret the world [ [@jones2011mental].]{.smaller}
:::

:::notes

They function as

- filter for relevant information,
- framework for interpretation (e.g., cause-and-effect dynamics), and
- guide for decision-making.

Mental models can change over time trough learning.

:::html-hidden
Simple example: When you drive a car, you have a mental model of how turning the steering wheel affects direction. You don't think about it consciously - it's internalized.

--> Mental models aren't just conscious frameworks - they operate at both conscious and unconscious levels, shaping what we notice and how we interpret it"
--> The quality of our decisions depends on the quality and diversity of our mental models"

:::
:::

## Discussion {.html-hidden .discussion-slide}

:::medium
Have you ever seen something fail because people were looking at problems in a narrow way?
:::

:::notes
Complex systems like digital systems, like elephants, can't be understood through any single perspective - they require multiple viewpoints
:::

## The latticework approach

Worldly wisdom requires models from all important disciplines [@parrish2020great].

::::columns

:::{.column width="66%"}

:::medium
> You've got to have models in your head. And you've got to array your experience—both vicarious and direct—on this latticework of models. *American businessman, investor, attorney, and philanthropist;  Warren Buffett's partner at Berkshire Hathaway (1924-2023)*
:::

:::

:::{.column width="33%"}
![Mental model latticework<br /> [@parrish2020great p.22]](images/latticework.jpg){fig-align="left" width="50%"}
:::

::::

:::notes
When findings from different disciplines support the same conclusion, it strengthens the validity of that conclusion. And combining insights across fields can reveal patterns and principles invisible within any single discipline (remember the tale of the elephant).

Example: Netflix uses **game theory** principles to structure their recommendation algorithms, these create **incentive structures** for continued engagement, these incentives are implemented through specific **UX design** patterns, these designs leverage **behavioral economics** principles like hyperbolic discounting (valuing immediate rewards over future ones)
:::

# Mental models x leaders {.headline-only}

## Introduction

Mental models provide the understanding of how a system works and allow us to use heuristics to quickly navigate within that system.

. . .

:::medium
Mental models are the key to making heuristics fast, frugal and accurate strategies. Such [simple mental shortcuts]{.link-color} in turn, enable rather then restrict decision-making under uncertainty [[@gigerenzer2022smart].]{.smaller}
:::


## Generic mental models

Generic mental models are models that are broadly applicable across multiple domains, disciplines, and situations, rather than being specific to a single field or context.

:::notes
First principles
: "Elon Musk attributes Tesla and SpaceX's innovations to breaking problems down to fundamental truths rather than reasoning by analogy - this is how they reimagined electric vehicles and rocket economics.

Second-order thinking
: Most people stop at first-order consequences, but digital leaders need to ask 'And then what?' repeatedly.

Probabilistic reasoning
: In environments of uncertainty, thinking in probabilities rather than certainties helps avoid both overconfidence and paralysis.

Inversion
: Instead of asking 'How can we make this project succeed?' also ask 'What would guarantee this project's failure?' - then avoid those conditions.

Occam's razor
: When presented with competing explanations or solutions, start with the simplest one with fewest assumptions.
:::

. . .

:::html-hidden
:::medium
According to @parrish2020great some effective generic mental models are [first principles thinking, ]{.fragment} [second-order thinking, ]{.fragment} [probabilistic reasoning, ]{.fragment} [inversion, ]{.fragment} [and Occam's razor.]{.fragment}
:::
:::


## Systems thinking models

Systems thinking models are mental frameworks that help us understand complex systems by focusing on relationships, interactions, and emergent properties rather than isolated components.

. . .

:::html-hidden
:::medium
Important system thinking mental models are [complex adaptive systems, ]{.framgent} [feedback loops, ]{.fragment} [emergence, ]{.fragment} [and network effects.]{.fragment}
:::
:::

:::notes
Complex adaptive systems
: Digital ecosystems behave like biological ones — they organize themself without central control or explicit coordination and, thus, can only influenced but not controlled through well-chosen interventions.

Feedback loops
: Systems are usually coined by reinforcing loops (where A increases B which further increases A) and balancing loops (where changes trigger countervailing forces) - understanding these dynamics helps predict how systems will evolve

Emergence
: "The most important properties of (digital) systems often aren't designed but emerge from interactions — like how simple feautures of systems like social networks created entirely new social behaviors (e.g., the hashtag effect).

Network effects
: Understanding Metcalfe's Law —that the value of a network grows with the square of the number of users— explains why platforms like LinkedIn or Slack become increasingly valuable and difficult to displace.

:::

## Human behavior models

Human behavior models are conceptual frameworks that help explain, predict, and influence how people think, decide, and act. These models are particularly valuable for digital leaders who need to understand both individual psychology and group dynamics when designing systems, implementing change, or leading organizations.

. . .

:::html-hidden
:::medium
Human behavior models include [incentives, ]{.framgent} [cognitive bias, ]{.fragment} [social and group behavior models, ]{.fragment} [and learning curves.]{.fragment}
:::
:::

:::notes
Incentives
: These models explain how rewards and punishments shape behavior such as the principal-agent problem, intrinsic vs. exdtrinsic motivation and hyperbolic discounting (i.e., the tendendy to overvalue immediate rewards compared to future ones).

Congnitive bias
: These models explain systematic errors in thinking and decision-making such as confirmation bias, framing effects, and availabiltiy heuristic (i.e., judging probability based on how easily examples come to mind).

Social and group behavior models
: These models explain how people interact and organize such as status games (competition for position and recognition), reciprocity, and social proof.
:::

# Implications {.headline-only}

## Map vs. territority 

:::medium
The map is not the territory ... the only usefulness of a map depends on similarity of structure between the empirical world and the map.[^1] [[@korzybski1958science].]{.smaller}
:::

[^1]: Korzybski developed this concept during a period when many fields were grappling with the limits of human understanding - similar to our current AI era.

. . .

As all models are wrong, but some are useful:

- Continually test and update models
- Maintain epistemic humility
- Seek disconfirming evidence
- Use multiple maps of the same territory

## Built your Latticework

. . .

:::html-hidden
:::medium
Start with fundamental, versatile models, [build deliberately across disciplines, ]{.fragment} [and test models through application.]{.fragment}
:::
:::

:::notes
@parrish2020great recommends to begin with a handful of models from different disciplines rather than many from one field. Quality and diversity trump quantity. In the beginning, focus on mental models with the broadest application - like feedback loops, incentive structures, inversion, and first principles thinking. Read widely, but with purpose. Look for concepts that explain phenomena across multiple domains.

Put an emphasis on application as the key difference between collecting mental models and building a latticework is application. Use the models learned actively in your decision-making and keep a decision journal where you explicitly note which mental models you applied and review outcomes to refine your understanding.

Approach this as a career-long project — the compound interest of mental models accumulates over decades.
:::

. . .
 
Three-step approach:

1. Learn (study diverse fields)
2. Apply (use models in real decisions)
3. Reflect (record outcomes and refine)

# Q&A {.html-hidden .headline-only .unlisted}

# Literature
::: {#refs}
:::
