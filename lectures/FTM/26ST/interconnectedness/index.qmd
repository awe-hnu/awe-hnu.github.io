---
title: "Interconnectedness and Distributivity"
subtitle: "Future Technologies & Media (FTM)"
lang: en

author: "Andy Weeger"
date: "09.23.2025"

bibliography: ../assets/literature.bib

title-slide-attributes:
  data-background-image: ../assets/bg.jpg
  data-background-size: cover
  data-background-opacity: "1"
  data-background-color: '#0333ff'
  
format: 
  presentation-revealjs:
    output-file: slides.html
    include-before-body: ../assets/footer.html
    toc-depth: 2
    slide-level: 5
---

# Revision {.headline-only .vertical-center background-color="#0333ff"}

## Emerging technologies

@rotolo2015emerging outlines five attributes that classify emerging technologies and differentiate them from other technologies:

:::medium
:::incremental
1. Radical novelty    
2. Relatively fast growth    
3. Coherence    
4. Prominent impact    
5. Uncertainty and ambiguity    
:::
:::

## Hypothesis 1 {background-color="#0333ff"}

:::large
Emerging information technologies enable **multimodal** and **immersive** systems.
:::

## Multimodality

:::fragment
:::medium
Multimodality refers to the use of [multiple modes of communication]{.highlight} to to create meaning.
:::
:::

:::fragment
Multimodality implies that the use of several means of communication contributes to a better overall understanding of a message.
:::

:::aside
@adami2016introducing
:::

## Immersion

:::fragment
:::medium
Immersion refers to the state of [being deeply engaged, absorbed, or submerged in an environment]{.highlight}, either physically or mentally.
:::
:::

:::fragment
Immersion implies that the consciousness of the immersed person is detached from their physical self. Immersiveness is the quality or degree of being immersive.
:::

:::aside
@suh2018state, @lee2013presence
:::

## Interdependency

:::fragment
:::medium
Stimuli that determine the [immersiveness]{.highlight} of environments created by technology are [multimodal]{.highlight}.
:::
:::

:::medium
[Visual, ]{.fragment .fade-in-then-semi-out} 
[auditory, ]{.fragment .fade-in-then-semi-out} 
[tactile,]{.fragment .fade-in-then-semi-out} 
[olfactory, and]{.fragment .fade-in-then-semi-out} 
[interactive.]{.fragment .fade-in-then-semi-out} 
:::

## Hypoptheses 2 {background-color="#0333ff"}

:::large
Emerging information technologies enable **intelligent** and **affective** systems.
:::

## Intelligence

:::fragment
:::medium
Intelligent systems work in [complex environments]{.highlight}, have [cognitive abilities]{.highlight}, and exhibit
[complex behavior]{.highlight}.
:::
:::

:::fragment
The capacity to work in a complex environment is described as agency, cognitive abilities are, for instance, perception and language, and complex behavior is reflected, for instance, by rationality and learning.
:::

:::aside
@RusselNorvig2022AIMA 
:::

## Affection

:::fragment
:::medium
Affective computer systems exhibit human-like capabilities of [observation]{.highlight}, [interpretation]{.highlight} and [generation of emotions]{.highlight}.
:::
:::

:::fragment
Affective systems simulate empathy — they can interpret the emotional states of humans and adapt their behavior to them, giving an appropriate response for those emotions.
:::

:::aside
@tao2005affective
:::

# Hypothesis 3 {background-color="#0333ff"}

:::large
Emerging information technologies enable strongly **interconnected** and **distributed** systems.
:::

# Interconnectedness {.headline-only}

## Definition

:::fragment
:::medium
Interconnectedness refers to a [formal linkage between two different systems]{.highlight}. 
:::
:::

:::fragment
[Interdependence]{.highlight}, denotes a closer relationship in which two systems are not only connected, but depend on each other in some way, such as functionally.
:::

:::fragment
Not all interconnected systems are interdependent, \
but all interdependent systems are interconnected.
:::

:::aside
@zimmerman2001social
:::

## Computer networks

:::fragment
:::medium
A computer network is a collection of computers and devices connected so that they can [share information and services]{.highlight}.
:::
:::

:::fragment
Unlike phone lines or cable TV, which are designed for specific tasks, computer networks are flexible. 
They use general-purpose equipment that can handle many kinds of data.
:::

:::fragment
This [versatility]{.highlight} allows computer networks to support a vast and constantly evolving range of applications.
:::

:::aside
@peterson2007computer, @mansfield2009computer
:::

## Information systems

An information system (IS) ...

:::incremental
1. is defined by its perimeters/boundaries, certain inputs and outputs and its internal states
2. can be broken down into a set of smaller subsystems
3. can be considered in interaction with other systems (no IS is an island)
4. goes through various stages in its lifetime^[Lifecycle stages can be design, development, test, operation phases, for instance]
4. connects to other IS via interfaces^[Interfaces reflect the mechanisms how IS interact]
6. can be modeled at various levels of abstraction
7. can be viewed along several layers (e.g., purpose, function, composition)
8. can be described through different perspectives
:::

:::aside
@krcmar2015informationsmanagement
:::

## The Internet

:::fragment
:::medium
The Internet can be defined as a [public wide area computer network]{.highlight} that uses the TCP/IP protocol suite to [interconnect computer systems across the world]{.highlight}.
:::
:::

:::fragment
The Internet [provides a vast range of information resources and services]{.highlight} such as communication (e.g., electronic mail, telephony, and instant messaging), or file transfer (e.g., file sharing, FTP, video, and audio streaming), and the metaservice WWW.
:::

:::aside
@Sunyaev2020IC
:::

## Internet connection

::: {.r-stack .html-hidden}

![Internet connection example based on @Sunyaev2020IC](images/internet-1.svg){.fragment height="420"}

![&nbsp;](images/internet-2.svg){.fragment height="420"}

![&nbsp;](images/internet-3.svg){.fragment height="420"}

![&nbsp;](images/internet-4.svg){.fragment height="420"}

![&nbsp;](images/internet-5.svg){.fragment height="420"}

![&nbsp;](images/internet.svg){.fragment height="420"}

:::

:::aside
POP = Point of Presence, IXP = Internet Exchange Point, ISP = Internet Service Provider
:::

:::notes

1. The transmission is initiated by the end user , whose computer is connected with the Point of presence (POP) of his regional ISP. The POP is a local access point of an ISP where the telecommunication lines from commercial or domestic buildings are connected to the ISP’s network.
2. Destination is not in the regional ISPs network, therefore handover to a tier 1 ISP
3. Tier 1 ISP establishs a connection with the tier 1 network, the destination tier 2 ISP is part of
4. Tier 1 ISP establishs a connection with the destination tier 2 ISP
5. POP connects tier 2 ISP with the destination endpoint

**Tier-1 networks** are the backbone of the internet. They are the elite internet service providers (ISPs) that can reach every other network on the internet without relying on any other provider. Here's what makes them special:

- **Global reach:** Tier-1 networks have extensive infrastructure spanning multiple countries, often built and owned by them. This allows them to directly connect to a vast portion of the internet.
- **Peering agreements:** Unlike other ISPs that might pay to route traffic through larger networks, Tier-1s exchange traffic directly with each other through peering agreements. These agreements are typically free and reciprocal, meaning both parties benefit from the direct connection.
- **High capacity:** Tier-1 networks are built to handle massive amounts of internet traffic. They have robust infrastructure with high bandwidth capabilities to ensure smooth data flow.
- **Independent routing:** Because they don't rely on other providers for transit, Tier-1 networks have more control over how they route traffic. This can lead to faster and more reliable connections for users.

**Tier-2 networks** are the workhorses of the internet, providing internet access to a large portion of users. Unlike Tier-1 networks, they don't have the same level of global reach and independence, but they play a vital role in connecting users to the broader internet. Here's how they function:

- **Regional or national reach:** Tier-2 networks typically operate within a specific region or country. They have their own infrastructure but might not have the extensive global reach of Tier-1 providers.
- **Hybrid approach to traffic routing:** Tier-2 networks use a combination of methods to reach the wider internet:
  - **Peering:** They establish peering agreements with other networks, including Tier-1s and other Tier-2s, to directly exchange traffic for specific destinations. This helps reduce reliance on expensive transit fees.
  - **Transit:** In some cases, Tier-2 networks might purchase transit services from Tier-1 providers to reach parts of the internet that they don't have direct peering connections with.

:::

## Internet Protocol Suite

:::fragment
:::medium
The Internet Protocol Suite is a set of protocols that [enable communication over the Internet]{.highlight} by specifying data transmission, addressing, and routing.
:::
:::

:::fragment
The protocol suite encompasses protocols that are designed to work together to govern how data is transferred from one system to another. The most important protocols are [BGP]{.highlight}, [TCP]{.highlight} and [IP]{.highlight}.
:::

:::aside
@Sunyaev2020IC
:::

## Border Gateway Protocol (BGP)

:::fragment
:::medium
BGP is the [routing protocol that makes the Internet possible]{.highlight} as an interconnected system of autonomous networks.
:::
:::

:::fragment
It enables different, independent networks (i.e., *Autonomous Systems*) to [exchange routing information]{.highlight} and determine the [best paths for data]{.highlight} to travel across the global Internet.
:::

:::incremental
- Each network announces which IP addresses it can reach
- Neighboring networks share these announcements
- Routers build a dynamic map of possible paths
- BGP selects optimal routes based on policies and network conditions
:::

:::aside
@peterson2007computer
:::

:::notes
[Autonomous Systems (AS)]{.h4}

- The Internet consists of thousands of independent networks (ISPs, companies, universities, CDN providers)
- Each has an AS number (e.g., Google is AS15169, Cloudflare is AS13335)

[Why BGP matters for interconnectedness]{.h4}

- Creates a self-organizing, distributed mesh network
- No central authority controls routing decisions
- Enables dynamic adaptation when networks fail or change
- Makes CDN technologies like Anycast routing possible

[Connection to previous slide]{.h4}

- BGP is how Tier-1 and Tier-2 ISPs coordinate routing
- Peering agreements rely on BGP announcements
- This distributed decision-making exemplifies the hypothesis: emergent technology (BGP) enables strongly interconnected systems

**Example:** When you access a website through a CDN, BGP routes your request to the nearest edge node by having multiple locations announce the same IP address.
:::

## TCP/IP

[TCP/IP enables [open networks]{.highlight} ]{.fragment}
[by providing a [universal, layered language]{.highlight} for diverse devices to communicate, ]{.fragment}
[breaking data into flexible packets that find the best route, ]{.fragment}
[ensuring reliability with error-checking (TCP) (i.e., robustness^[Robustness refers to the a built in failure recovery mechanism that provides reliable end to end communication]), ]{.fragment}
[and operating on [open, non-proprietary standards,]{.highlight} making it a common, accessible foundation for the internet, ]{.fragment}
[[independent of specific hardware or software]{.highlight}^[Independency refers to the fact that there are no specific hardware and software requirements]).]{.fragment}


:::fragment
In essence, TCP/IP acts like a universal postal system for data, where everyone uses the same addressing (IP) and delivery rules (TCP), allowing any sender to reach any recipient, making the internet an [open platform]{.highlight}.
:::

:::incremental
- IP is responsible for addressing host interfaces, encapsulating data into datagrams and routing data from a source host to a destination host
- TCP guarantees that all bytes are received in the right order by using positive acknowledgements (ACK) with re-transmission^[The receiver responds with an positive acknowledgements (ACK) for ever data packet received, sender retransmits packets for missing ACKs after a given time]
::::

## TCP/IP stack

::: {.r-stack .html-hidden}

![The TCP/IP stack and its four abstraction layers](images/tcp-ip-1.svg){.fragment height="420"}

![&nbsp;](images/tcp-ip.svg){.fragment height="420"}

:::

:::notes

**The application layer**

- provides applications with standardized interfaces that allow to send data to other applications and receive data from them
- makes use of lower layers and treats them as black boxes

**The transport layer**

- provides end-to-end data transfer by delivering data from an application to a remote or a local host
- TCP is well suited where data integrity is important 
- UDP provides highly efficient but less reliable data transmission and has no error recovery mechanism. It is therefore used for applications that need a fast transport mechanism and can tolerate the loss of some data (e.g., video streams)

**The network layer** (or Internet layer)

- exchanges data in form of datagrams across network boundaries, provides a uniform networking interface and enables internetworking
- defines the addressing and routing structures for the TCP/IP protocol suite. (IP is a connectionless protocol that provides a routing function that forwards data to a specific destination in the network that is identified by its unique IP address)

**The data link layer** (network interface layer or physical layer)

- provides the interface to the actual network hardware
- is the lowest layer because TCP/IP is designed to be hardware independent. As a result TCP/IP
- may be implemented on top of any networking technology
- includes all protocols used to describe network topology and to move data between two different hosts (e.g., Ethernet)

**Scalability**

- Decentralized design - No central control; each router makes autonomous decisions
- Hierarchical addressing (IP) - Enables efficient routing in massive networks
- Statelessness at IP level - Routers don't need to store connection states
- Packet switching - Dynamic load distribution across different paths

**Flexibility**

- Layering principle - Each layer can be developed independently (each layer provides services to the layer above and uses services from the layer below, but doesn't need to know how those services work internally)
- Technology independence - Runs over any physical medium (Ethernet, WiFi, fiber optic, etc.)
- End-to-end principle - Complexity resides at endpoints, not in the network
- Modular protocols - TCP for reliability, UDP for speed - choose as needed


192.168.x.x is one of the private IP address ranges reserved specifically for internal networks like home networks, office LANs, etc.

**Private IP ranges (RFC 1918)**

- 10.0.0.0 - 10.255.255.255 (large organizations)
- 172.16.0.0 - 172.31.255.255 (medium networks)
- 192.168.0.0 - 192.168.255.255 (home/small office - most common)

**Key characteristics**

- Not routable on the public Internet - Your router blocks these from going out
- Reusable - Millions of homes can use 192.168.1.1 simultaneously without conflict
- NAT (Network Address Translation) converts them to your public IP when accessing the Internet (a clever workaround for IPv4's limited address space)

:::

## Data transmission example

:::r-stack
![A simplified example for a TCP/IP data transmission](images/tcp-ip-example.svg){.fragment height="420"}
:::

## Exercise {.discussion-slide}

:::medium
IPv6, packet switching, router,\
and Domain Name System (DNS)
:::

What is meant by the these concepts?\
Why are they important for the functioning of the Internet?\
How do they support the principles of independency and robustness?

Research the concepts and to prepare yourself to explain these.

{{< countdown "10:00" top="0">}}

:::notes

An **IP address** is a unique string of numbers separated by full stops that identifies each computer using the Internet Protocol to communicate over a network.

**IPv6** is composed of eight hexadecimal numbers, separated through colons (128 bit long binary string) --> 2^128 unique adresses

**Packet switching** describes a switching and transmission technology which splits complete messages into smaller packets. These packets can be transmitted along different lines of a network and they are re assembled into the original message by the receiving host.

A **router** is a device on a network that determines the best path for forwarding a data packet toward its destination. The router is connected to at least two networks and is located at the gateway where one network meets another.

The **Domain Name System (DNS)** is a hierarchically structured, distributed set of databases that map IP addresses to corresponding domain names.

:::

## Importance

The internet protocol suite based on TCP/IP is the foundation upon which the modern internet is built, and **its importance extends to a vast range of emergent digital technologies** such as IoT, Cloud computing, and mobile computing.

:::medium
[TCP/IP provides a [universal language]{.highlight},]{.fragment}
[is [open]{.highlight} and [standardized]{.highlight},]{.fragment}
[is highly [scalable]{.highlight},]{.fragment}
[and allows for great [flexibility]{.highlight}.]{.fragment}
:::

:::notes

TCP/IP acts as a universal language for communication between devices on a network. It defines a set of rules and protocols that ensure data is broken down into packets, addressed correctly, and delivered reliably across different network types. This universality allows various devices and technologies to seamlessly connect and exchange information.

TCP/IP is an open standard, meaning its specifications are publicly available and not controlled by any single entity. This openness fosters innovation and development as anyone can build technologies that interoperate with the TCP/IP framework. This has been crucial for the rapid growth and diversification of the digital landscape.

TCP/IP is designed to be scalable and flexible. It can handle a vast number of devices and networks, making it adaptable to the ever-growing demands of the digital world. This scalability is essential for supporting the massive growth of connected devices and data traffic associated with emergent technologies.

TCP/IP is a modular protocol suite, meaning it consists of independent protocols that work together. This modularity allows for flexibility and customization. New protocols can be added or existing ones modified to address specific needs of emerging technologies.

:::

## Summary

Emergent digital technologies facilitate more strongly interconnected systems, as they enhance

:::medium
[[networking infrastructure]{.fragment .highlight-current-blue fragment-index=1}, ]{.fragment .fade-in fragment-index=1}
[[connectivity capabilities]{.fragment .highlight-current-blue fragment-index=2}, and]{.fragment .fade-in fragment-index=2}
[[interoperability]{.fragment .highlight-current-blue fragment-index=3} (e.g. through standardization).]{.fragment .fade-in fragment-index=3}
:::

. . .

Examples of interconnected systems are\
[smart cities,]{.fragment} 
[connected cars (Car2X), and]{.fragment}
[smart supply chains.]{.fragment}

:::notes
**Advancements in networking infrastructure:** Emerging technologies like 5G networks and advancements in fiber optic cables are enabling faster and more reliable data transmission. This improved infrastructure allows for a greater volume of data exchange and supports the growing number of interconnected devices.

**Enhancements in connectivity capabilities:** Emerging technologies allow to connect and exchange data more efficiently and reliably, e.g., due to miniturization, improved battery live, improved network management, self-configuration capabilities, etc.

**Standardization and interoperability:** New protocols and standards are constantly emerging to ensure different technologies can work together. This fosters a more interconnected ecosystem where devices and platforms can communicate and share information more easily.

:::

## Discussion {.discussion-slide}

:::medium
What challenges arise from increasingly interconnected systems?
:::

:::notes

**Security concerns:** With more interconnected systems, there's a greater risk of cyberattacks. Ensuring the security and privacy of data in these systems is crucial.

**Digital divide:** Not everyone has equal access to the technologies that enable interconnected systems. Bridging this digital divide is essential to ensure everyone can benefit from these advancements.

:::

## Governance challenge

One challenge example relates to the governance of interconnected systems:

:::fragment
:::medium
Who controls the connections?
:::
:::

:::fragment 
Interconnected systems raise questions about power and control:
:::

[E.g., ISPs can influence traffic flow, when they act as [gatekeepers]{.highlight}]{.fragment} [and they can decide how data flows through their networks, and 
[prioritize certain traffic]{.highlight}.]{.fragment}


:::notes
This brings us to a critical debate: Net Neutrality
:::

# Case Study: Net Neutrality and Network Control {.headline-only}

## Motivation

We have seen how the internet—[the most important interconnected system today]{.fragment}[— is conceptualized as a [free and open network of networks]{.highlight}]{.fragment} [built on [open, non-proprietary standards]{.highlight} like TCP/IP and BGP.]{.fragment}

:::fragment
Yet it relies on [ISPs and Tier-1/Tier-2 networks as gatekeepers]{.highlight}.
:::

:::medium
:::fragment
So who actually has power in this interconnected system?
:::
:::

:::fragment
Net neutrality addresses this fundamental tension in network architecture, aiming to keep the internet [free and open]{.highlight}.
:::

## Concept

The phrase net neutrality is used to signify the concept that the Internet is merely a carrier of online content that does not distinguish one website from another^[Neut neutrality implies that, all things being equal, small content providers can deliver content just as fast, to the same people, as large content providers such as YouTube.]. 

. . .

:::medium
A “maximally useful public information network aspires to treat all content, sites, and platforms equally” [[@wu2003network, p. 142].]{.smaller}
:::

. . .

That usually means that ISPs charge consumers [only once for Internet access]{.highlight}, [do not favor one content provider
over another]{.highlight}, and [do not charge content providers for sending information]{.highlight} over broadband lines to end users.

## Exercise {.discussion-slide}

Form small groups to work on following tasks:

- Select one stakeholder group: ISPs, content providers, consumers, or government.
- Research your assigned stakeholder group and summarize their arguments for net neutrality and against net neutrality.

After individual research, each student presents their assigned stakeholder perspective.

We then discuss potential compromises or solutions that address the concerns of the various stakeholders.

{{< countdown "10:00" top="0">}}

# Distributivity {.headline-only}

## Distributed systems

. . .

:::medium
A distributed system is a collection of independent computers that appear to its users as a single coherent system.
:::

. . .

The independent computers, also known as nodes, communicate and coordinate their actions by passing messages as they do not share a common memory.

:::aside
@andrew2002distributed
:::

## CDNs {.discussion-slide}

:::medium
Let's discuss your insights:
:::

1. How does a CDN's architecture differ from a single centralized server?
2. What are the main benefits of such an architecture?

:::notes

[Key architectural differences]{.h4}

- Data replication vs. centralization: CDNs replicate content across nodes; centralized systems maintain one canonical copy
- Request routing intelligence: CDNs use DNS-based or Anycast routing to direct requests; centralized systems have fixed IP addresses
- Coordination overhead: CDNs need mechanisms to synchronize content, invalidate caches, and route traffic; centralized systems avoid this complexity
- Network topology: CDNs create a mesh or hub-spoke network of servers; centralized systems create a star topology with all clients connecting to one point

[Resilience benefits]{.h4}

When one node fails:

- Traffic automatically reroutes to other healthy nodes
- Only users served by that specific node experience disruption (temporary)
- DNS TTLs expire, and subsequent requests route around the failed node
- Content remains available because it's replicated elsewhere
- Geographic isolation: a power outage in Europe doesn't affect Asian users

[New vulnerabilities]{.h4}

When one node fails:

- Cascade risk: sudden traffic surge to remaining nodes can overload them
- Stale routing: DNS caching means some users may still try failed node until TTL expires
- Inconsistent user experience: some users see degraded performance while others don't

When coordination mechanisms fail_

- More catastrophic: the "brain" of the system is damaged
- Cache invalidation breaks: stale content persists, or new content doesn't propagate
- Routing logic fails: requests go to wrong nodes or load balancing breaks
- DNS resolution issues: users can't find any nodes, even though they're functional
- Content synchronization stops: nodes diverge, creating inconsistent versions

[The paradox]{.h4}

- Interconnectedness creates resilience through redundancy but introduces complexity
- More interconnections = more communication pathways that can fail
- Centralized coordination systems (like DNS, control planes) become single points of failure in distributed architectures
- The very protocols that enable distribution (BGP routing, DNS) become attack surfaces

**Real-world example:** The 2021 Fastly outage showed this perfectly—edge nodes were healthy, but a configuration bug in the coordination layer took down major websites globally. The distributed infrastructure was working, but the centralized control mechanism failed.

:::


## Key characteristics

Key characteristics of distributed systems include:

:::large
[[autonomy]{.fragment .highlight-current-blue fragment-index=1},]{.fragment .fade-in  fragment-index=1}
[[hidden complexity]{.fragment .highlight-current-blue fragment-index=2} (transparency),]{.fragment .fade-in  fragment-index=2}
[[reliability]{.fragment .highlight-current-blue fragment-index=3},]{.fragment .fade-in  fragment-index=3}
[[scalability]{.fragment .highlight-current-blue fragment-index=4}, and]{.fragment .fade-in  fragment-index=4}
[[efficiency]{.fragment .highlight-current-blue fragment-index=5}.]{.fragment .fade-in  fragment-index=5}
:::

:::fragment
[Open distributed system]{.highlight} are further characterized by their ability to integrate and interoperate with heterogeneous components, achieved through [standardized interfaces and protocols]{.highlight} that ensure different components can communicate and function together seamlessly.
:::

:::aside
@attiya2004distributed
:::

:::notes

Autonomy
: Each node operates independently and has its own local memory. Distributed systems can be composed of different types of hardware and software components. 

Transparency
: Transparency means hiding the complexity of distribution from users and applications. A distributed system that is able to present itself to users and applications as it were only a single computer system is said to be transparent.

Reliability
: Distributed systems are designed to be fault-tolerant. This means that even if one or more nodes fail, the system can continue to operate with minimal disruption. 

Scalability
: The system can grow by adding more nodes without significant changes to its architecture. This allows them to handle increasing demands and workloads effectively.

Efficiency
: While distributed systems introduce some overhead due to communication and coordination between nodes, they can also achieve efficiency by distributing tasks and leveraging parallel processing capabilities of multiple nodes.
:::

## Example: WWW

The World Wide Web is an information space in which the items of interest, referred to as resources, are identified by global identifiers called Uniform Resource Identifiers (URI) [@berners2004architecture].

. . .

:::medium
Resources are hosted on [servers distributed worldwide]{.highlight}, and information is routed efficiently through various networks and ISPs to reach the end user.
:::

## Distributed vs. decentralized systems

Both distributed and decentralized systems involve multiple nodes working together.\
However, the key difference lies in how control and decision-making are managed.

. . .

:::medium
Distributed systems can have a central coordinating authority, whereas decentralized systems [distribute control and decision-making]{.highlight} equally among all nodes.
:::

## Exercise {.discussion-slide}

:::medium
What specific distributed and/or decentralised systems are you familiar with?
:::

Form small groups to discuss examples and work out key characteristics.

{{< countdown "10:00" top="0">}}

:::notes

Key characteristics of distributed systems

- Physical separation: Nodes are located on different machines or networks.
- Communication: Nodes communicate through message passing.
- Coordination: There may be a central authority that coordinates the actions of the nodes, but it is not a requirement.
- Resource sharing: Resources such as processing power and data are shared across nodes.
- Fault tolerance: The system can continue to operate even if some nodes fail.
- Scalability: The system can grow by adding more nodes without significant changes to its architecture

Key characteristics of decentralized systems

- No central authority: There is no single point of control; each node operates independently.
- Equal authority: Every node has equal authority and decision-making capability.
- Consensus-based decision-making: Decisions are made by consensus among the nodes.
- Fault tolerance: The system is resilient to failures because there is no single point of failure.
- Scalability: The system can scale by adding more nodes, similar to distributed systems

Key differences

- Control and governance
- Decision-making
- Failure impact
- Application
- Complexity and maintenance

:::

## Degrees of decentralization

::: {.r-stack .html-hidden}

![Degree of decentralization [@Sunyaev2020IC]](images/centralization-1.svg){height="420"}

![&nbsp;](images/centralization-2.svg){.fragment height="420"}

![&nbsp;](images/centralization-3.svg){.fragment height="420"}

![&nbsp;](images/centralization.svg){.fragment height="420"}

:::

## Distributed computing

. . .

:::medium
Distributed computing refers to the [use of distributed systems to solve computational problems]{.highlight}—a problem is divided into many tasks, each of which is solved by one or more computers that communicate with each other. 
:::

:::aside
@attiya2004distributed
:::

:::notes
Distributed computing is a model in which components of a software system are shared among multiple computers to improve efficiency and performance. 
:::

## Examples

Some notable examples and use cases of distributed computing:

:::medium
[[Training neural networks]{.fragment .highlight-current-blue fragment-index=1},]{.fragment .fade-in  fragment-index=1}
[[analyzing large-scale DNA sequences]{.fragment .highlight-current-blue fragment-index=2},]{.fragment .fade-in  fragment-index=2}
[[climate modelling]{.fragment .highlight-current-blue fragment-index=3},]{.fragment .fade-in  fragment-index=3}
[[performing large-scale risk assessments]{.fragment .highlight-current-blue fragment-index=4}, and]{.fragment .fade-in  fragment-index=4}
[[swarm robotics]{.fragment .highlight-current-blue fragment-index=5}.]{.fragment .fade-in  fragment-index=5}
:::


:::notes
Training Neural Networks
: Distributed computing is extensively used in training AI and ML models, which require processing vast amounts of data. For instance, companies like Netflix and Amazon use distributed computing platforms to deploy recommendation algorithms that process millions of requests per second, providing personalized recommendations in real-time.

Genomics
: Distributed computing is used to analyze large-scale DNA sequences. Projects like the Human Genome Project, which mapped the entire human genome, leveraged distributed computing to handle the enormous computational resources required.

Climate Modeling
: Distributed systems are used to run complex climate models that predict weather patterns and climate change by processing large datasets from various sources.

Risk Assessment Models
: Financial institutions use distributed computing to perform large-scale risk assessments and economic simulations. For example, a global investment bank used Hazelcast Platform to perform highly parallelized calculations for risk exposure in capital markets

Swarm Robotics
: This involves multiple AI-powered autonomous agents working together to perform tasks, such as in robotics where multiple robots collaborate to complete a mission.
:::

## Summary

Distributed systems and distributed computing are integral to the advancement of emergent digital technologies as they they provide the necessary infrastructure for ...

:::medium
[[scalability and efficiency]{.fragment .highlight-current-blue fragment-index=1}, ]{.fragment .fade-in fragment-index=1}
[[fault tolerance and reliability]{.fragment .highlight-current-blue fragment-index=2},]{.fragment .fade-in fragment-index=2}
[[real-time processing]{.fragment .highlight-current-blue fragment-index=3},]{.fragment .fade-in fragment-index=3}
[[data privacy and security]{.fragment .highlight-current-blue fragment-index=4}, and]{.fragment .fade-in fragment-index=4}
[[cost-effectiveness]{.fragment .highlight-current-blue fragment-index=5}.]{.fragment .fade-in fragment-index=5}
:::

. . .

As digital technologies continue to evolve, [the role of distributed systems will only become more critical in enabling innovative applications and services]{.highlight} across various industries.

:::aside
@jogalekar2000evaluating, @attiya2004distributed, @andrew2002distributed, @Sunyaev2020IC
:::

:::notes

Scalability and efficiency
: Distributed systems allow for horizontal scaling, which means adding more nodes to handle increased workloads without significant changes to the system architecture. This scalability is crucial for modern applications that need to handle large volumes of data and high user traffic efficiently.

Fault tolerance and reliability
: Distributed systems are designed to be fault-tolerant, meaning they can continue to operate even if some components fail. This reliability is essential for critical applications such as financial transactions, healthcare systems, and autonomous vehicles, where downtime or data loss can have severe consequences.

Real-time processing
: emergent technologies like edge computing and the Internet of Things (IoT) require real-time data processing. Distributed systems enable data to be processed closer to its source, reducing latency and improving response times, which is vital for applications such as autonomous driving, industrial automation, and remote health monitoring.

Data privacy and security
: In distributed systems, data can be processed locally, which enhances privacy and security by reducing the need to transmit sensitive information over potentially insecure networks. This is particularly important for applications in healthcare, finance, and other sectors that handle sensitive data.

Cost-effectiveness
: Distributed computing can be more cost-effective than traditional centralized systems. By leveraging clusters of low-cost machines, organizations can achieve high performance without the need for expensive, high-end hardware.

:::

# Case Study: The Decentralized Web {.headline-only}

## Problem statement

:::fragment
:::medium
The internet as it is today^[The internet as of today is often termed Web 2.0] 
is increasingly dominated by a few large platforms^[Dominant internet players are the large platforms for e.g., social media, marketplaces, and CDN] 
and cloud service providers^[The internet big five, also known as GAFAM, are Google, Amazon, Meta (formerly Facebook), Apple, and Microsoft]
[— [counteracting the original decentralized nature of the internet]{.highlight}.]{.fragment}
:::

:::fragment
What has driven this development?
:::
:::

:::notes

**Reasons**

Network effects
:  Web 2.0 platforms thrive on network effects. The more users a platform has, the more valuable it becomes to others who want to join. This creates a snowball effect, making it difficult for new competitors to gain traction.  

Data advantage
:  The dominant companies collect vast amounts of user data on their platforms. This data allows them to personalize user experiences, target advertising effectively, and develop new features that keep users engaged. It's a cycle that feeds itself – the more data they have, the better their services become, attracting more users and even more data.

Barriers to entry
:  Building and maintaining large-scale web infrastructure is expensive.  It requires significant investments in servers, data centers, and content delivery networks. This creates a barrier for new players to compete with established giants who already have this infrastructure in place.

ISPs and control of access
:  ISPs (Internet Service Providers) act as the gatekeepers to the internet. They can potentially influence the speed and accessibility of certain websites, giving an edge to well-established platforms that have negotiated deals with them.

Regulations and laws
:  The current regulatory landscape might favor existing companies.  It can be challenging for new startups to navigate complex data privacy laws and copyright regulations.

:::

## Discussion {.discussion-slide}

:::large
Do developments like AI overview and AI mode in search worsen the problem?
:::

What is your opinion regarding the problems and responses discussed in the Decoder episode ["Google CEO Sundar Pichai on AI-powered search and the future of the web"](https://www.theverge.com/24158374/google-ceo-sundar-pichai-ai-search-gemini-future-of-the-internet-web-openai-decoder-interview)?

## Web 3.0

. . .

:::medium
The decentralized web, often referred to as Web3, [aims to restore the original decentralized nature of the internet]{.highlight}.
:::

. . .

Web3 is an evolving concept, which encompasses technologies broadly aimed at [providing greater transparency, openness, and democracy]{.highlight} on the web.

:::aside
@cao2022decentralized, @murray2023promise
:::

## Discussion {.discussion-slide}

:::large
If a decentralized web is the future,\
what is the present and the past?
:::

## Evolution of the web

| Feature          | Web 1.0                                     | Web 2.0                                       | Web 3.0 (Web3)                                                             |
|------------------|-----------------------------------------------|-----------------------------------------------|-----------------------------------------------|
| Focus            | Information access and publishing           | User-generated content and social interaction | Decentralization, user ownership, and machine understanding              |
| User Role        | Consumer of information                     | Content creator and consumer                  | Active participant and potential owner                                   |
| Data Storage     | Centralized servers                         | Centralized servers controlled by platforms   | Potentially distributed storage using blockchains                        |
| Key Technologies | HTML, static web pages                      | Social media platforms, mobile web, APIs      | Blockchain, cryptocurrencies, semantic web                               |
| Examples         | Simple websites, directories | Facebook, YouTube, Wikipedia                  | Early stage |

## Web 3.0 applications

@murray2023promise propose that following four key  applications could play a significant role in Web3:

:::medium
[[Cryptocurrencies]{.highlight-current-blue .fragment fragment-index=1}, ]{.fragment fragment-index=1}\
[[Metaverses]{.highlight-current-blue .fragment fragment-index=2}, ]{.fragment fragment-index=2}\
[[NFTs]{.highlight-current-blue .fragment fragment-index=3}^[NFT stands for Non-fungible Tokens], and]{.fragment fragment-index=3}\
[[DAOs]{.highlight-current-blue .fragment fragment-index=4}^[ DAO stands for Decentralized Autonomous Organizations]]{.fragment fragment-index=4}
:::

:::notes
Cryptocurrencies
: Digital tokens that can be used for secure online transactions without relying on banks. Think of them like digital money, but not controlled by any one government. These digital tokens could be the fuel for Web3, facilitating transactions and potentially serving as a unit of account within decentralized applications.

Metaverses
: Immersive virtual worlds accessed through VR or AR technology. Users can interact with each other, own virtual land or items (potentially as NFTs), and participate in various activities. These virtual worlds could be a major platform for Web3 experiences, where users interact, spend their crypto, and potentially own virtual assets as NFTs.

NFTs (Non-Fungible Tokens)
: Unique digital certificates that represent ownership of digital assets like artwork, music, or even virtual items within a Metaverse. Unlike currencies, each NFT is one-of-a-kind. NFTs could represent ownership of digital assets within the Metaverse and potentially other Web3 applications. Imagine using an NFT to represent your unique avatar in a virtual world.

DAOs (Decentralized Autonomous Organizations)
: Online communities with shared goals, governed by rules encoded on a blockchain (a secure digital ledger).  Decisions are made collectively by members through voting with tokens they own. DAOs could be a way to govern online communities and manage resources within Web3 projects. For instance, a DAO could oversee a virtual world within the Metaverse.
:::

## Homework {.discussion-slide}

:::medium
[Mastodon]{.highlight} — a good example of the future of social media in Web3?
:::

Research on Mastodon. Try to make up your mind if Mastodon is an interesting case in relation to Web 3.0 and how well it fits the Web 3.0 mold.

# Example: Cloud Computing {.headline-only}

## Definition

:::{.medium .fragment fragment-index=0}
Cloud Computing is a model which enables 
[[flexible and demand oriented access to]{.fragment .highlight-current-blue fragment-index=1}]{.fragment fragment-index=1}
[[a shared pool of configurable]{.fragment .highlight-current-blue fragment-index=2}]{.fragment fragment-index=2}
[[IT resources]{.fragment .highlight-current-blue fragment-index=3}]{.fragment fragment-index=3}
[[which can be accessed at any time and from anywhere via the Internet or a network]{.fragment .highlight-current-blue fragment-index=4}.]{.fragment fragment-index=4}
:::

:::aside
@mell2011nist
:::

## Discussion {.discussion-slide}

:::large
How do the characteristics of [distributed systems]{.highlight} apply to Cloud Computing? 
:::

## Emergence

:::{.fragment fragment-index=0}
The arrival of the cloud computing era can be seen as an [evolutionary development]{.highlight} in the history of computing. It is the result of the progress of various technologies, such as:
:::

[[Hardware]{.medium .fragment .highlight-current-blue fragment-index=1}\
(e.g., virtualization)]{.fragment fragment-index=1}

[[Internet technologies]{.medium .fragment .highlight-current-blue fragment-index=2}\
(e.g., web services)]{.fragment fragment-index=2} 

[[Distributed computing]{.medium .fragment .highlight-current-blue fragment-index=3}\
(e.g., networks, clusters)]{.fragment fragment-index=3} 

:::aside
@voorsluys2011introduction
:::

## Importance

:::fragment
:::medium
Cloud computing provides the infrastructure that fuels the digital transformation.
:::
:::

:::fragment
Enabled by the Internet and distributed computing, cloud computing
:::

:::incremental
- powers digital trends such as mobile computing, IoT, Digital Twins, and AI;
- accelerates industry dynamics and disrupts existing business models; and
- will continue to transform the world we live in on multiple levels and in various ways.
:::

:::aside
@benlian2018transformative
:::

## Cloud Computing stack

::: {.r-stack .html-hidden}

![The cloud computing stack and its three abstraction layers](images/cloud-stack-1.svg){.fragment height="420"}

![&nbsp;](images/cloud-stack-2.svg){.fragment height="420"}

![&nbsp;](images/cloud-stack-3.svg){.fragment height="420"}

![&nbsp;](images/cloud-stack-4.svg){.fragment height="420"}

![&nbsp;](images/cloud-stack.svg){.fragment height="420"}

:::

## Unique characteristics of cloud services

:::medium
[[Service-based IT resources]{.fragment .highlight-current-blue fragment-index=1}, ]{.fragment fragment-index=1}
[[on demand self-service]{.fragment .highlight-current-blue fragment-index=2}, ]{.fragment fragment-index=2}
[[ubiquitous access]{.fragment .highlight-current-blue fragment-index=3}, ]{.fragment fragment-index=3}
[[multitenancy]{.fragment .highlight-current-blue fragment-index=4}, ]{.fragment fragment-index=4}
[[location independence]{.fragment .highlight-current-blue fragment-index=5}, ]{.fragment fragment-index=5}
[[rapid elasticity]{.fragment .highlight-current-blue fragment-index=6}, and]{.fragment fragment-index=6}
[[pay per use billing]{.fragment .highlight-current-blue fragment-index=7}.]{.fragment fragment-index=7}
:::

:::aside
@mell2011nist
:::

:::notes

**Service-based IT resources**

- All cloud offerings can be expressed as a service
- Each service comes with a Service Level Agreement, defining functions and qualities

**On demand self-service**

- A consumer can provide computing capacity (e.g., server time, network storage and user licenses) unilaterally and automatically as required
- No need for human interaction with a service provider

**Ubiquitous access**

- Cloud services are provisioned via the Internet or private networks
- Cloud service rely on standardized interfaces to ease interaction
- Consumption by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations)

**Multitenancy**

- The provider’s computing resources are pooled to serve multiple consumers
- Using a multi tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand
- Sharing computing resources is part of what could makes cloud computing economically beneficial

**Location independence**

- There is a sense of location independence in that the cloud customer generally has no control over or knowledge of where the provided resources are actually located
- This challenges providers to be compliant with existing data protection requirements, among others

**Rapid elasticity**

- Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand
- To the consumer, the capabilities available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time

**Pay per use billing**

- Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts)
- Resource usage can be monitored, controlled, and reported, providing providing transparency for both the provider and consumer of the utilized service
:::

## Advantages of cloud services

Due to its inherent characteristics, cloud computing enables persons and organizations to achieve diverse benefits and opportunities, such as

:::medium
[[Low entry barriers]{.fragment .highlight-current-blue fragment-index=1}, ]{.fragment fragment-index=1}
[[access to leading edge tech]{.fragment .highlight-current-blue fragment-index=2}, ]{.fragment fragment-index=2}
[[focus on core capabilities]{.fragment .highlight-current-blue fragment-index=3}, ]{.fragment fragment-index=3}
[[reduced time to market]{.fragment .highlight-current-blue fragment-index=4}, ]{.fragment fragment-index=4}
[[greater flexibility]{.fragment .highlight-current-blue fragment-index=5}, and]{.fragment fragment-index=5}
[[enhanced cost-control]{.fragment .highlight-current-blue fragment-index=6}.]{.fragment fragment-index=6}
:::

:::aside
@Sunyaev2020IC
:::

## Hot topics

Cloud computing powers digital trends such as 

:::large
[[Cloud gaming]{.fragment .highlight-current-blue fragment-index=1}, ]{.fragment fragment-index=1}\
[[AI as a service]{.fragment .highlight-current-blue fragment-index=2}, ]{.fragment fragment-index=2}\
[[GAIA-X]{.fragment .highlight-current-blue fragment-index=3} as well as ]{.fragment fragment-index=3}\
[[Fog and Edge Computing]{.fragment .highlight-current-blue fragment-index=4}. ]{.fragment fragment-index=4}
:::

## Exercise {.discussion-slide}

:::medium
Research on a hot topic\
in cloud computing.
:::

Form small groups, select either **Cloud gaming**, **AI as a service**, or **GAIA-X**, and

- find a definition,
- outline the challenge(s) that the topic aims to address,
- and list the key characteristics

{{< countdown "15:00" top="0">}}

## Summary

:::medium
Interconnectedness and distributivity are crucial aspects of cloud computing.
:::

:::incremental
- [Cloud computing relies heavily on interconnected networks.]{.highlight} The internet, along with private networks, allows various cloud data centers, servers, and user devices to communicate and share resources seamlessly. This interconnectedness enables users to access cloud services and data from anywhere.
- [Cloud services are rendered by distributed systems.]{.highlight}  Cloud storage, applications, and services run on a vast network of interconnected computers spread across data centers. These individual computers work together to provide unified services.
:::

# Q&A {.html-hidden .unlisted .headline-only .vertical-center background-color="#0333ff" background-image="../assets/bg.jpg"}

# Literature
::: {#refs}
:::
