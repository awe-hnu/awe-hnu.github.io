---
title: "Neural Networks"
subtitle: "Introduction to AI (I2AI)"
lang: en
categories: ["Lecture Notes"]

bibliography: ../assets/literature.bib

date: "06.01.2025"

title-slide-attributes:
  data-background-image: ../assets/bg.jpeg
  data-background-size: cover
  data-background-opacity: "1"
  data-background-color: '#0333ff'

format:
  html:
    output-file: index.html
    margin-header: | 
      [Slides](slides.html){.btn .btn-primary target="blank"}
    format-links: false       
  presentation-revealjs:
    output-file: slides.html
    include-before-body: ../assets/footer.html   

---

# Neural Networks and Convolutional Neural Networks

* Author: Gemini (based on Johannes Maucher's lecture notes)
* Last Update: 06.01.2025

---

# Introduction to Neural Networks

## Natural vs. Artificial Neurons

### Natural Neuron

Neurons are the fundamental units for information processing in biological systems[cite: 253]. They receive electrical signals from other neurons via dendrites[cite: 253]. These signals are accumulated in the cell body, and if a certain threshold is exceeded, the neuron activates and sends an electrical signal via its axon[cite: 254]. Synapses, the connection points between dendrites and axons, regulate signal strength through their conductivity, which is adapted during learning[cite: 257, 258].

### Artificial Neuron

The artificial neuron models this biological process[cite: 262].
* **Input**: It calculates a weighted sum of inputs ($in = \sum_{j=0}^d w_j x_j$)[cite: 263]. The $x_j$ values are outputs from other neurons, each weighted by a scalar $w_j$ (representing synaptic conductivity)[cite: 264].
* **Activation Function**: This weighted sum is then passed through an **activation function** $g()$[cite: 266]. This function determines the neuron's output, similar to a threshold in a natural neuron[cite: 266].
* **Bias**: A special input $x_0$ with a constant value of 1, weighted by $w_0$ (often denoted as $b$), acts as a bias, shifting the activation function's output[cite: 273, 274, 275].

## Activation Functions

Activation functions process the weighted sum of inputs element-wise[cite: 90, 105]. They introduce non-linearity, enabling neural networks to learn complex patterns[cite: 392].

Common types include:
* **Threshold**: Outputs 1 if input $\geq 0$, else 0[cite: 267]. Provides a "hard" classification decision[cite: 342].
* **Sigmoid**: $g(in)=\frac{1}{1+exp(-in)}$[cite: 268]. Outputs values between 0 and 1, useful for probabilities in binary classification[cite: 343].
* **Tanh (Hyperbolic Tangent)**: $g(in)=\tanh(in)$[cite: 268]. Outputs values between -1 and 1.
* **ReLU (Rectified Linear Unit)**: $g(in)=\max(0, in)$[cite: 268]. Widely used in hidden layers due to its simplicity and effectiveness[cite: 105, 398].
* **Softmax**: $g(in_i,in_j)=\frac{\exp(in_i)}{\sum_{j=1}^{K} \exp(in_j)}$[cite: 269]. Used in the output layer for multi-class classification, providing probabilities for each class that sum to 1[cite: 352, 353].

## Neural Network Architecture: Layers

Artificial neurons are organized into layers[cite: 277].
* **Input Layer**: Receives the raw features of the data. Its number of neurons is determined by the number of features[cite: 280]. It doesn't perform any processing[cite: 282].
* **Hidden Layers**: One or more layers between the input and output layers[cite: 381]. Neurons in hidden layers typically use activation functions like sigmoid, tanh, or ReLU[cite: 398].
* **Output Layer**: Produces the network's final result. The number of neurons and the activation function depend on the specific task (e.g., regression, binary classification, multi-class classification)[cite: 281, 399].

A layer where all neurons are connected to all neurons in the successive layer is called a **fully-connected layer** or **dense layer**[cite: 318, 382].

---

# Supervised Learning with Neural Networks

Neural Networks are primarily applied for **supervised learning** tasks like classification and regression[cite: 295].

## General Concept of Supervised Learning

In supervised learning, the network learns from **labeled training data**, where each input has a corresponding "correct" target output[cite: 297].
1.  An input is fed to the network.
2.  The network calculates an output based on its current weights[cite: 299].
3.  This output is compared to the true target, and the difference (error) is used to adapt the network's weights[cite: 300, 301].

This iterative process aims to minimize the deviation between the network's output and the target for all training elements[cite: 301].

## Gradient Descent Learning

The core of training neural networks is **Gradient Descent**[cite: 303].
1.  **Loss Function**: A function $E(T, \Theta)$ quantifies the deviation between the network's output and the target[cite: 305].
2.  **Gradient Calculation**: The gradient of the loss function ($\nabla E(T, \Theta)$) indicates the direction of the steepest ascent[cite: 307, 308].
3.  **Weight Adaptation**: Parameters (weights $W_{i,j}$) are adapted in the direction of the *negative* gradient ($-\nabla E(T, \Theta)$) to iteratively minimize the loss[cite: 309, 310].
    * $W_{i,j} = W_{i,j} + \Delta W_{i,j} = W_{i,j} + \eta \cdot -\frac{\partial E}{\partial W_{i,j}}$ [cite: 311]
    * $\eta$ is the **learning rate**, a crucial hyperparameter controlling the step size of weight adaptations[cite: 311, 312].

### Batch vs. Stochastic Gradient Descent (SGD)

* **Batch Learning**: All training elements are considered for a single weight adaptation[cite: 365].
* **Online Learning / SGD**: Only a single training element is used for each weight adaptation[cite: 371]. This is an approximation of true gradient descent[cite: 372].
* **Minibatch Learning**: A practical compromise where the training data is partitioned into smaller "minibatches," and weight adaptations occur over these batches[cite: 377]. This balances computational efficiency with learning stability[cite: 377, 378].

---

# Single Layer Perceptron (SLP)

An SLP is a Feedforward Neural Network (FNN) with only an input and a single output layer[cite: 316]. All input neurons are connected to all output neurons (a "dense" or "fully-connected" layer)[cite: 317, 318].

## SLP for Regression

* **Output Layer**: Single neuron[cite: 321].
* **Activation Function**: Identity function ($g(in) = in$)[cite: 322].
* **Loss Function**: Sum of Squared Errors (SSE)[cite: 323]:
    $E(T,\Theta)= \frac{1}{2} \sum_{t=1}^N (r_t-y_t)^2$ [cite: 323]

## SLP for Binary Classification

* **Output Layer**: Single neuron[cite: 340].
* **Activation Function**: Threshold or Sigmoid function[cite: 341]. Sigmoid is preferred as its output can indicate a-posteriori probability[cite: 343].
* **Loss Function**: Binary Cross-Entropy[cite: 344]:
    $E(T,\Theta)= - \sum_{t=1}^N r_{t} \log y_{t}+(1-r_{t}) \log(1-y_{t})$ [cite: 344]

## SLP for K-ary Classification (K > 2 classes)

* **Output Layer**: K neurons, one for each class[cite: 351].
* **Activation Function**: Softmax function[cite: 352]. Each output $y_k$ represents the probability of belonging to class $C_k$, and $\sum y_k = 1$[cite: 353].
* **Loss Function**: Cross-Entropy[cite: 354]:
    $E(T,\Theta)= - \sum_{t=1}^N \sum_{k=1}^K r_{t,k} \log(y_{t,k})$ [cite: 354]

---

# Multi Layer Perceptron (MLP)

An MLP is an FNN with at least one hidden layer between the input and output layers ($L \geq 2$ layers)[cite: 381].
* **Non-linear models**: MLPs can learn non-linear relationships, unlike SLPs[cite: 391, 392].
* **Hidden Layers**: The number of hidden layers and neurons per layer are crucial hyperparameters that affect performance and complexity[cite: 393, 394].
* **Activation functions in hidden layers**: Typically sigmoid, tanh, ReLU, or leaky ReLU[cite: 398].
* **Output Layer**: Configuration (number of neurons, activation, loss) is the same as for SLPs, depending on the task[cite: 399].

## Backpropagation Algorithm

MLPs also use Gradient Descent, specifically the **Backpropagation Algorithm**, to adapt weights[cite: 408, 409]. This algorithm efficiently calculates the gradients for weights in all layers by propagating the error signal backward from the output layer through the hidden layers[cite: 409, 413].

---

# Convolutional Neural Networks (CNNs)

CNNs are a specialized type of deep neural network, often considered the "mother" of deep learning, especially for computer vision tasks like image classification and object detection[cite: 3, 4, 6]. Unlike conventional machine learning, CNNs *learn* relevant features from raw input data[cite: 8].

## Overall Architecture

A deep neural network consists of an "Extractor-Part" and a "Classifier-Part"[cite: 8, 9].
* The **Extractor-Part** learns a hierarchy of increasingly meaningful features from low-level input features (e.g., pixel values)[cite: 10].
* The **Classifier-Part** uses these informative features to determine the correct class[cite: 11].

For example, AlexNet, a famous CNN that won the ImageNet contest in 2012, recognized 1000 different objects from 15 million images[cite: 14, 15].

The primary layer types in a CNN are:
* **Convolutional layers** [cite: 21]
* **Pooling layers** [cite: 21]
* **Fully connected layers** [cite: 21]

## Convolutional Layer

This is a key concept of CNNs, applying a technique called **convolutional filtering** to extract features[cite: 23].
* **Learned Filters**: Unlike conventional image processing where filters are predefined, in CNNs, the filter coefficients (called **weights**) are **learned** during training to detect frequently occurring patterns[cite: 24, 88, 89].
* **Feature Maps**: The output of a convolutional filter is a **feature map**[cite: 89]. All elements in a single feature map are calculated using the same set of **shared weights**[cite: 90].
* **Activation**: Feature map elements are typically passed through an **activation function** element-wise[cite: 91].
* **Multiple Feature Maps**: Multiple feature maps are calculated in parallel from a given input, each with different sets of shared weights[cite: 91, 92].
* **Channels**: Input data often has multiple "channels" (e.g., RGB image has 3 channels)[cite: 93, 98]. For calculating a single feature map from multiple input channels, an individual filter is learned and applied for each channel[cite: 101].

### Concept of 2D-Convolution Filtering

A 2D filter is applied across a 2D input (e.g., an image)[cite: 27]. At each position, the scalar product of the filter coefficients and the input values covered by the filter is calculated[cite: 27]. This scalar product forms the output at that position[cite: 28].

* **Stepsize ($s$)**: The filter can be convolved with a step size greater than 1, yielding a smaller output[cite: 38, 39].
* **Zero-Padding ($p$)**: Adding layers of zeros around the input boundaries. This can help maintain the output size similar to the input size, especially with a step size of $s=1$ and uneven filter width[cite: 41, 42, 43].
* **Output Size**: For a quadratic input of side length $r$, a quadratic filter of side length $a$, padding $p$, and step size $s$, the output side length $o$ is given by $o = \frac{r-a+2p}{s}+1$[cite: 44].

## Pooling Layer

Pooling layers reduce the dimensionality of feature maps[cite: 115].
* **Fixed Operations**: Unlike convolutional layers, pooling layers do not learn weights; they perform fixed, often non-linear operations[cite: 117].
* **Common Operations**:
    * **Max-pooling**: Outputs the maximum value in the filter's current input region[cite: 118]. This is the most frequent type[cite: 118].
    * **Min-pooling**: Outputs the minimum value[cite: 118].
    * **Mean-pooling**: Outputs the arithmetic mean[cite: 118].
* **Non-overlapping Regions**: Typically, the step size $s$ equals the filter width $w$, meaning pooling operates on non-overlapping regions, further reducing size[cite: 119].

## Concatenation of Convolution and Pooling

In CNNs, a cascade of convolutional layers followed by pooling layers is common[cite: 123].
* **Feature Extraction and Reduction**: Convolutional layers extract meaningful features, while pooling layers reduce the spatial resolution and complexity[cite: 124].
* **Hierarchy of Features**: This process creates a hierarchy of features. Earlier layers capture fine-grained local details, while deeper layers capture more abstract, higher-level features with less spatial precision[cite: 149, 150].

## Fully Connected Layers (Classifier Part)

After several convolutional and pooling layers, the extracted features are typically fed into one or more fully connected layers[cite: 139].
* **Classification/Regression**: These layers act as a classifier or regressor, outputting the estimated class or numeric value[cite: 138, 139].
* **Serialization**: The 3D output of the last pooling layer (feature maps) is typically flattened into a 1D vector before being fed to the fully connected layers[cite: 142].
* **Output**: The final fully connected layer's output (e.g., 1000 neurons for ImageNet's 1000 categories) is often processed by a Softmax activation function for classification tasks, indicating the most probable class[cite: 19, 144, 145].

## CNN Summary

* CNNs learn **local structures** and build a **hierarchy of increasingly meaningful features**[cite: 149].
* They exhibit **translation invariance** because the filters learn to detect patterns regardless of their exact position in the input[cite: 148].
* Earlier layers retain more **location information** of features, which diminishes in deeper layers as spatial resolution decreases[cite: 150, 151].
* **Requirements**: CNNs (and deep neural networks in general) require large training datasets due to many learnable parameters[cite: 153]. They are most effective when input data exhibits **spatial or temporal correlations** (e.g., images, time-series, language)[cite: 155].

---

# Advanced Concepts (Briefly)

## Deconvolution (Transpose Convolution)

* This operation is used to **increase the size of the output**, making it larger than the input[cite: 169].
* It's applied in tasks like image resolution increase, generative networks (GANs), and semantic segmentation[cite: 170, 171, 172, 173].
* Despite its name, it's more accurately called **transpose convolution** because it is mathematically defined by multiplying the transposed filter matrix with the serialized input[cite: 174, 175].

## Dilated Convolution

* Allows filters to have a **larger "receptive field"** (the area of the input that influences the output at a given position) without increasing the number of filter parameters[cite: 180, 184].
* A **dilation rate ($d$)** introduces $d-1$ gaps between the elements of the filter, making the receptive field non-contiguous[cite: 181].

## Batch Normalization

* A trainable layer that normalizes data *within* the neural network, typically before the activation function[cite: 206, 208].
* Helps mitigate **Internal Covariate Shift** (changes in the distribution of network activations during training)[cite: 218].
* Enables better propagation of gradients, allowing for deeper networks and addressing the **vanishing gradient problem**[cite: 218].
* For each neuron, it normalizes its activation over a minibatch and then scales and shifts it using learned parameters ($\gamma$ and $\beta$)[cite: 219, 220, 221, 222, 223, 224].

## 1x1 Convolution

* A filter of size $1 \times 1$ that operates across all channels at a specific pixel location[cite: 228].
* Can be viewed as a Dense layer applied across channels.
* Primarily used for **dimensionality reduction** (reducing the number of channels) while keeping spatial resolution constant[cite: 229, 230]. Useful in architectures like Inception modules.

## Fully Convolutional Networks (FCNs)

* Replace dense layers with $1 \times 1$ convolutional filters[cite: 231].
* Allow the network to process images of varying sizes because no dense layers with fixed input sizes are present[cite: 231, 232].
* Outputs a **classification map** (defining class likelihood for regions), rather than a single class per input[cite: 233]. Often used for **semantic segmentation**[cite: 234].

## Inception Layer / GoogLeNet

* Allows **multiple filters of different sizes** to be applied in parallel within a single layer[cite: 235, 236].
* Enables learning of features spread over different spatial regions simultaneously[cite: 235].
* Modern Inception modules (like in GoogLeNet) often include $1 \times 1$ convolutions for dimensionality reduction to manage computational complexity[cite: 241, 242].

## Residual Blocks / ResNet

* Introduced **short-cut connections** that bypass one or more layers[cite: 243, 244].
* The idea is to learn a "residual mapping" $F(\mathbf{x}) = H(\mathbf{x}) - \mathbf{x}$ instead of the target mapping $H(\mathbf{x})$ directly[cite: 243]. This can make training very deep networks easier[cite: 243].
* The output of a residual block is the output of the bypassed layers plus the original input: $\mathbf{y}=F(\mathbf{x},\lbrace W_i,b_i \rbrace ) + \mathbf{x}$[cite: 246].

---

# Model Complexity: Bias-Variance Tradeoff

Selecting the right model complexity is a key challenge in Machine Learning[cite: 789]. This is illustrated by the **bias-variance tradeoff**[cite: 790, 791].

* **Bias**: Error from erroneous assumptions in the learning algorithm[cite: 792].
    * **High Bias (Underfitting)**: Model is too simple, misses relevant relationships, and performs poorly on both training and new data[cite: 792, 802].
* **Variance**: Error from sensitivity to small fluctuations in the training set[cite: 793].
    * **High Variance (Overfitting)**: Model is too complex, fits training data too closely (memorizes noise), but performs poorly on new, unseen data[cite: 794, 804].

The goal is to find a balance where the model is complex enough to capture the underlying patterns but simple enough to generalize well to new data[cite: 803]. This is often summarized by **Ockham's razor**: the simplest model that adequately explains the data likely has the best predictive power[cite: 805, 806].

---

# The Learning Process Phases

Machine learning projects follow a systematic workflow:

1.  **Training Phase**:
    * A model learns from a **training dataset** to minimize error or maximize reward[cite: 767, 768].
    * This involves iterative optimization of model parameters[cite: 768].
    * Overfitting (memorizing training examples instead of learning generalizable patterns) is a significant pitfall here[cite: 770].

2.  **Validation Phase**:
    * The model is evaluated on a separate **validation dataset** (unseen during training) to assess its generalization capacity[cite: 772, 773].
    * Used for evaluating individual models, comparing different approaches, and **tuning hyperparameters**[cite: 775].
    * Helps identify overfitting or underfitting before final deployment[cite: 776].

3.  **Test Phase**:
    * The chosen model is evaluated on a completely separate **test dataset** (unseen during training and validation) to provide an unbiased estimate of operational performance[cite: 778, 779].
    * The test dataset should mirror real-world data distribution[cite: 780].
    * Essential safeguard against deploying underperforming models[cite: 782].

4.  **Operational Mode**:
    * The model is deployed in real-world environments to fulfill its purpose[cite: 783, 784].
    * Requires ongoing monitoring for performance degradation (e.g., due to data shifts) and often incorporates feedback loops for periodic retraining[cite: 786, 787].

# Literature
::: {#refs}
:::