{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Probability Theory\"\n",
        "subtitle: \"Introduction to AI (I2AI)\"\n",
        "lang: en\n",
        "categories: [\"Lecture Notes\"]\n",
        "jupyter: python3\n",
        "\n",
        "bibliography: ../assets/literature.bib\n",
        "\n",
        "date: \"03.12.2025\"\n",
        "\n",
        "title-slide-attributes:\n",
        "  data-background-image: ../assets/bg.jpeg\n",
        "  data-background-size: cover\n",
        "  data-background-opacity: \"1\"\n",
        "  data-background-color: '#0333ff'\n",
        "\n",
        "format: \n",
        "  html:\n",
        "    output-file: index.html\n",
        "    format-links: false        \n",
        "---\n",
        "\n",
        "\n",
        "# Introduction {.headline-only}\n",
        "\n",
        "## Discussion {.html-hidden .discussion-slide .unlisted background-color=\"#efefef\"}\n",
        "\n",
        "::: larger\n",
        "What are challenges related to logic as an approach to **knowledge representation**?\n",
        ":::\n",
        "\n",
        "## Qualification problem\n",
        "\n",
        "Logic is good, but often **conclusions under uncertainty** need to be drawn, as:\n",
        "\n",
        ":::incremental\n",
        "- the knowledge of the world is incomplete (not enough information) or uncertain (sensors are unreliable);\n",
        "- every possible explanation for given percepts need to be considered (no matter how unlikely), which leads to a large belief-state full of unlikely possibilities and arbitrarily large contingent plans; and\n",
        "- rules about the world are often incomplete (e.g., are all preconditions for an action known?) or even incorrect\n",
        ":::\n",
        "\n",
        ":::notes\n",
        ":::callout-note\n",
        "#### Qualification problem\n",
        "\n",
        "In philosophy and AI, the qualification problem is concerned with the impossibility of listing all the preconditions required for a real-world action to have its intended effect.\n",
        "\n",
        "As we have learned, there is no complete solution within logic. System designers must use good judgment in deciding how much detail to specify in their model and what details to omit. The reason for that is quite simple: often, all the conditions for an action that are necessary to achieve the intended effect can't be known. Or if they can be known, they often lead to a large belief-state full of unlikely possibilities. This is called the **qualification problem in logic**.\n",
        "\n",
        "However, there are good news: probability theory allows all exceptions to be grouped together without explicitly naming them.\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Uncertainty in rules\n",
        "\n",
        "Take an expert dental diagnosis system as an example.\n",
        "\n",
        "$Toothache \\implies Cavity$\n",
        "\n",
        "\n",
        "\n",
        "This rule is incorrect as there are other causes for toothache, thus a better rule would be:\n",
        "\n",
        "\n",
        "\n",
        "$Toothache \\implies Cavity \\lor GumProblem \\lor Abscess \\lor...$\n",
        "\n",
        "\n",
        "\n",
        "However, as we don’t know all the causes, this rule for **diagnostic reasoning** is still incomplete[^1].\n",
        "\n",
        "\n",
        "\n",
        "What about a rule for **causal reasoning**?\n",
        "\n",
        "\n",
        "\n",
        "$Cavity \\implies Toothache$\n",
        "\n",
        "\n",
        "\n",
        "This is still wrong as not cavity does not always imply toothache. Furthger, it does not allow to reason from symptoms to causes.\n",
        "\n",
        "[^1]: See qualification problem.\n",
        "\n",
        "## Discussion {.html-hidden .discussion-slide .unlisted background-color=\"#efefef\"}\n",
        "\n",
        ":::large\n",
        "What do we learn from that?\n",
        ":::\n",
        "\n",
        "## Learnings \n",
        "\n",
        ":::incremental\n",
        "- We cannot enumerate all possible causes (i.e., **laziness**).\n",
        "- And even if we could, we do not know how correct the rules are (i.e., **theoretical ignorance**[^2]).\n",
        "- And even if we did there will always be uncertainty about the patient (i.e., **practical ignorance**[^3]).\n",
        "- And even if there would be no uncertainty about the case, our sensors could be imprecises (e.g., the recognition of a caviety).\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "Thus, formal logical reasoning systems have significant limitations when dealing with real-world problems where we lack complete information.\n",
        "\n",
        "[^2]: Theoretical ignorance refers to our incomplete understanding of the precise mechanisms and relationships between conditions and symptoms, even when we can identify them.\n",
        "[^3]: Practical ignorance refers to our unavoidable uncertainty about the specific details and circumstances of an individual case, even when we have comprehensive theoretical knowledge about the conditions involved (e.g., individual pain tresholds differ significantly).\n",
        "\n",
        "# Probability theory {.headline-only}\n",
        "\n",
        "## Probabilities\n",
        "\n",
        ":::incremental\n",
        "- We (and other agents) are convinced by facts and rules only up to a certain degree\n",
        "- One possibility for expressing the degree of belief is to use **probabilities**\\\n",
        "  (e.g., we expect that the informatio is correct in 9 out of 10 cases — probability of .9)\n",
        "- Probabilities sum up the “uncertainty” that stems from lack of knowledge.\n",
        "- Probabilities are not to be confused with vagueness\\\n",
        "  (e.g., the predicate tall is vague; the statement, “A man is 1.75–1.80m tall” is uncertain)\n",
        ":::\n",
        "\n",
        "## Example\n",
        "\n",
        "**Goal:** Be in Ulm at 8:15 to give a lecture\n",
        "\n",
        "\n",
        "\n",
        "There are several plans that achieve the goal:\n",
        "\n",
        ":::{.incremental}\n",
        "- $P_1:$ Get up at 6:00, take the bike, arrive at 7:30, take a shower, ...\n",
        "- $P_2:$ Get up at 6:30, take the car at 7:00, arrive at 7:45, ...\n",
        "- ...\n",
        ":::\n",
        " \n",
        "\n",
        "\n",
        "All these plans are correct, but they imply different **costs** and different **probabilities** of actually achieving the goal.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ":::smaller\n",
        "$P_2$ is probably the plan of choice, as the success rate of $P_1$ is only 80%, though the rewards are high.\n",
        ":::\n",
        "\n",
        "## Decision making under uncertainty\n",
        "\n",
        ":::large\n",
        "`(utility - cost) * probability`\n",
        ":::\n",
        "\n",
        ":::incremental\n",
        "- We have a choice of actions (or plans)\n",
        "- These can lead to different solutions with different **probabilities**\n",
        "- The actions have different (subjective) costs\n",
        "- The results have different (subjective) utilities\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "It would be rational to choose the action with the **maximum expected utility (MEU)** — the \"average\", or \"statistical mean\" of the outcome utilities minus the costs of the actions leading to the outcome, weighted by the probability of the outcome.\n",
        "\n",
        "## Discrete and continuous variables\n",
        "\n",
        "A random variable[^5] $X$ is a variable that can take multiple values $X=x_i$ depending on the outcome of a random event. We denote the set of possible values, that can be taken by the variable, by $V(X)$.\n",
        "\n",
        "[^5]:Variables in probability theory are called random variables\n",
        "\n",
        "\n",
        ":::incremental\n",
        "- If the outcomes are finite[^finite] or at least countable the random variable is said to be **discrete**. \n",
        "- If the possible outcomes are not finite, for example, drawing a real number $x \\in \\left[0,1\\right] \\subset \\mathbb{R}$, the random variable is said to be **continuous**.\n",
        ":::\n",
        "\n",
        "[^finite]: Finite outcomes are, e.g., the 6 possibilities in a dice-rolling event.\n",
        "\n",
        "\n",
        "\n",
        "The probability that the random variable $X$ takes the value $x$ is dentoted by $P(X=x)$ or for short $P(x)$. \n",
        "\n",
        "\n",
        "\n",
        "The description of the probabilities $P(x)$ for all possible $x \\in V(X)$ is called the **probability distribution** of variable $X$.\n",
        "\n",
        "# Unconditional probability {.headline-only}\n",
        "\n",
        "## Prior probability\n",
        "\n",
        "$P(x)$ denotes the unconditional probability or **prior probability** that $x$ will appear in the absence of any other information, e.g. $P(Cavity) = 0.1$\n",
        "\n",
        ":::{.incremental}\n",
        "- Prior probabilities can be obtained from statistical analysis or general rules\n",
        "- Logical connectors can be used to build **probabilistic propositions**, e.g. $P(Cavity \\land \\neg Insured) = 0.06$\n",
        "- Propositions can contain equations over random variables, e.g. $P(NoonTemp=x) = \\textrm{Uniform}(x;18C;26C)$[^6], usually called a **probability density function**\n",
        ":::\n",
        "\n",
        "[^6]:Represents the belief that the temperature at noon is distributed uniformly between 18 and 26 degrees Celcius\n",
        "\n",
        "## Probability mass function \n",
        "\n",
        "In the case of discrete random variables $X$, the probability distribution is called **probability mass function (PMF) $p_X(x)$**, it assigns to all $x \\in V(X)$ the corresponding probability $P(x)$. For all probabilities the following conditions must be satisfied:\n",
        "\n",
        "\n",
        "\n",
        "[**1. Non-negativity**]{.h4}\n",
        "\n",
        "The probability assigned to any value $X$ must be greater than or equal to zero.\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "p_X(x) \\geq 0 \\quad \\text{for all } x  &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "[**2. Normalization**]{.h4}\n",
        "\n",
        "The sum of the probabilities of all possible values that the random variable $X$ can take is equal to 1.\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "\\sum_{x \\in V(X)} p_X(x) = 1 &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "[**3. Support**]{.h4}\n",
        "\n",
        "The probability that the random variable $X$ takes a value outside its possible values is zero. So only values $X$ can actually take (with non-zero probability) are those in the support.\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "p_X(x) = 0 \\quad \\text{for all } x \\notin V(X) &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "## Cumulative distribution function\n",
        "\n",
        "The **cumulative distribution function (CDF) $F_X(x)$** of a real-valued random variable $X$ is the probability that $X$ will take a value less than or equal to $x$.\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "F_X(x) = P(X \\leq x) &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Hence, the probability that X takes a value larger than $x=a$ and smaller or equal than $x=b$ is:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(a <x \\leq b) = F_X(b) - F_X(a) &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Every CDF is non-decreasing and has a maximum value of 1.\n",
        "\n",
        "## Discrete uniform distribution\n",
        "\n",
        "For the experiment \"rolling a dice\" the possible outcomes are 1, 2, 3, 4, 5 and 6. The corresponding discrete random variable $X$, has the probability mass function $p_X(x)$, which is defined by:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(X=1) &= P(X=2) = P(X=3) =P(X=4) \\\\\n",
        "&= P(X=5) = P(X=6) = \\frac{1}{6}\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "Such a distribution, for which $P(x)$ is equal for all $x \\in V(X)$ is called a **uniform distribution**. "
      ],
      "id": "49e71117"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import comb\n",
        "from cycler import cycler\n",
        "\n",
        "plt.rc('axes', prop_cycle=(cycler('color', ['black']))) # Set default color to black\n",
        "plt.rc('text', color='black')\n",
        "plt.rc('xtick', color='black')\n",
        "plt.rc('ytick', color='black')\n",
        "plt.rc('axes', edgecolor='black')\n",
        "plt.rc('figure', facecolor='white')\n",
        "plt.rc('axes', facecolor='white')\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.stem(range(1,7),[1/6]*6)\n",
        "plt.xlabel(\"$x$\"), plt.ylabel(\"$PMF(x)$\"), plt.title(\"PMF of Dice Rolling Experiment\", y=1.1)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.stem(range(0,8),[np.min([1/6*i,1]) for i in range(0,8)])\n",
        "plt.xlabel(\"$x$\"), plt.ylabel(\"$CDF(x)$\"), plt.title(\"CDF of Dice Rolling Experiment\", y=1.1)\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5) # Adjust the horizontal spacing\n",
        "\n",
        "plt.show()"
      ],
      "id": "364d7bf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bernoulli distribution\n",
        "\n",
        "A discrete binary random variable $X$ has only two possible outcomes: $0$ or $1$, i.e. in this case $V(X) = \\lbrace 0,1 \\rbrace$. The corresponding probability mass function is the **Bernoulli distribution**, which is defined as follows:\n",
        "\n",
        "$$\n",
        "p_X(x) = \\left\\{ \n",
        "\\begin{array}{ll}\n",
        "p & \\mbox{ for } x=1 \\\\\n",
        "1-p & \\mbox{ for } x=0\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "For example, if the probability $P(X=1)=p=0.7$, the PMF of the Bernoulli distribution is as plotted below:"
      ],
      "id": "bbd0590b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "p=0.7\n",
        "plt.stem([0,1],[1-p,p])\n",
        "plt.xlabel(\"$X$\"), plt.ylabel(\"$P(X)$\"), plt.title(\"PMF of Bernoulli Distribution\", y=1.1)\n",
        "plt.show()"
      ],
      "id": "f6850f57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binomial distribution\n",
        "\n",
        "The **Binomial distribution** helps us calculate the probability of getting a specific number of successful outcomes (let's say $k$) in a fixed number of independent trials (let's say $n$).\n",
        "\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}{c}\n",
        "n \\\\ \n",
        "k\n",
        "\\end{array}\n",
        "\\right) \n",
        "p^k (1-p)^{n-k}\n",
        "$$\n",
        "\n",
        "For example in a coin-tossing experiment the probability of success is $P(X=1)=p=0.5$. If we toss the coin $n=5$ times the probability, that exactly $k=4$ tosses yield success is\n",
        "\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}{c}\n",
        "5 \\\\ \n",
        "4\n",
        "\\end{array}\n",
        "\\right) \n",
        "0.5^4 (1-0.5)^{5-4}\n",
        "= 0.15625 \n",
        "$$\n",
        "\n",
        "The PDF and CDF of binomial distributed variables of different success probabilities $p$ are plotted below:"
      ],
      "id": "32a45580"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import comb\n",
        "\n",
        "n=30\n",
        "p_list=[0.2,0.5,0.7]\n",
        "colors=[\"0.3\",\"0.5\",\"0.7\"] # Using grey tones\n",
        "markers=[\"o\"]\n",
        "marker_size = 3 # Reduced marker size\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "for p,c in zip(p_list,colors):\n",
        "    y=[comb(n,k)*p**k*(1-p)**(n-k) for k in range(n)]\n",
        "    plt.plot(range(n),y,marker=markers[0], color=c, linestyle=\"--\", label=\"p=\"+str(p), markersize=marker_size)\n",
        "plt.xlabel(\"k successes\")\n",
        "plt.ylabel(\"Prob. for k successes\")\n",
        "plt.legend()\n",
        "plt.title(\"PMF of Binomial Distribution, $n=30$\", y=1.1)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "for p,c in zip(p_list,colors):\n",
        "    y=[np.sum([comb(n,k)*p**k*(1-p)**(n-k) for k in range(i)]) for i in range(n)]\n",
        "    plt.plot(range(n),y,marker=markers[0], color=c, linestyle=\"--\", label=\"p=\"+str(p), markersize=marker_size)\n",
        "plt.xlabel(\"k successes\")\n",
        "plt.ylabel(\"Prob. for k successes\")\n",
        "plt.legend()\n",
        "plt.title(\"CDF of Binomial Distribution, $n=30$\", y=1.1)\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5) # Add space between plots\n",
        "\n",
        "plt.show()"
      ],
      "id": "718c14b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Geometric distribution\n",
        "\n",
        "The **Geometric distribution** models the number of tries you need to keep going until you finally get a success. Think about rolling a die until you roll a 6. The Geometric distribution helps you find the probability that it takes exactly $k$ rolls. Every roll is independent, has two potential outcomes (getting a 6 or not), and the probability of rolling a 6 ($p$) remains constant.\n",
        "\n",
        "$$\n",
        "(1-p)^{(k-1)} \\cdot p\n",
        "$$\n",
        "\n",
        "For the coin-tossing experiment, the probability that the first success comes ...\n",
        "\n",
        "- ... at the first toss ($k=1$) is $0.5^0 \\cdot 0.5 = 0.5$\n",
        "- ... at the second toss ($k=2$) is $0.5^1 \\cdot 0.5 = 0.25$\n",
        "- ... at the third toss ($k=3$) is $0.5^2 \\cdot 0.5 = 0.125$\n",
        "- ..."
      ],
      "id": "9ef23412"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n=7\n",
        "p_list=[0.3,0.5,0.7]\n",
        "colors=[\"0.3\",\"0.5\",\"0.7\"] # Using grey tones\n",
        "plt.figure(figsize=(7,4))\n",
        "\n",
        "for p,c in zip(p_list,colors):\n",
        "    y=[(1-p)**(k-1)*p for k in range(1,n)]\n",
        "    plt.plot(range(1,n),y,marker=\"o\", color=c, linestyle=\"--\", label=\"p=\"+str(p))\n",
        "plt.xlabel(\"k draws\")\n",
        "plt.ylabel(\"Prob. for first successes after k draws\")\n",
        "plt.legend()\n",
        "plt.title(\"Geometric Distribution for different success probabilities\", y=1.1)\n",
        "plt.show()"
      ],
      "id": "9fc6418c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Continuous random variables\n",
        "\n",
        "For **continuous** random variables, we use the **probability density function (PDF)**, written as $p_X(x)$, to understand how the variable's values are distributed. Think of it like the probability mass function (PMF) for discrete variables.\n",
        "\n",
        "The PDF tells us the **relative likelihood** of the random variable $X$ taking on a specific value $x$. While the exact probability of a continuous variable being any single value is zero (because there are infinitely many possibilities), the PDF helps us compare how likely different values are. A higher PDF value at one point means that values around that point are more likely to occur than values around a point with a lower PDF.\n",
        "\n",
        "More precisely, the PDF is used to find the probability of the random variable falling within a **range** of values. This probability is the **area under the PDF curve** over that range.\n",
        "\n",
        "Key properties of a PDF:\n",
        "\n",
        "- It's always non-negative: $p_X(x) > 0$.\n",
        "- The total area under the curve is 1: $\\int_{-\\infty}^{\\infty} p_X(x) \\cdot dx = 1$.\n",
        "- Unlike the PMF, the PDF's value can be greater than 1.\n",
        "\n",
        "## Cumulative distribution function\n",
        "\n",
        "The CDF, written as $F_X(x)$, tells us the probability that the random variable $X$ will be less than or equal to a specific value $x$. For continuous variables, this is the area under the PDF curve from negative infinity up to $x$:\n",
        "\n",
        "$$\n",
        "F_X(x)= \\int_{-\\infty}^{x} p_X(t) \\cdot dt\n",
        "$$\n",
        "\n",
        "Using the CDF, we can find the probability that $X$ falls within a range between $a$ and $b$ (where $a < b$):\n",
        "\n",
        "$$\n",
        "P(a< x \\leq b) = F_X(b)-F_X(a)=\\int_{a}^{b} p_X(t) \\cdot dt\n",
        "$$\n",
        "\n",
        "### The Gaussian distribution\n",
        "\n",
        "The **Gaussian distribution** (also called the normal distribution) is a common continuous distribution. Its PDF has a bell shape and is defined by its mean ($\\mu$) and standard deviation ($\\sigma$):\n",
        "\n",
        "$$p_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
        "\n",
        "The plots below show the PDF of a Gaussian distribution with the same mean but two different standard deviations. Notice how the PDF values can be greater than 0."
      ],
      "id": "7a82a784"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(6.5, 6)) # Create figure and two subplots in two rows, one column\n",
        "\n",
        "# Subplot 1: Standard Deviation = 1\n",
        "mean1 = 3\n",
        "std_dev1 = 1\n",
        "x1 = np.arange(0, 6, 0.01)\n",
        "pdf1 = 1 / (std_dev1 * np.sqrt(2 * np.pi)) * np.exp(- (x1 - mean1)**2 / (2 * std_dev1**2))\n",
        "axes[0].plot(x1, pdf1, color='0.5') # Grey tone\n",
        "axes[0].fill_between(x1, pdf1, alpha=0.4, color='0.8') # Light grey fill\n",
        "axes[0].set_title(\"PDF of Gaussian distributed Variable $\\sigma =1.0$\", y=1.1) # Adjust title position\n",
        "axes[0].set_xlabel(\"x\")\n",
        "axes[0].set_ylabel(\"Probability Density\")\n",
        "axes[0].grid(True)\n",
        "axes[0].text(1.05, 0.5, f\"Mean: {mean1:.2f}\\nStd Dev: {std_dev1:.2f}\", transform=axes[0].transAxes, verticalalignment='center') # Caption beside\n",
        "\n",
        "# Subplot 2: Standard Deviation = 0.3\n",
        "mean2 = 3\n",
        "std_dev2 = 0.3\n",
        "x2 = np.arange(0, 6, 0.01)\n",
        "pdf2 = 1 / (std_dev2 * np.sqrt(2 * np.pi)) * np.exp(- (x2 - mean2)**2 / (2 * std_dev2**2))\n",
        "axes[1].plot(x2, pdf2, color='0.3') # Darker grey tone\n",
        "axes[1].fill_between(x2, pdf2, color=\"0.6\", alpha=0.4) # Medium grey fill\n",
        "axes[1].set_title(\"PDF of Gaussian distributed Variable $\\sigma =0.3$\", y=1.1) # Adjust title position\n",
        "axes[1].set_xlabel(\"x\")\n",
        "axes[1].set_ylabel(\"Probability Density\")\n",
        "axes[1].grid(True)\n",
        "axes[1].text(1.05, 0.5, f\"Mean: {mean2:.2f}\\nStd Dev: {std_dev2:.2f}\", transform=axes[1].transAxes, verticalalignment='center') # Caption beside\n",
        "\n",
        "plt.tight_layout(h_pad=3) # Adjust vertical spacing between subplots\n",
        "plt.show()"
      ],
      "id": "513f26e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Joint probability {.headline-only}\n",
        "\n",
        "## Definiton\n",
        "\n",
        "The **joint probability** of two random variables $X$ and $Y$, denoted as $P(X=x_i, Y=y_j)$ or simply $P(x_i, y_j)$, represents the probability that $X$ takes the specific value $x_i$ **and** $Y$ simultaneously takes the specific value $y_j$. The comma \",\" signifies the logical \"and\".\n",
        "\n",
        "\n",
        "\n",
        "The **joint probability distribution** of two random variables $X$ and $Y$ is the set of all possible joint probabilities for all possible value combinations of $X$ (from its value set $V(X)$) and $Y$ (from its value set $V(Y)$):\n",
        "\n",
        "$$\n",
        "P(X=x_i, Y=y_j) \\quad \\forall \\quad x_i \\in V(X), y_j \\in V(Y)\n",
        "$$\n",
        "\n",
        "Consider two binary random variables: `Toothache` (True or False, denoted as $\\neg$Toothache) and `Cavity` (True or False, denoted as $\\neg$Cavity). The full joint probability distribution for these variables is given in the table below\n",
        "\n",
        "|             |           |            |\n",
        "|------------:|:---------:|:----------:|\n",
        "|             | Toothache | ¬Toothache |\n",
        "|    Cavity   |    0.04   |    0.06    |\n",
        "|  ¬Cavity    |    0.01   |    0.89    |\n",
        "\n",
        ": Probabilities of the atomic events (full joint distribution for the *Toothache*, *Cavity* world) {#tbl-distribution}\n",
        "\n",
        "From this table, we can directly read the joint probability of any combination of `Toothache` and `Cavity`. For example, the probability of having a toothache and a cavity is:\n",
        "\n",
        "$$\n",
        "P(\\text{Toothache=True, Cavity=True}) = 0.04\n",
        "$$\n",
        "\n",
        "Similarly, the probability of not having a toothache and not having a cavity is:\n",
        "\n",
        "$$\n",
        "P(\\text{Toothache=False, Cavity=False}) = 0.89\n",
        "$$\n",
        "\n",
        "## Generalization \n",
        "\n",
        "For $N$ random variables $X_1, X_2, \\ldots, X_N$, the joint probability is:\n",
        "\n",
        "$$\n",
        "P(X_1=x_{i_1}, X_2=x_{i_2}, \\ldots, X_N=x_{i_N}) \\quad \\text{or} \\quad P(x_{i_1}, x_{i_2}, \\ldots, x_{i_N})\n",
        "$$\n",
        "\n",
        "This represents the probability that $X_1$ takes value $x_{i_1}$ and $X_2$ takes value $x_{i_2}$ and so on, up to $X_N$ taking value $x_{i_N}$. \n",
        "\n",
        "The **joint probability distribution** for these $N$ variables is the collection of all such probabilities for all possible combinations of their values.\n",
        "\n",
        ":::small\n",
        "For continuous random variables, the joint probability distribution is described by a joint **cumulative distribution function** (CDF) or a joint **probability density function** (PDF). For discrete random variables, it's described by a **probability mass function** (PMF) or the CDF.\n",
        ":::\n",
        "\n",
        "## Other distributions\n",
        "\n",
        "These joint probability distributions are important because they help to derive other key distributions, as we will see in more detail below.\n",
        "\n",
        "Marginal distribution\n",
        ": The probability distribution of a subset of the variables, considering only those variables and ignoring the specific values of the others.\n",
        "\n",
        "Conditional probability distribution\n",
        ": The probability distribution of one variable given that we know the value(s) of other variable(s).\n",
        "\n",
        "## Independence of random variables\n",
        "\n",
        "Two random variables $X$ and $Y$ are considered independent if the value taken by one variable does not influence the value taken by the other.\n",
        "\n",
        ":::small\n",
        "Example: Two consecutive rolls of a fair die are independent events. The outcome of the second roll is not affected by the outcome of the first roll. In contrast, in a Lotto draw where balls are drawn without replacement, the outcomes are dependent. The probability of drawing a specific number on the second draw depends on which number was drawn first.\n",
        ":::\n",
        "\n",
        "Two random variables $X$ and $Y$ are independent if and only if their joint probability can be factored into the product of their individual probabilities (also known as marginal probabilities):\n",
        "\n",
        "$$\n",
        "P(X=x_{i}, Y=y_j) = P(X=x_{i}) \\cdot P(Y=y_j)\n",
        "$$\n",
        "\n",
        ":::small\n",
        "Example: For two independent dice rolls, the probability of getting a 1 on the first roll ($X=1$) and a 2 on the second roll ($Y=2$) is:\n",
        ":::\n",
        "\n",
        "$$\n",
        "P(X=1, Y=2) = P(X=1) \\cdot P(Y=2) = \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{1}{36}\n",
        "$$\n",
        "\n",
        "## Marginal probability\n",
        "\n",
        "The **marginal distribution** of a subset of random variables from a larger set is the probability distribution of only those variables in the subset, without considering the specific values of the other variables. It essentially gives the probabilities of the values of the chosen variables by \"averaging out\" or \"summing over\" the possibilities of the other variables.\n",
        "\n",
        "If we have a set of random variables $X_1, X_2, \\ldots, X_N$ and we know their joint probability distribution $P(X_1=x_{i_1}, X_2=x_{i_2}, \\ldots, X_N=x_{i_N})$, we can find the marginal probability distribution for a subset ${X_{i_1}, X_{i_2}, \\ldots, X_{i_Z}}$ by marginalizing (summing or integrating over) the variables that are not in the subset.\n",
        "\n",
        "**Marginalization law**: For two discrete random variables $X$ and $Y$ with a known joint probability distribution $P(x_i, y_j)$, the marginal probability of $X$ taking the value $x_i$ is obtained by summing the joint probabilities over all possible values of $Y$:\n",
        "\n",
        "$$\n",
        "P(x_i) = \\sum_{y_j \\in V(Y)} P(x_i, y_j)\n",
        "$$\n",
        "\n",
        "Similarly, the marginal probability of $Y$ taking the value $y_j$ is:\n",
        "\n",
        "$$\n",
        "P(y_j) = \\sum_{x_i \\in V(X)} P(x_i, y_j)\n",
        "$$\n",
        "\n",
        "The variables we are interested in (like $X$ in the first equation) are called marginal variables.\n",
        "\n",
        "----\n",
        "\n",
        "[**Generalization to multiple variables**]{.h4}\n",
        "\n",
        "For three random variables $X, Y, Z$, the marginal probability of $X$ is:\n",
        "\n",
        "$$\n",
        "P(x_i) = \\sum_{y_j \\in V(Y)} \\sum_{z_k \\in V(Z)} P(x_i, y_j, z_k)\n",
        "$$\n",
        "\n",
        "This concept extends to any number of variables and any subset.\n",
        "\n",
        "[**Example**]{.h4}\n",
        "\n",
        "Let's calculate the marginal probability of having a cavity ($P(\\text{Cavity=True})$). To do this, we sum the joint probabilities where `Cavity` is True, across all possible values of `Toothache`:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(\\text{Cavity=True}) &= P(\\text{Toothache=True, Cavity=True}) \\\\\n",
        "&+ P(\\text{Toothache=False, Cavity=True})\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "Substituting the values from the table:\n",
        "\n",
        "$$\n",
        "P(\\text{Cavity=True}) = 0.04 + 0.06 = 0.10\n",
        "$$\n",
        "\n",
        "So, the marginal probability of having a cavity is 0.10 or 10%. This probability considers all individuals in our population, regardless of whether they have a toothache or not.\n",
        "\n",
        "Similarly, we can calculate the marginal probability of not having a cavity ($P(\\text{Cavity=False})$) by summing the joint probabilities where `Cavity` is False:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(\\text{Cavity=False}) &= P(\\text{Toothache=True, Cavity=False}) \\\\\n",
        "&+ P(\\text{Toothache=False, Cavity=False})\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "Substituting the values from the table:\n",
        "\n",
        "$$\n",
        "P(\\text{Cavity=False}) = 0.01 + 0.89 = 0.90\n",
        "$$\n",
        "\n",
        "The marginal probability of not having a cavity is 0.90 or 90%.\n",
        "\n",
        "These calculations demonstrate how we can obtain the probability distribution for a single variable (`Cavity`) by marginalizing over the other variable (`Toothache`) in the joint probability distribution.\n",
        "\n",
        "# Conditional probability {.headline-only}\n",
        "\n",
        "New information (usually called evidence) can change the probability, e.g. the probability of a cavity increases if we know the patient has a toothache\n",
        "\n",
        "The conditional probability of a random variable $X$ taking the value $x_i$ given that another random variable $Y$ has taken the value $y_j$, denoted as $P(X=x_i | Y=y_j)$ or $P(x_i | y_j)$, is the probability of $X=x_i$ occurring under the condition that $Y=y_j$ is already known or has been observed.\n",
        "\n",
        "Marginal probability is the probability of an event occurring on its own, whereas conditional probability considers the occurrence of one event given that another has already happened. This introduces a dependency in the probability calculation.\n",
        "\n",
        "The conditional probability of $X$ given $Y$ is calculated as the ratio of the joint probability of $X$ and $Y$ to the marginal probability of $Y$ (provided $P(y_j) > 0$):\n",
        "\n",
        "$$\n",
        "P(x_i | y_j) = \\frac{P(x_i \\land y_j)}{P(y_j)} = \\frac{P(x_i, y_j)}{P(y_j)}\n",
        "$$\n",
        "\n",
        "## Product rule\n",
        "\n",
        "By rearranging the conditional probability equation we can calculate a joint probability as a product of a conditional probability and an a-priori probability:\n",
        "\n",
        "$$\n",
        "P(x_i,y_j)= P(x_i|y_j)\\cdot P(y_j)\n",
        "$$\n",
        "\n",
        "This is actually the most simple case of the product rule. \n",
        "\n",
        "For 3 variables we can write:\n",
        "\n",
        "$$\n",
        "P(x_i,y_j,z_k)= P(x_i|y_j,z_j)\\cdot P(y_j,z_j).\n",
        "$$\n",
        "\n",
        "Since the last factor on the right hand side of this equation can be again written as \n",
        "\n",
        "$$\n",
        "P(y_j,z_j)= P(y_j|z_k)\\cdot P(z_k), \n",
        "$$\n",
        "\n",
        "we finally obtain:\n",
        "\n",
        "$$\n",
        "P(x_i,y_j,z_k)= P(x_i|y_j,z_j)\\cdot P(y_j|z_k)\\cdot p(z_k)\n",
        "$$\n",
        "\n",
        "I.e. the joint probability can be expressed as a product of conditional probabilities and an a-priori probability.\n",
        "\n",
        "This can be generalized to the case of $N$ random variables. The general form of the chain rule is: \n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(x_{i_1}, x_{i_2},  \\ldots x_{i_{N}}) &= P(x_{i_1} | x_{i_2},  \\ldots x_{i_{N}}  ) \\cdot P(x_{i_2} | x_{i_3},  \\ldots x_{i_{N}}) \\\\\n",
        "& \\cdot P(x_{i_3} | x_{i_4},  \\ldots x_{i_{N}})  \\cdots P(x_{i_{N}}) \\\\\n",
        "&= \\prod\\limits_{j=1}^N P(x_{i_j} | x_{i_{j+1}},  \\ldots x_{i_{N}}  )\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "# Bayesian inference\n",
        "\n",
        "## Bayes' Theorem\n",
        "\n",
        "**Bayes' Theorem** is a fundamental concept in probability theory and serves as the cornerstone of modern AI and machine learning. It allows us to update our beliefs based on new evidence.\n",
        "\n",
        "At its core, Bayes' Theorem relates conditional probabilities. Starting from the conditional probability formula:\n",
        "\n",
        "$$\n",
        "P(x_i | y_j) = \\frac{P(x_i, y_j)}{P(y_j)}\n",
        "$$\n",
        "\n",
        "We can derive Bayes' Theorem as:\n",
        "\n",
        "$$\n",
        "P(x_i | y_j)=\\frac{P(y_j | x_i) P(x_i)}{P(y_j)}\n",
        "$$\n",
        "\n",
        "Each term in this equation has a specific interpretation:\n",
        "\n",
        "* $P(x_i | y_j)$ is the **posterior probability:**\\\n",
        "  our updated belief about $x_i$ after observing evidence $y_j$\n",
        "* $P(x_i)$ is the **prior probability:**\\\n",
        "  our initial belief about $x_i$ before seeing any evidence\n",
        "* $P(y_j | x_i)$ is the **likelihood:**\\\n",
        "  the probability of observing evidence $y_j$ if $x_i$ is true\n",
        "* $P(y_j)$ is the **evidence:**\\\n",
        "  the total probability of observing $y_j$ under all possible conditions\n",
        "\n",
        "## Computing the evidence term\n",
        "\n",
        "The denominator $P(y_j)$ can be expanded using the law of total probability[^LtP]:\n",
        "\n",
        "$$\n",
        "P(y_j) = \\sum_{x_k \\in V(X)} P(y_j | x_k) P(x_k)\n",
        "$$\n",
        "\n",
        "[^LtP]: The law of total probability states that if you have a sample space divided into mutually exclusive and exhaustive events (often called a partition), then the probability of any event $y_j$ can be calculated by adding up the conditional probabilities of $y_j$ given each event in the partition $X$, weighted by the probabilities of those partition events (i.e., representing all possible paths or scenarios through which the event can occur).\n",
        "\n",
        "Where $V(X)$ represents all possible values of the random variable $X$.\n",
        "\n",
        "This gives us the complete form of Bayes' Theorem:\n",
        "\n",
        "$$\n",
        "P(x_i | y_j)=\\frac{P(y_j | x_i) P(x_i)}{\\sum\\limits_{x_k \\in V(X)}P(y_j | x_k) P(x_k)}\n",
        "$$\n",
        "\n",
        "## Bayesian inference in practice\n",
        "\n",
        "Bayesian inference is the process of applying Bayes' Theorem to update probabilities as new information becomes available. Here's how it works in practice:\n",
        "\n",
        "1. Start with a **prior probability** $P(x_i)$ based on existing knowledge\n",
        "2. Collect new evidence $y_j$\n",
        "3. Calculate how likely that evidence would be under different scenarios using the **likelihood** $P(y_j|x_i)$\n",
        "4. Update your belief to the **posterior probability** $P(x_i|y_j)$ using Bayes' Theorem\n",
        "\n",
        "[**Example scenario**]{.h4}\n",
        "\n",
        "Imagine we want to determine if a person has a certain disease.\n",
        "\n",
        "- Prior: 1% of the population has the disease, so $P(\\text{disease})=0.01$\n",
        "- Likelihood: A test is 95% accurate for positive cases, so $P(\\text{positive}|\\text{disease})=0.95$\n",
        "- Evidence: We need to account for both true positives and false positives\n",
        "- Posterior: We calculate the probability of having the disease given a positive test result\n",
        "\n",
        "## Naive Bayes classifiers\n",
        "\n",
        "Naive Bayes classifiers are based on Bayes' Theorem but make a simplifying assumption that the features (or predictors) are conditionally independent given the class label. This assumption is rarely true in real-world data, which is why it's called \"naive,\" but the classifier often performs surprisingly well despite this simplification.\n",
        "\n",
        "Given a feature vector[^fV] $X = (x_1, x_2, ..., x_n)$ and a class variable[^CV] $Y$, Bayes' Theorem gives us:\n",
        "\n",
        "[^fV]: A feature vector is an n-dimensional vector of numerical features that represent an object. It's essentially an n-dimensional vector where each dimension represents a specific attribute. Features can be numerical (age, income), categorical (color, brand), binary (yes/no), or derived (calculated from other features).\n",
        "\n",
        "[^CV]: A class variable (Y) is what we're trying to predict or classify. It represents the outcome or category that our machine learning model should determine. Examples include: spam/not spam, disease present/absent, sentiment (positive/negative/neutral). Can be binary (two options), multi-class (several distinct categories), or multi-label (multiple categories can apply simultaneously).\n",
        "\n",
        "$$P(Y|X) = \\frac{P(X|Y) \\cdot P(Y)}{P(X)}$$\n",
        "\n",
        "The naive independence assumption states that:\n",
        "\n",
        "$$P(X|Y) = P(x_1|Y) \\cdot P(x_2|Y) \\cdot ... \\cdot P(x_n|Y)$$\n",
        "\n",
        "This gives us:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(Y|X) &= \\frac{P(X|Y) \\cdot P(Y)}{P(X)}\\\\\n",
        "&= \\frac{P(x_1|Y) \\cdot P(x_2|Y) \\cdot ... \\cdot P(x_n|Y) \\cdot P(Y)}{P(x_1, x_2, \\ldots, x_n)}\\\\\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "In practice, when calculating Naive Bayes, often the unnormalized probabilities are calculated for each class:\n",
        "\n",
        "$$P(x_1|Y) \\cdot P(x_2|Y) \\cdot ... \\cdot P(x_n|Y) \\cdot P(Y)$$\n",
        "\n",
        "And then normalized by dividing by the total probability:\n",
        "\n",
        "$$P(x_1, x_2, \\ldots, x_n) = \\sum_{j=1}^{m} \\left[\\prod_{i=1}^{n} P(x_i|Y=y_j)\\right] \\cdot P(Y=y_j)$$\n",
        "\n",
        "Overall, Naive Bayes simplify the calculation dramatically, as we now only need to estimate these simpler conditional probabilities from the training data.\n",
        "\n",
        "[**Types of Naive Bayes classifiers**]{.h4}\n",
        "\n",
        "1. **Gaussian Naive Bayes**: For continuous features, assumes that values follow a Gaussian distribution.\n",
        "2. **Multinomial Naive Bayes**: Commonly used for text classification, where features represent word frequencies or counts.\n",
        "3. **Bernoulli Naive Bayes**: Used when features are binary (e.g., words present/absent in a document).\n",
        "\n",
        "[**Advantages**]{.h4}\n",
        "\n",
        "- Simple and fast to train and make predictions\n",
        "- Works well with small datasets\n",
        "- Handles high-dimensional data efficiently\n",
        "- Often performs well even when the independence assumption is violated\n",
        "- Requires less training data than many other classifiers\n",
        "- Not sensitive to irrelevant features\n",
        "\n",
        "[**Limitations**]{.h4}\n",
        "\n",
        "- The \"naive\" independence assumption rarely holds in real data\n",
        "- Can be outperformed by more sophisticated models\n",
        "- May give poor probability estimates (though the class predictions might still be accurate)\n",
        "\n",
        "[**Applications**]{.h4}\n",
        "\n",
        "- Text classification (spam filtering, sentiment analysis, document categorization)\n",
        "- Medical diagnosis\n",
        "- Recommendation systems\n",
        "- Real-time prediction due to its computational efficiency\n",
        "\n",
        "\n",
        "# Exercises {.vertical-center background-color=black}\n",
        "\n",
        "## Conditional probability\n",
        "\n",
        "Show from the definition of conditional probability that $P(a | b \\land a) = 1$.\n",
        "\n",
        ":::{.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "The definition of conditional probability states that the conditional probability of A given B is the probability of the intersection of A and B divided by the probability of B.\n",
        "\n",
        "For any propositions $a$ and $b$ (if $P(b)>0$), we have\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(a|b)=\\frac{P(a \\land b)}{P(b)} &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "Thus:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(a|b \\land a) = \\frac{P(a\\land(b \\land a))}{P(b \\land a)} &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "Which is equal to \n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(a|b \\land a) = \\frac{P(b \\land a)}{P(b \\land a)} = 1 &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Beliefs\n",
        "\n",
        "An agent holds the three beliefs: $P(A)=0.4$, $P(B)=0.3$, and $P(A \\lor B)=0.5$.\n",
        "\n",
        "What is the probability of $A \\land B$? \n",
        "\n",
        "Make up a table like the one in @tbl-distribution to answer that question. \n",
        "\n",
        ":::{.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "|   |   |    |\n",
        "|--:|:-:|:--:|\n",
        "|   | B | ¬B |\n",
        "| A | a | b  |\n",
        "| ¬A| c | d  |\n",
        "\n",
        "- $P(A) = a + b = 0.4$\n",
        "- $P(B) = a + c = 0.3$\n",
        "- $P(A \\lor B) = a + b + c = 0.5$\n",
        "- $P(True) = a + b + c + d = 1$\n",
        "\n",
        "From these, it is straightforward to infer that a = 0.2, b = 0.2, c = 0.1, and d = 0.5. \n",
        "\n",
        "Therefore, $P(A \\land B) = a = 0.2$. Thus the probabilities given are consistent with a rational assignment, and the probability $P(A \\land B)$ is exactly determined.\n",
        "\n",
        ":::\n",
        "\n",
        "## Medical tests\n",
        "\n",
        "A medical test is being used to screen for a rare disease that affects 1% of the population. The test has the following characteristics:\n",
        "\n",
        "- If a person has the disease, the test will correctly identify them as positive 95% of the time (sensitivity).\n",
        "- If a person does not have the disease, the test will correctly identify them as negative 98% of the time (specificity).\n",
        "\n",
        "A random person from the population takes the test and receives a positive result.\n",
        "\n",
        "[**Questions**]{.h4}\n",
        "\n",
        "1. What is the probability that this person actually has the disease?\n",
        "2. If the person takes the test a second time and again receives a positive result, what is the updated probability that they have the disease? Assume that test results are independent for the same person.\n",
        "3. Would you recommend that this person undergo treatment based on these test results? Why or why not? Consider the reliability of the diagnosis given the test results.\n",
        "\n",
        ":::{.callout-tip collapse=\"true\"}\n",
        "#### Hints\n",
        "\n",
        "- Use Bayes' Theorem to solve this problem:\n",
        "  $$P(Disease|Positive) = \\frac{P(Positive|Disease) \\cdot P(Disease)}{P(Positive)}$$\n",
        "\n",
        "- For the denominator, remember to use the law of total probability:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(Positive) &= P(Positive|Disease) \\cdot P(Disease) \\\\\n",
        "& + P(Positive|No Disease) \\cdot P(No Disease)\n",
        "\\end{flalign}\n",
        "$$\n",
        "  \n",
        "\n",
        "- For question 2, use your answer from question 1 as the new prior probability.\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "[**Question 1: probability of disease given a positive test**]{.h4}\n",
        "\n",
        "Let's define our notation:\n",
        "\n",
        "- D = person has the disease\n",
        "- ¬D = person does not have the disease\n",
        "- `+` = positive test result\n",
        "- `-` = negative test result\n",
        "\n",
        "Given information:\n",
        "\n",
        "- P(D) = 0.01 (prior probability of disease)\n",
        "- P(¬D) = 0.99 (prior probability of no disease)\n",
        "- P(+|D) = 0.95 (test sensitivity)\n",
        "- P(-|¬D) = 0.98 (test specificity)\n",
        "- P(+|¬D) = 1 - P(-|¬D) = 0.02 (false positive rate)\n",
        "\n",
        "Using Bayes' Theorem:\n",
        "\n",
        "$$P(D|+) = \\frac{P(+|D) \\cdot P(D)}{P(+)}$$\n",
        "\n",
        "The denominator P(+) can be calculated using the law of total probability:\n",
        "\n",
        "$$P(+) = P(+|D) \\cdot P(D) + P(+|¬D) \\cdot P(¬D)$$\n",
        "$$P(+) = 0.95 \\times 0.01 + 0.02 \\times 0.99$$\n",
        "$$P(+) = 0.0095 + 0.0198$$\n",
        "$$P(+) = 0.0293$$\n",
        "\n",
        "Now we can calculate P(D|+):\n",
        "\n",
        "$$P(D|+) = \\frac{0.95 \\times 0.01}{0.0293}$$\n",
        "$$P(D|+) = \\frac{0.0095}{0.0293}$$\n",
        "$$P(D|+) = 0.324 \\text{ or } 32.4\\%$$\n",
        "\n",
        "Therefore, despite receiving a positive test result, the probability that the person actually has the disease is only about 32.4%. This is a classic example of the \"base rate fallacy\" - even with a highly accurate test, when testing for a rare condition, many positive results will still be false positives.\n",
        "\n",
        "[**Question 2: probability after a second positive test**]{.h4}\n",
        "\n",
        "For the second test, we use our updated probability as the new prior:\n",
        "\n",
        "- P(D) = 0.324 (updated prior probability of disease after first positive test)\n",
        "- P(¬D) = 0.676 (updated prior probability of no disease)\n",
        "\n",
        "Using Bayes' Theorem again:\n",
        "\n",
        "$$P(D|+_2) = \\frac{P(+_2|D) \\cdot P(D)}{P(+_2)}$$\n",
        "\n",
        "Where P(+₂) is calculated using:\n",
        "\n",
        "$$P(+_2) = P(+_2|D) \\cdot P(D) + P(+_2|¬D) \\cdot P(¬D)$$\n",
        "$$P(+_2) = 0.95 \\times 0.324 + 0.02 \\times 0.676$$\n",
        "$$P(+_2) = 0.3078 + 0.01352$$\n",
        "$$P(+_2) = 0.32132$$\n",
        "\n",
        "Now we can calculate P(D|+₂):\n",
        "\n",
        "$$P(D|+_2) = \\frac{0.95 \\times 0.324}{0.32132}$$\n",
        "$$P(D|+_2) = \\frac{0.3078}{0.32132}$$\n",
        "$P(D|+_2) = 0.958 \\text{ or } 95.8\\%$\n",
        "\n",
        "After the second positive test, the probability that the person has the disease increases dramatically to about 95.8%.\n",
        "\n",
        "[**Question 3: treatment recommendation**]{.h4}\n",
        "\n",
        "Based on the test results:\n",
        "\n",
        "- After one positive test: 32.4% probability of disease\n",
        "- After two positive tests: 95.8% probability of disease\n",
        "\n",
        "Treatment recommendation considerations:\n",
        "\n",
        "After just one positive test, there's only a 32.4% chance the person actually has the disease, meaning there's a 67.6% chance they don't have the disease. This is not reliable enough for most medical interventions, especially if the treatment has significant side effects or risks.\n",
        "\n",
        "However, after two consecutive positive tests, the probability increases to 95.8%, which provides much stronger evidence that the person has the disease. At this point, treatment would generally be recommended in most medical contexts.\n",
        "\n",
        "Factors that might influence this recommendation:\n",
        "\n",
        "1. The severity of the disease and consequences of not treating it\n",
        "2. The risks, side effects, and costs of the treatment\n",
        "3. The availability of additional confirmatory tests with higher specificity\n",
        "\n",
        "This example demonstrates why doctors often order multiple tests before beginning treatment for serious conditions - a single test result can be misleading, but multiple independent confirmations greatly increase diagnostic certainty.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Conditional independence\n",
        "\n",
        "Show that the statement of conditional independence\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(A,B|C)=P(A|C)P(B|C) &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "is equivalent to each of the statements\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(A|B,C)=P(A|C) \\quad \\textrm{and} \\quad P(B|A,C)=P(B|C) &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        ":::{.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "The key to this exercise is rigorous and frequent application of the definition of conditional\n",
        "probability, $P(X|Y) = \\frac{P(X,Y)}{P(Y)}$.\n",
        "\n",
        "The original statement that we are given is:\n",
        "\n",
        "$P(A,B|C) = P(A|C)P(B|C)$\n",
        "\n",
        "We start by applying the definition of conditional probability to two of the terms in this\n",
        "statement:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(A,B|C) = \\frac{P(A,B,C)}{P(C)} \\quad \\textrm{and} \\quad P(B|C) = \\frac{P(B,C)}{P(C)} &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "Now we substitute the right-hand side of these definitions for the left-hand sides in the original\n",
        "statement to get:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "\\frac{P(A,B,C)}{P(C)} = P(A|C) \\frac{P(B,C)}{P(C)} &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "Now we need the definition of conditional probability once more: \n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(X|Y) &= \\frac{P(X,Y)}{P(Y)} \\\\\n",
        "= P(X,Y) &= P(X|Y)P(Y) \\\\\n",
        "\\text{where } X = A \\text{ and } Y &= B,C \\\\\n",
        "P(A,B,C) &= P(A|B,C)P(B,C) \\\\\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "\n",
        "We substitute this right-hand side for P(A,B,C) to get:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "\\frac{P(A|B,C)P(B,C)}{P(C)} = P(A|C) \\frac{P(B,C)}{P(C)} &&\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "Finally, we cancel the $P(B,C)$ and $P(C)$s to get:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(A|B,C) = P(A|C)\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "The second part of the exercise follows from by a similar derivation, or by noticing that $A$ and $B$ are interchangeable in the original statement (because multiplication is commutative and $A,B$ means the same as $B,A$).\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Pacman\n",
        "\n",
        "Pacman has developed a hobby of fishing. Over the years, he has learned that a day can be considered fit or unfit for fishing $Y$ which results in three features: whether or not Ms. Pacman can show up $M$, the  temperature of the day $T$, and how high the water level is $W$. Pacman models it as an Naive \n",
        "Bayes classification problem.\n",
        "\n",
        "We wish to calculate the probability a day is fit for fishing given features of the  day. Consider the conditional probability tables that Pacman has estimated over the years:\n",
        "\n",
        "\n",
        "| $Y$   | $P(Y)$ |\n",
        "|-------|--------|\n",
        "| yes   | 0.1    |\n",
        "| no.   | 0.9    |\n",
        "\n",
        ":::columns\n",
        ":::column\n",
        "| $M$ | $Y$ | $P(M|Y)$ |\n",
        "|-----|-----|-----------|\n",
        "| yes | yes | 0.5       |\n",
        "| no  | yes | 0.5       |\n",
        "| yes | no  | 0.2       |\n",
        "| no  | no  | 0.8       |\n",
        ":::\n",
        ":::column\n",
        "| $W$  | $Y$ | $P(W|Y)$ |\n",
        "|------|-----|-----------|\n",
        "| high | yes | 0.1       |\n",
        "| low  | yes | 0.9       |\n",
        "| high | no  | 0.5       |\n",
        "| low  | no  | 0.5       |\n",
        ":::\n",
        ":::\n",
        "\n",
        "| $T$  | $Y$ | $P(T|Y)$ |\n",
        "|------|-----|-----------|\n",
        "| cold | yes | 0.2       |\n",
        "| warm | yes | 0.3       |\n",
        "| hot  | yes | 0.5       |\n",
        "| cold | no  | 0.5       |\n",
        "| warm | no  | 0.2       |\n",
        "| hot  | no  | 0.3       |\n",
        "\n",
        "Determine if a day is fit for fishing given the following conditions:\n",
        "\n",
        "- Ms. Pacman is available: M = yes\n",
        "- The weather is cold: T = cold\n",
        "- The water level is high: W = high\n",
        "\n",
        ":::{.callout-tip collapse=\"true\"}\n",
        "#### Hint\n",
        "\n",
        "Calculate $P(Y=yes|M=yes, T=cold, W=high)$ and $P(Y=no|M=yes, T=cold, W=high)$, then choose the class with the higher probability.\n",
        "\n",
        ":::\n",
        "\n",
        ":::column-page-right\n",
        ":::{.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "We need to find:\n",
        "$$P(Y=yes | M=yes, T=cold, W=high)$$\n",
        "\n",
        "By Bayes' theorem:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "&P(Y=yes | M=yes, T=cold, W=high)\\\\\n",
        "&= \\frac{P(M=yes, T=cold, W=high | Y=yes) \\cdot P(Y=yes)}{P(M=yes, T=cold, W=high)}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Using the Naive Bayes assumption of conditional independence:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "&P(M=yes, T=cold, W=high | Y=yes) \\\\\n",
        "&= P(Y=yes) \\cdot P(M=yes | Y=yes) \\cdot P(T=cold | Y=yes) \\cdot P(W=high | Y=yes)\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "For $Y=yes$:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "&P(M=yes, T=cold, W=high | Y=yes) \\\\\n",
        "&= P(Y=yes) \\cdot P(M=yes | Y=yes) \\cdot P(T=cold | Y=yes) \\cdot P(W=high | Y=yes)\\\\\n",
        "&= 0.1 \\cdot 0.5 \\cdot 0.2 \\cdot 0.1 = 0.001\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "For $Y=no$:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "&P(M=yes, T=cold, W=high | Y=no) \\\\\n",
        "&= P(Y=no) \\cdot P(M=yes | Y=no) \\cdot P(T=cold | Y=no) \\cdot P(W=high | Y=no)\\\\\n",
        "&= 0.9 \\cdot 0.2 \\cdot 0.5 \\cdot 0.5 = 0.045\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "$$\n",
        "$$$$\n",
        "\n",
        "[Normalizing to get final probabilities]{.h4}\n",
        "\n",
        "Total probability = 0.001 + 0.045 = 0.046\n",
        "\n",
        "Therefore:\n",
        "$$P(Y=yes | M=yes, T=cold, W=high) = \\frac{0.001}{0.046} \\approx 0.0217 \\approx 2.17\\%$$\n",
        "$$P(Y=no | M=yes, T=cold, W=high) = \\frac{0.045}{0.046} \\approx 0.9783 \\approx 97.83\\%$$\n",
        "\n",
        "The probability that the day is fit for fishing given that Ms. Pacman is available, the temperature is cold, and the water level is high is approximately 2.17%.\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "# Literature\n",
        "::: {#refs}\n",
        ":::"
      ],
      "id": "a4a209ac"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/awe/Library/Python/3.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}