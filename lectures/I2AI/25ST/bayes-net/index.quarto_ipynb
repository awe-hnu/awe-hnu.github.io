{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Bayesian Networks\"\n",
        "subtitle: \"Introduction to AI (I2AI)\"\n",
        "lang: en\n",
        "categories: [\"Lecture Notes\"]\n",
        "jupyter: python3\n",
        "\n",
        "bibliography: ../assets/literature.bib\n",
        "\n",
        "date: \"04.14.2025\"\n",
        "\n",
        "title-slide-attributes:\n",
        "  data-background-image: ../assets/bg.jpeg\n",
        "  data-background-size: cover\n",
        "  data-background-opacity: \"1\"\n",
        "  data-background-color: '#0333ff'\n",
        "\n",
        "format: \n",
        "  html:\n",
        "    output-file: index.html\n",
        "    format-links: false        \n",
        "---\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "## Knowledge-based agents\n",
        "\n",
        "As discussed in the section [Knowledge & Inference](../knowledge/), knowledge-based agents combine two essential components:\n",
        "\n",
        "- A **knowledge base** (KB) that stores information about the environment in a formal, structured representation\n",
        "- An **inference engine** that applies logical rules to answer questions and derive new insights\n",
        "\n",
        "When knowledge-based agents interact with their environment, they\n",
        "\n",
        "- process new observations to update their knowledge,\n",
        "- answer queries based on what they know, and\n",
        "- make decisions by applying reasoning to their knowledge base\n",
        "\n",
        "![A simplified illustration of a knowledge-based agent](images/knowledgeBasedAgents.svg){#fig-kba}\n",
        "\n",
        "## Traditional knowledge-based agents\n",
        "\n",
        "Traditional knowledge-based agents use logical frameworks like propositional or first-order logic. While powerful in certain domains (such as medical diagnosis), they face significant challenges:\n",
        "\n",
        "* **Binary reasoning only** — they can only represent *True* or *False* statements, with no ability to handle uncertainty or probability\n",
        "* **Complexity barriers** — as knowledge bases grow, both representation and reasoning become increasingly difficult\n",
        "* **Limited expressiveness** — more formal languages often sacrifice the ability to represent nuanced knowledge\n",
        "\n",
        "## From logic to probability\n",
        "\n",
        "The limitations of traditional knowledge-based systems highlight a fundamental challenge: real-world reasoning rarely operates in absolutes of true and false. Instead, our knowledge is often incomplete, uncertain, or subject to change as new evidence emerges.\n",
        "\n",
        "By incorporating probability theory, we can represent knowledge in a structured form while replacing rigid logical rules with probabilistic relationships.\n",
        "\n",
        "The core of probabilistic reasoning is the **joint probability distribution** (JPD), which specifies the probability of every possible combination of values for all variables in our domain. For $N$ variables, this distribution represents all possible combinations of values and their probabilities:\n",
        "\n",
        "$$\n",
        "P(X_1=x_{i_1},X_2=x_{i_2}, \\ldots, X_N=x_{i_N}) \\quad \\text{or simply} \\quad P(x_{i_1},x_{i_2}, \\ldots, x_{i_N})\n",
        "$$\n",
        "\n",
        "With a complete JPD, an agent can answer two fundamental types of questions:\n",
        "\n",
        "1. **Joint probability queries**:\\\n",
        "  What's the likelihood of multiple events occurring together?\\\n",
        "  $P(X_1=a,X_2=b)$\n",
        "\n",
        "2. **Conditional probability queries**:\\\n",
        "  Given that one event has occurred, what's the probability of another?\\\n",
        "   $P(X_1=a \\mid X_2=b)$\n",
        "\n",
        "The power of a complete JPD is that it contains all information needed to answer any probabilistic query about the modeled variables. \n",
        "\n",
        "### Example: Pacman\n",
        "\n",
        "Consider a Pacman game with three key events: \n",
        "\n",
        "- encountering a ghost ($G$), \n",
        "- eating a power pellet ($P$), and \n",
        "- successfully completing the level ($L$). \n",
        "\n",
        "We've analyzed game data to create the following joint probability distribution (JPD):\n",
        "\n",
        "| Ghost ($G$) | Power pellet ($P$) | Level complete ($L$) | Probability |\n",
        "|-------------|--------------------|----------------------|-------------|\n",
        "| True        | True               | True                 | 0.15        |\n",
        "| True        | True               | False                | 0.05        |\n",
        "| True        | False              | True                 | 0.05        |\n",
        "| True        | False              | False                | 0.25        |\n",
        "| False       | True               | True                 | 0.20        |\n",
        "| False       | True               | False                | 0.10        |\n",
        "| False       | False              | True                 | 0.10        |\n",
        "| False       | False              | False                | 0.10        |\n",
        "\n",
        ": Joint probability distribution of the Pacman scenarios {#tbl-pacman}\n",
        "\n",
        "\n",
        "This knowledge-base tells us, for example:\n",
        "\n",
        "* The probability that a player encounters a ghost, eats a power pellet, and completes the level is $P(G,P,L)=0.15$\n",
        "* The probability that a player encounters no ghost, no power pellet, and fails to complete the level is $P(\\neg G, \\neg P, \\neg L) = 0.10$\n",
        "\n",
        "With this complete JPD, we can answer questions like:\n",
        "\n",
        "- What's the probability of encountering a ghost during gameplay? $P(G)$?\n",
        "- What's the probability of both encountering a ghost and eating a power pellet? $P(G,P)$?\n",
        "- If a player encounters a ghost, what's the probability they'll complete the level? $P(L \\mid G)$?\n",
        "- If a player eats a power pellet, what's the probability they'll encounter a ghost? $P(G \\mid P)$?\n",
        "- If a player encounters a ghost and eats a power pellet, what's the probability they'll complete the level? $P(L \\mid G, P)$?\n",
        "\n",
        "## The scalability problem\n",
        "\n",
        "While a complete JPD is theoretically powerful, it quickly becomes impractical. With $d$ variables that each take $n$ values, the JPD requires storing $n^d$ probabilities. For example, with just 10 binary variables, we need to store 1,024 values; with 20 variables, over a million!\n",
        "\n",
        "**Bayesian networks** address these limitations by exploiting independence relationships between variables to create a more compact representation.\n",
        "\n",
        "# Bayesian Networks\n",
        "\n",
        "By the use of **Bayesian networks**, complexity can be dramatically reduced by exploiting:\n",
        "\n",
        "- Variable **independence**: When variables don't influence each other\n",
        "- **Conditional independence**: When variables are unrelated once we know the value of other variables\n",
        "\n",
        ":::callout-note\n",
        "## Unconditional indendence and conditional independence\n",
        "\n",
        "[**Unconditional indendence**]{.h4}\n",
        "\n",
        "When two variables are independent, knowing the value of one tells you nothing about the value of the other. Mathematically, we say A and B are independent if\n",
        "$P(A,B) = P(A) × P(B)$.\n",
        "\n",
        "The terms unconditional independence, variable independence, and marginal independence describe the same concept: two variables are independent without conditioning on any other variables, meaning the probability distribution of one variable is not affected by the value of the other.\n",
        "\n",
        "For example, in the [Pacman example](#pacmanexample), if encountering a ghost were completely independent of eating a power pellet, then knowing a player ate a power pellet wouldn't change our estimate of whether they'll encounter a ghost.\n",
        "\n",
        "[**Conditional independence**]{.h4}\n",
        "\n",
        "This is more subtle and powerful. Variables A and B are conditionally independent given C if, once you know C, learning A gives you no additional information about B. \n",
        "\n",
        "Mathematically:\\\n",
        "$P(A,B \\mid C) = P(A \\mid C) \\cdot P(B \\mid C)$\n",
        "\n",
        "Or equivalently:\\\n",
        "$P(A \\mid C,B) = P(A \\mid C) \\quad \\text{and} \\quad P(B \\mid C,A) = P(B \\mid C)$\n",
        "\n",
        "For example, imagine that in Pacman:\n",
        "\n",
        "- Ghost encounters ($G$) influence level completion ($L$)\n",
        "- Power pellets ($P$) influence level completion ($L$)\n",
        "- But once we know whether the level was completed ($L$), knowing about ghost encounters ($G$) tells us nothing new about power pellets ($P$)\n",
        "\n",
        "In this case, $G$ and $P$ would be conditionally independent given $L$ (see [fork structure](#path-blocking-rules).\n",
        "\n",
        ":::\n",
        "\n",
        "## Reasoning\n",
        "\n",
        "Bayesian networks excel at two types of reasoning:\n",
        "\n",
        "- **Causal reasoning** (predicting effects):\\\n",
        "  $P(\\text{effects} \\mid \\text{causes})$\\\n",
        "  Example: \"Given a patient's symptoms, what disease do they likely have?\"\n",
        "- **Diagnostic reasoning** (inferring causes):\\\n",
        "  $P(\\text{causes} \\mid \\text{evidence})$\\\n",
        "  Example: \"Given a patient has disease X, what symptoms might they develop?\"\n",
        "\n",
        "# Structure\n",
        "\n",
        "Bayesian networks exploit these independencies by organizing variables in a directed acyclic graphs structure where:\n",
        "\n",
        "- Each node only depends directly on its parent nodes\n",
        "- Nodes are conditionally independent of their non-descendants, given their parents\n",
        "\n",
        "\n",
        "This structure dramatically reduces the number of probabilities that need to stored. Instead of storing the full JPD with $2^n$ values (where $n$ is the number of variables), we only need to store conditional probabilities for each node given its parents.\n",
        "\n",
        "For instance, with 20 binary variables, a full JPD would require $2^{20}$ ≈ 1 million probabilities, but a Bayesian network might need only a few thousand, making probabilistic reasoning computationally feasible.\n",
        "\n",
        "Bayesian networks are directed acyclic graphs where:\n",
        "\n",
        "- **Nodes** represent random variables (i.e., events or propositions about the world)\n",
        "- **Edges** represent direct causal or influential relationships (i.e., they show which variables directly influence each other)\n",
        "- Each node contains a **probability distribution** conditioned on its parent nodes (i.e., a **conditional probability table** (CPT) showing how likely its values are, based on its parent nodes' values; $P(X \\mid parents(X))$)\n",
        "\n",
        "\n",
        ":::callout-note\n",
        "#### Directed acyclic graphs\n",
        "\n",
        "Directed acyclic graphs (DAGs) are mathematical structures with:\n",
        "\n",
        "- Nodes (vertices) connected by directed edges (arrows)\n",
        "- No cycles or loops (you cannot follow arrows and return to a starting node)\n",
        "- A partial ordering of nodes (some nodes come \"before\" others)\n",
        "\n",
        "DAGs are essential in Bayesian networks because they encode conditional independence relationships and support efficient probabilistic inference algorithms.\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "%%| fig-cap: \"Example of a Directed Acyclic Graph (DAG)\"\n",
        "graph TD\n",
        "    A[Disease] --> B[Symptom 1]\n",
        "    A --> C[Symptom 2]\n",
        "    D[Risk Factor] --> A\n",
        "    D --> E[Comorbidity]\n",
        "    E --> C\n",
        "```\n",
        "\n",
        "\n",
        "This DAG example shows how a risk factor might influence a disease, which causes symptoms, while a comorbidity influenced by the same risk factor affects one of the symptoms.\n",
        "\n",
        ":::small\n",
        "In contrast, knowledge graphs:\n",
        "\n",
        "- allow cycles (bidirectional relationships),\n",
        "- use labeled edges to represent different types of relationships,\n",
        "_ primarily represent factual knowledge rather than probabilistic dependencies,\n",
        "- have no acyclicity constraint, and\n",
        "- focus on representing semantic connections between entities.\n",
        "\n",
        "While both structures organize information as nodes and edges, they serve different purposes: DAGs for probabilistic reasoning and knowledge graphs for representing interconnected factual knowledge.\n",
        ":::\n",
        ":::\n",
        "\n",
        "## D-Separation\n",
        "\n",
        "To systematically determine conditional independence relationships in Bayesian networks, we use the concept of **d-separation** (directional separation). D-separation provides formal criteria for identifying whether variables are conditionally independent given a set of evidence variables.\n",
        "\n",
        "Two variables $A$ and $B$ are d-separated given a set of evidence variables $E$ if all paths between $A$ and $B$ are \"blocked\" by variables in $E$. When variables are d-separated, they are conditionally independent.\n",
        "\n",
        "In the context of d-separation, a path being \"blocked\" means that information cannot flow along that path between the variables, resulting in conditional independence. When all paths between two variables are blocked, those variables are conditionally independent.\n",
        "\n",
        "Think of a Bayesian network as a system of information channels. A path is \"open\" if it allows probabilistic information to flow from one variable to another. A path is \"blocked\" if something prevents this information flow, making the connected variables conditionally independent.\n",
        "\n",
        "### Path blocking rules\n",
        "\n",
        "Whether a path is blocked depends on both the structure of the path and which variables we already know (our evidence variables $E$):\n",
        "\n",
        "[**Chain structure**]{.h4} \n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "graph LR\n",
        "  A-->C\n",
        "  C-->B\n",
        "```\n",
        "\n",
        "\n",
        "- The path is blocked when $C$ is observed (when $C$ is in our evidence set $E$)\n",
        "- **Result**: $A$ and $B$ are conditionally independent given $C$\n",
        "- **Formally**: $P(B|A,C) = P(B|C)$ and $P(A|B,C) = P(A|C)$\n",
        "- **Example**: If \"Rain\" causes \"Wet Ground\" which causes \"Slippery Road\", then once we know the ground is wet, learning about rain gives us no additional information about road slipperiness\n",
        "- **Intuition**: Once we know the middle variable's value, the first variable can't tell us anything new about the third variable\n",
        "\n",
        "[**Fork structure**]{.h4}\n",
        "\n",
        "$A \\leftarrow C \\rightarrow B$\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "graph LR\n",
        "  C-->A\n",
        "  C-->B\n",
        "```\n",
        "\n",
        "\n",
        "- The path is blocked when $C$ is observed (when $C$ is in our evidence set $E$)\n",
        "- **Result**: $A$ and $B$ are conditionally independent given $C$\n",
        "- **Formally**: $P(A|B,C) = P(A|C)$ and $P(B|A,C) = P(B|C)$\n",
        "- **Example**: If \"Season\" influences both \"Temperature\" and \"Daylight Hours\", then once we know the season, learning about temperature gives us no additional information about daylight hours\n",
        "- **Intuition**: When we know the common cause, its effects become independent of each other\n",
        "\n",
        "[**Collider structure** or v-structure]{.h4}\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "graph LR\n",
        "  A-->C\n",
        "  B-->C\n",
        "```\n",
        "\n",
        "\n",
        "- The path is blocked when $C$ and all its descendants are NOT observed\n",
        "- **Result**: $A$ and $B$ are marginally independent (unconditionally independent)\n",
        "- **Formally**: $P(A|B) = P(A)$ and $P(B|A) = P(B)$\n",
        "\n",
        "- The path becomes unblocked when $C$ OR any descendant of $C$ IS observed\n",
        "- **Result**: $A$ and $B$ become conditionally dependent given $C$\n",
        "- **Formally**: $P(A|B,C) \\neq P(A|C)$ and $P(B|A,C) \\neq P(B|C)$\n",
        "- **Example**: If both \"Flu\" and \"Allergies\" can cause \"Sneezing\", then initially flu and allergies are independent. However, if we observe sneezing, knowing someone doesn't have allergies increases the probability they have the flu\n",
        "- **Intuition**: Independent causes become dependent when we observe their common effect\n",
        "\n",
        "Understanding d-separation allows us to identify the conditional independence assumptions encoded in a Bayesian network structure, which is crucial for both building appropriate networks and performing efficient inference.\n",
        "\n",
        "# Example: Pacman {#pacmanexample}\n",
        "\n",
        "Let's model a Pacman video game scenario where we want to predict when the player will encounter ghosts and be defeated.\n",
        "\n",
        "Our Bayesian network will model:\n",
        "\n",
        "- **Player Skill** ($S$): We classify players into three skill levels:\n",
        "  - $25\\%$ of players are beginners ($b$)\n",
        "  - $45\\%$ of players are intermediate ($i$)\n",
        "  - $30\\%$ of players are advanced ($a$)\n",
        "- **Level Type** ($L$): \n",
        "  - $40\\%$ of levels are maze-type ($m$)\n",
        "  - $60\\%$ of levels are open-area type ($o$)\n",
        "- **Ghost Activity** ($G$): There are high activity ($h$) and low activity ($l$) levels:\n",
        "  - For maze levels:\n",
        "    - Beginners face high ghost activity with probability $0.8$\n",
        "    - Intermediate players face high ghost activity with probability $0.5$\n",
        "    - Advanced players face high ghost activity with probability $0.3$\n",
        "  - For open-area levels:\n",
        "    - Beginners face high ghost activity with probability $0.6$\n",
        "    - Intermediate players face high ghost activity with probability $0.4$\n",
        "    - Advanced players face high ghost activity with probability $0.2$\n",
        "- **Power Pellet Collected** ($P$):\n",
        "  - With high ghost activity, the probability of collecting a power pellet is $0.7$\n",
        "  - With low ghost activity, the probability of collecting a power pellet is $0.4$\n",
        "- **Player Defeated** ($D$):\n",
        "  - With high ghost activity and no power pellet, defeat probability is $0.9$\n",
        "  - With high ghost activity and a power pellet, defeat probability is $0.3$\n",
        "  - With low ghost activity and no power pellet, defeat probability is $0.4$\n",
        "  - With low ghost activity and a power pellet, defeat probability is $0.1$\n",
        "\n",
        "The Bayesian Network for this Pacman scenario is:\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "graph TD\n",
        "    S[Player Skill] --> G[Ghost Activity]\n",
        "    L[Level Type] --> G\n",
        "    G --> P[Power Pellet Collected]\n",
        "    G --> D[Player Defeated]\n",
        "    P --> D\n",
        "```\n",
        "\n",
        "\n",
        "In this network:\n",
        "\n",
        "* **Nodes** represent our 5 key variables (Skill, Level Type, Ghost Activity, Power Pellet, Defeated)\n",
        "* **Edges** connect variables that directly influence each other (e.g., Player Skill affects Ghost Activity)\n",
        "* **Missing edges** indicate conditional independence (e.g., Level Type and Player Skill don't directly affect whether a Power Pellet is collected - this influence flows through Ghost Activity)\n",
        "* Each node has an associated **conditional probability table (CPT)** showing how likely each value is given its parent nodes' values\n",
        "\n",
        "\n",
        "### Example in the Pacman Network\n",
        "\n",
        "In our Pacman network:\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "flowchart TD\n",
        "    S[Player Skill] --> G[Ghost Activity]\n",
        "    L[Level Type] --> G\n",
        "    G --> P[Power Pellet Collected]\n",
        "    G --> D[Player Defeated]\n",
        "    P --> D\n",
        "```\n",
        "\n",
        "\n",
        "We can identify the following d-separation relationships:\n",
        "\n",
        "- $S$ and $L$ are d-separated (marginally independent) because their only connecting path is through a collider at $G$\n",
        "  - **This means**: $P(S|L) = P(S)$ and $P(L|S) = P(L)$\n",
        "\n",
        "- $S$ and $P$ are d-separated given $G$ (conditionally independent given $G$) because knowing $G$ blocks the chain $S \\rightarrow G \\rightarrow P$\n",
        "  - **This means**: $P(S|P,G) = P(S|G)$ and $P(P|S,G) = P(P|G)$\n",
        "\n",
        "- $S$ and $D$ are d-separated given $G$ and $P$ (conditionally independent given both $G$ and $P$) because all paths between them are blocked\n",
        "  - **This means**: $P(S|D,G,P) = P(S|G,P)$ and $P(D|S,G,P) = P(D|G,P)$\n",
        "\n",
        "- $P$ and $L$ are d-separated given $G$ (conditionally independent given $G$) because knowing $G$ blocks the fork $P \\leftarrow G \\leftarrow L$\n",
        "  - **This means**: $P(P|L,G) = P(P|G)$ and $P(L|P,G) = P(L|G)$\n",
        "\n",
        "However, note that:\n",
        "\n",
        "- $S$ and $L$ are not d-separated given $G$ (they become dependent when conditioning on their common effect)\n",
        "  - **This means**: $P(S|L,G) \\neq P(S|G)$ and $P(L|S,G) \\neq P(L|G)$\n",
        "\n",
        "- $G$ and $D$ are not d-separated given $P$ (they have a direct link)\n",
        "  - **This means**: $P(G|D,P) \\neq P(G|P)$ and $P(D|G,P) \\neq P(D|P)$\n",
        "\n",
        "# Building a Bayes net\n",
        "\n",
        "To construct a Bayesian network that accurately models a domain:\n",
        "\n",
        "1. **Identify root nodes**:\\\n",
        "  Start with variables that don't depend on any others (no parents).\\\n",
        "  [In our Pacman example, these are Player Skill ($S$) and Level Type ($L$).]{.small}\n",
        "\n",
        "2. **Add dependent variables in layers**:\\\n",
        "  Connect variables to their direct influences.\\\n",
        "  [Ghost Activity ($G$) depends on both Player Skill and Level Type, so it forms the second layer.]{.small}\n",
        "\n",
        "3. **Continue by causal relationship**:\\\n",
        "  Add variables that depend on the previous layer.\\\n",
        "  [Power Pellet ($P$) and part of the Player Defeated ($D$) probability depend on Ghost Activity.]{.small}\n",
        "\n",
        "4. **Complete all dependencies**:\\\n",
        "  Ensure all influence relationships are captured.\\\n",
        "  [Player Defeated ($D$) depends on both Ghost Activity and Power Pellet status.]{.small}\n",
        "\n",
        "# The Markov blanket\n",
        "\n",
        "The **Markov blanket** of a node is the minimal set of variables that shield it from the rest of the network. For any node $X$, once you know its Markov blanket, no other variable provides additional information about $X$.\n",
        "\n",
        "A node's Markov blanket consists of:\n",
        "* Its parents\n",
        "* Its children\n",
        "* The parents of its children (other than itself)\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "graph TD\n",
        "    subgraph \"Markov Blanket of X\"\n",
        "        P1[Parent 1] --> X\n",
        "        P2[Parent 2] --> X\n",
        "        X --> C1[Child 1]\n",
        "        X --> C2[Child 2]\n",
        "        CP[Co-Parent] --> C2\n",
        "    end\n",
        "    Other1[Other Variable] --- Other2[Other Variable]\n",
        "```\n",
        "\n",
        "\n",
        "This property allows us to perform **localized reasoning**, making Bayesian networks computationally efficient.\n",
        "\n",
        "# Inference in Bayes nets\n",
        "\n",
        "With a complete Bayesian network, we can calculate three types of probabilities:\n",
        "\n",
        "1. **Marginal probabilities** for single variables:\\\n",
        "  $P(X=x)$\n",
        "2. **Joint probabilities** for multiple variables:\\\n",
        "  $P(X=x,Y=y)$\n",
        "3. **Conditional probabilities** between variables:\\\n",
        "  $P(Y=y \\mid X=x)$\n",
        "\n",
        "### Calculating marginal probabilities\n",
        "\n",
        "#### For root nodes\n",
        "\n",
        "For variables at the top of the network (like Player Skill in our Pacman example), we can directly read their probabilities from their CPT:\n",
        "\n",
        "$$P(S=\\text{advanced}) = 0.30$$\n",
        "\n",
        "#### For other nodes\n",
        "\n",
        "For variables with parents, we must use **marginalization** — summing over all possible parent values. For example, to find the probability of high Ghost Activity:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "& P(G=h) \\\\\n",
        "&= \\sum_{s \\in \\{b,i,a\\}} \\sum_{l \\in \\{m,o\\}} P(G=h \\mid S=s, L=l) \\cdot P(S=s) \\cdot P(L=l)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        ":::small\n",
        "In plain language: To find the overall probability of high ghost activity, consider every possible combination of player skill and level type, multiply the probability of that combination by the probability of high ghost activity given that combination, then sum all these products.\n",
        ":::\n",
        "\n",
        "### Calculating joint probabilities\n",
        "To calculate the joint probability of multiple variables, we use the **chain rule of probability** applied to the structure of the Bayesian network.\n",
        "\n",
        "For any Bayesian network with variables $X_1, X_2, ..., X_n$, the joint probability is:\n",
        "\n",
        "$$P(X_1=x_1, X_2=x_2, ..., X_n=x_n) = \\prod_{i=1}^{n} P(X_i=x_i \\mid \\text{Parents}(X_i))$$\n",
        "\n",
        ":::small\n",
        "In our Pacman example, to calculate the probability of advanced player skill, original level, and high ghost activity:\n",
        "\n",
        "$$P(S=a, L=o, G=h) = P(S=a) \\cdot P(L=o) \\cdot P(G=h \\mid S=a, L=o)$$\n",
        "\n",
        "If $P(S=a) = 0.30$, $P(L=o) = 0.40$, and $P(G=h \\mid S=a, L=o) = 0.70$ \n",
        "\n",
        "then $P(S=a, L=o, G=h) = 0.30 \\cdot 0.40 \\cdot 0.70 = 0.084$\n",
        "\n",
        "This calculation works because the Bayesian network structure encodes conditional independence assumptions, allowing us to decompose the joint probability into this product.\n",
        ":::\n",
        "\n",
        "### Calculating conditional probabilities\n",
        "\n",
        "Conditional probabilities ask questions like \"given that X has occurred, what is the probability of Y?\"\n",
        "\n",
        "#### Using Bayes' rule directly\n",
        "\n",
        "For simple queries, we can apply Bayes' rule:\n",
        "\n",
        "$$P(Y=y \\mid X=x) = \\frac{P(X=x \\mid Y=y) \\cdot P(Y=y)}{P(X=x)}$$\n",
        "\n",
        "#### Using joint probabilities\n",
        "\n",
        "More commonly in Bayesian networks, we calculate conditional probabilities using:\n",
        "\n",
        "$$P(Y=y \\mid X=x) = \\frac{P(Y=y, X=x)}{P(X=x)}$$\n",
        "\n",
        ":::small\n",
        "\n",
        "In our Pacman example we want to find the probability of player skill given ghost activity. To find $P(S=a \\mid G=h)$ (the probability the player is advanced given high ghost activity):\n",
        "\n",
        "1. Calculate the joint probability $P(S=a, G=h)$:\n",
        "   $$P(S=a, G=h) = \\sum_{l \\in \\{m,o\\}} P(S=a) \\cdot P(L=l) \\cdot P(G=h \\mid S=a, L=l)$$\n",
        "\n",
        "2. Calculate the marginal probability $P(G=h)$ using marginalization as shown earlier.\n",
        "\n",
        "3. Apply the conditional probability formula:\n",
        "   $$P(S=a \\mid G=h) = \\frac{P(S=a, G=h)}{P(G=h)}$$\n",
        "\n",
        ":::\n",
        "\n",
        "When we have evidence on multiple variables, the calculation extends naturally. \n",
        "\n",
        "For example, to find $P(S=a \\mid G=h, L=o)$:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "& P(S=a \\mid G=h, L=o) = \\frac{P(S=a, G=h, L=o)}{P(G=h, L=o)} \\\\\n",
        "& = \\frac{P(S=a) \\cdot P(L=o) \\cdot P(G=h \\mid S=a, L=o)}{P(G=h, L=o)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The denominator $P(G=h, L=o)$ can be calculated by summing over all possible values of $S$:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "& P(G=h, L=o) \\\\\n",
        "& = \\sum_{s \\in \\{b,i,a\\}} P(S=s) \\cdot P(L=o) \\cdot P(G=h \\mid S=s, L=o)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "### Law of Total Probability\n",
        "\n",
        "The **law of total probability** is a fundamental rule that allows us to calculate the probability of an event by considering all the ways that event can occur. For any variable $Y$ and another variable $X$ with possible values $x_1, x_2, ..., x_n$:\n",
        "\n",
        "$$P(Y=y) = \\sum_{i=1}^{n} P(Y=y \\mid X=x_i) \\cdot P(X=x_i)$$\n",
        "\n",
        "This law naturally emerges from the marginalization process and is particularly useful when calculating probabilities of variables that are not directly connected in a Bayesian network.\n",
        "\n",
        ":::small\n",
        "For example, to calculate $P(R=True)$ (the probability a patient requires insulin treatment) in our diabetes model, we can apply the law of total probability using HbA1c levels:\n",
        "\n",
        "$$\\begin{align}\n",
        "P(R=True) &= P(R=True \\mid B=h) \\cdot P(B=h) \\\\\n",
        "& + P(R=True \\mid B=l) \\cdot P(B=l) \\\\\n",
        "&= 0.7 \\cdot P(B=h) + 0.1 \\cdot P(B=l)\n",
        "\\end{align}$$\n",
        "\n",
        "Similarly, when calculating conditional probabilities between variables that are not directly connected, like $P(R=True \\mid T=t1)$, we can use an expanded form:\n",
        "\n",
        "$$\\begin{align}\n",
        "P(R=True \\mid T=t1) &= P(R=True \\mid B=h) \\cdot P(B=h \\mid T=t1) \\\\\n",
        "& + P(R=True \\mid B=l) \\cdot P(B=l \\mid T=t1)\n",
        "\\end{align}$$\n",
        "\n",
        "This approach explicitly accounts for how information flows through the intermediate variable $B$ in our network.\n",
        ":::\n",
        "\n",
        "\n",
        ":::small\n",
        "Note: For more complex Bayesian networks, exact inference can become computationally expensive. In such cases, we often use approximation algorithms like variable elimination[^1], belief propagation[^2], or sampling methods[^3] to perform efficient inference.\n",
        "\n",
        "[^1]: Variable elimination is an algorithm that computes exact marginal probabilities by systematically \"eliminating\" variables through summation, reducing computational complexity by exploiting conditional independence relationships.\n",
        "\n",
        "[^2]: Belief propagation (also known as message passing) works by having nodes in the network exchange \"messages\" containing probability information, enabling efficient inference even in networks with loops.\n",
        "\n",
        "[^3]: Sampling methods like Markov Chain Monte Carlo (MCMC) and Gibbs sampling approximate probability distributions by generating random samples, making them useful for very large networks where exact inference is intractable.\n",
        ":::\n",
        "\n",
        "\n",
        "# Common misconceptions\n",
        "\n",
        "When working with Bayesian networks, students often encounter several conceptual hurdles. Being aware of these common misconceptions can help you avoid errors in both understanding and application.\n",
        "\n",
        "[**Confusing causality with conditional dependence**]{.h4}\n",
        "\n",
        "- **Misconception**: Assuming that an edge $A \\rightarrow B$ in a Bayesian network means that $A$ is always the cause of $B$ in the real world.\n",
        "- **Reality**: Edges represent statistical dependencies, not necessarily causation. While Bayesian networks are often built to reflect causal relationships, the arrows formally indicate conditional probability relationships.\n",
        "- **Example**: In a medical diagnostic network, a disease node might point to a symptom node, reflecting causality. However, the same probabilistic relationship could be modeled in reverse for diagnostic reasoning.\n",
        "\n",
        "[**Misinterpreting missing edges**]{.h4}\n",
        "\n",
        "- **Misconception**: Assuming that if there's no edge between nodes $X$ and $Y$, they must be completely independent.\n",
        "- **Reality**: Nodes without direct connections might still be dependent through other paths. Only specific independence relationships are guaranteed by the network structure (as defined by d-separation).\n",
        "- **Example**: In our Pacman example, there's no direct edge from Player Skill to Power Pellet, but they are still statistically dependent because Player Skill influences Ghost Activity, which influences Power Pellet collection.\n",
        "\n",
        "[**Confusing joint and conditional probabilities**]{.h4}\n",
        "\n",
        "- **Misconception**: Mistaking $P(A, B)$ for $P(A \\mid B)$ or vice versa in calculations.\n",
        "- **Reality**: These are fundamentally different quantities. The joint probability $P(A, B)$ represents the probability of both events occurring, while the conditional probability $P(A \\mid B)$ represents the probability of $A$ occurring given that $B$ has occurred.\n",
        "- **Example**: In the Pacman network, $P(G=h, L=m)$ is the probability of high ghost activity in a maze level, while $P(G=h \\mid L=m)$ is the probability of high ghost activity given that we know we're in a maze level.\n",
        "\n",
        "[**Neglecting the chain rule factorization**]{.h4}\n",
        "\n",
        "- **Misconception**: Calculating the joint probability of all variables incorrectly, often by multiplying probabilities without respecting the network structure.\n",
        "- **Reality**: In a Bayesian network, the joint probability factorizes according to the network structure: $P(X_1, X_2, ..., X_n) = \\prod_{i=1}^{n} P(X_i \\mid \\text{Parents}(X_i))$.\n",
        "- **Example**: In our diabetes example, $P(A, T, B, R) = P(A) \\cdot P(T) \\cdot P(B \\mid A, T) \\cdot P(R \\mid B)$, not simply the product of all marginal probabilities.\n",
        "\n",
        "[**Misapplying Bayes' rule**]{.h4}\n",
        "\n",
        "- **Misconception**: Incorrectly applying Bayes' rule, especially when dealing with multiple variables.\n",
        "- **Reality**: Bayes' rule states that $P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}$. When multiple variables are involved, careful attention must be paid to which variables are being conditioned on.\n",
        "- **Example**: To find $P(S=a \\mid G=h)$ in the Pacman example, we need $P(G=h \\mid S=a) \\cdot P(S=a) / P(G=h)$, where $P(G=h)$ must be calculated by marginalizing over both $S$ and $L$.\n",
        "\n",
        "Remember that these misconceptions often arise from the complexity of probabilistic thinking rather than simple carelessness. Taking time to clearly write out the appropriate formulas and checking each step of your calculations can help avoid many of these common errors.\n",
        "\n",
        "\n",
        "# Exercises {.headline-only}\n",
        "\n",
        "## Basic probabilities\n",
        "\n",
        "Consider a simple Bayesian network with two binary variables: Cloudy ($C$) and Rain ($R$), where Cloudy is a parent of Rain. \n",
        "\n",
        "The conditional probability tables are:\n",
        "\n",
        "- $P(C=\\text{true}) = 0.3$\n",
        "- $P(R=\\text{true} \\mid C=\\text{true}) = 0.8$\n",
        "- $P(R=\\text{true} \\mid C=\\text{false}) = 0.1$\n",
        "\n",
        "What is the joint probability $P(C=\\text{true}, R=\\text{true})$?\n",
        "\n",
        ":::{.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "Using the chain rule for Bayesian networks:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(C=\\text{true}, R=\\text{true}) &= P(R=\\text{true} \\mid C=\\text{true}) \\cdot P(C=\\text{true}) \\\\\n",
        "&= 0.8 \\cdot 0.3 = 0.24\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "The probability of it being both cloudy and rainy is 0.24 or 24%.\n",
        ":::\n",
        "\n",
        "What is the marginal probability $P(R=\\text{true})$?\n",
        "\n",
        ":::{.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "To find the marginal probability, we need to marginalize over all possible values of $C$:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(R=\\text{true}) &= P(R=\\text{true} \\mid C=\\text{true}) \\cdot P(C=\\text{true}) \\\\\n",
        "& + P(R=\\text{true} \\mid C=\\text{false}) \\cdot P(C=\\text{false})\n",
        "\\end{flalign}\n",
        "$$\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(R=\\text{true}) &= 0.8 \\cdot 0.3 + 0.1 \\cdot 0.7 \\\\\n",
        "&= 0.24 + 0.07 = 0.31\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "The probability of rain, regardless of cloudiness, is 0.31 or 31%.\n",
        ":::\n",
        "\n",
        "Using the weather network from Questions 1 and 2, what is the probability that it's cloudy given that it's raining? That is, calculate $P(C=\\text{true} \\mid R=\\text{true})$.\n",
        "\n",
        "::: {.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "We use Bayes' rule:\n",
        "\n",
        "$P(C=\\text{true} \\mid R=\\text{true}) = \\frac{P(R=\\text{true} \\mid C=\\text{true}) \\cdot P(C=\\text{true})}{P(R=\\text{true})}$\n",
        "\n",
        "We already calculated:\n",
        "\n",
        "$P(R=\\text{true} \\mid C=\\text{true}) = 0.8$\n",
        "$P(C=\\text{true}) = 0.3$\n",
        "$P(R=\\text{true}) = 0.31$\n",
        "\n",
        "So:\n",
        "$P(C=\\text{true} \\mid R=\\text{true}) = \\frac{0.8 \\cdot 0.3}{0.31} = \\frac{0.24}{0.31} \\approx 0.774$\n",
        "\n",
        "There's about a 77.4% chance it's cloudy, given that it's raining.\n",
        ":::\n",
        "\n",
        "## Conditional independence\n",
        "\n",
        "Consider a Bayesian network with three binary variables: Season ($S$), Temperature ($T$), and Ice Cream Sales ($I$), structured as $S \\rightarrow T \\rightarrow I$ (Season affects Temperature, which affects Ice Cream Sales).\n",
        "\n",
        "Are Season ($S$) and Ice Cream Sales ($I$) conditionally independent given Temperature ($T$)? Explain your answer.\n",
        "\n",
        ":::{.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "Yes, Season ($S$) and Ice Cream Sales ($I$) are conditionally independent given Temperature ($T$).\n",
        "\n",
        "In a Bayesian network, a variable is conditionally independent of its non-descendants given its parents. Looking at the structure $S \\rightarrow T \\rightarrow I$, when we know the value of Temperature ($T$), the value of Season ($S$) provides no additional information about Ice Cream Sales ($I$). This is because all the influence of Season on Ice Cream Sales flows through Temperature.\n",
        "\n",
        "Mathematically, this means: $P(I \\mid T, S) = P(I \\mid T)$\n",
        "\n",
        "For example, if we know it's hot (high Temperature), then Ice Cream Sales are likely high regardless of whether it's summer or winter. The season only affects ice cream sales by affecting temperature.\n",
        ":::\n",
        "\n",
        "\n",
        "## D-Separation\n",
        "\n",
        "Consider a Bayesian network with the following structure:\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "graph LR\n",
        "    A-->C\n",
        "    B-->C\n",
        "```\n",
        "\n",
        "\n",
        "Are variables $A$ and $B$ independent? What about if we condition on $C$?\n",
        "\n",
        "::: {.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "$A$ and $B$ are marginally independent (unconditionally independent) because there's no direct path between them and no common cause. This means $P(A,B) = P(A) \\cdot P(B)$.\n",
        "\n",
        "However, $A$ and $B$ become conditionally dependent when we condition on their common effect $C$. This is known as \"explaining away\" - once we know the value of the effect $C$, knowing the value of one cause provides information about the other cause. This means $P(A \\mid B, C) \\neq P(A \\mid C)$.\n",
        "\n",
        "For example, if $A$ represents \"Sprinkler On\", $B$ represents \"Rain\", and $C$ represents \"Grass is Wet\", then Sprinkler and Rain are independent events. But if we know the grass is wet, learning the sprinkler was off increases the probability that it rained (explaining away).\n",
        ":::\n",
        "\n",
        "## Pacman\n",
        "\n",
        "Please use the given [JPD](#tbl-pacman) to answer the questions raised [here](#example-pacman).\n",
        "\n",
        "::: notes\n",
        "\n",
        ":::{.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "**What's the probability of encountering a ghost during gameplay? $P(G)$?**\n",
        "\n",
        "To find $P(G)$, we need to sum all probabilities where $G$ is True:\n",
        "\n",
        "$$P(G) = P(G,P,L) + P(G,P,\\neg L) + P(G,\\neg P,L) + P(G,\\neg P,\\neg L)$$\n",
        "$$P(G) = 0.15 + 0.05 + 0.05 + 0.25$$\n",
        "$$P(G) = 0.50$$\n",
        "\n",
        "So there's a 50% chance of encountering a ghost during gameplay.\n",
        "\n",
        "**What's the probability of both encountering a ghost and eating a power pellet? $P(G,P)$?**\n",
        "\n",
        "To find $P(G,P)$, we need to sum all probabilities where both $G$ and $P$ are True:\n",
        "\n",
        "$$P(G,P) = P(G,P,L) + P(G,P,\\neg L)$$\n",
        "$$P(G,P) = 0.15 + 0.05$$\n",
        "$$P(G,P) = 0.20$$\n",
        "\n",
        "So there's a 20% chance of both encountering a ghost and eating a power pellet.\n",
        "\n",
        "**If a player encounters a ghost, what's the probability they'll complete the level? $P(L|G)$?**\n",
        "\n",
        "This is a conditional probability, calculated as:\n",
        "\n",
        "$$P(L|G) = \\frac{P(L,G)}{P(G)}$$\n",
        "\n",
        "$$P(L,G) = P(G,P,L) + P(G,\\neg P,L) = 0.15 + 0.05 = 0.20$$\n",
        "$$P(G) = 0.50$$\n",
        "\n",
        "Therefore:\n",
        "\n",
        "$$P(L|G) = \\frac{0.20}{0.50} = 0.40$$\n",
        "\n",
        "So if a player encounters a ghost, there's a 40% chance they'll complete the level.\n",
        "\n",
        "**If a player eats a power pellet, what's the probability they'll encounter a ghost? $P(G|P)$?**\n",
        "\n",
        "This conditional probability is:\n",
        "\n",
        "$$P(G|P) = \\frac{P(G,P)}{P(P)}$$\n",
        "\n",
        "$$P(G,P) = 0.20$$\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(P) &= P(G,P,L) + P(G,P,\\neg L) + P(\\neg G,P,L) + P(\\neg G,P,\\neg L) \\\\\n",
        "&= 0.15 + 0.05 + 0.20 + 0.10 = 0.50\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "Therefore:\n",
        "\n",
        "$$P(G|P) = \\frac{0.20}{0.50} = 0.40$$\n",
        "\n",
        "So if a player eats a power pellet, there's a 40% chance they'll encounter a ghost.\n",
        "\n",
        "<hr />\n",
        "\n",
        "**If a player encounters a ghost and eats a power pellet, what's the probability they'll complete the level? $P(L|G,P)$?**\n",
        "\n",
        "This conditional probability is:\n",
        "\n",
        "$$P(L|G,P) = \\frac{P(L,G,P)}{P(G,P)}$$\n",
        "\n",
        "$$P(L,G,P) = 0.15$$\n",
        "$$P(G,P) = 0.20$$\n",
        "\n",
        "Therefore:\n",
        "\n",
        "$$P(L|G,P) = \\frac{0.15}{0.20} = 0.75$$\n",
        "\n",
        "So if a player both encounters a ghost and eats a power pellet, there's a 75% chance they'll complete the level.\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## Medical diagnosis\n",
        "\n",
        "Consider a diagnostic model for diabetes with the following variables and conditional probability tables (CPTs):\n",
        "\n",
        "- **Age ($A$)**: Patient's age group\n",
        "  - Young ($y$): 20% (under 40 years)\n",
        "  - Middle-aged ($m$): 40% (40-60 years)\n",
        "  - Older ($o$): 40% (over 60 years)\n",
        "- **Diabetes Type ($T$)**: Classification of diabetes\n",
        "  - Type 1 ($t1$): 30% (autoimmune form, typically insulin-dependent)\n",
        "  - Type 2 ($t2$): 70% (insulin resistance form, often lifestyle-related)\n",
        "- **HbA1c Level ($B$)**: Glycated hemoglobin blood test result\n",
        "  - Low ($l$): Within normal or slightly elevated range (below 7.0%)\n",
        "  - High ($h$): Significantly elevated (7.0% or higher)\n",
        "  - Probabilities depend on Age and Diabetes Type as follows:\n",
        "    - Young patient with Type 1 diabetes: $P(B=h|A=y,T=t1) = 0.1$\n",
        "    - Young patient with Type 2 diabetes: $P(B=h|A=y,T=t2) = 0.3$\n",
        "    - Middle-aged patient with Type 1 diabetes: $P(B=h|A=m,T=t1) = 0.6$\n",
        "    - Middle-aged patient with Type 2 diabetes: $P(B=h|A=m,T=t2) = 0.5$\n",
        "    - Older patient with Type 1 diabetes: $P(B=h|A=o,T=t1) = 0.7$\n",
        "    - Older patient with Type 2 diabetes: $P(B=h|A=o,T=t2) = 0.9$\n",
        "- **Insulin Treatment ($R$)**: Whether insulin therapy is required\n",
        "  - Required (True): Probability depends on HbA1c level\n",
        "    - With high HbA1c: $P(R=True|B=h) = 0.7$\n",
        "    - With low HbA1c: $P(R=True|B=l) = 0.1$\n",
        "\n",
        "This Bayesian network is structured such that Age and Diabetes Type are parent nodes for HbA1c Level, and HbA1c Level is the parent node for Insulin Treatment.\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "flowchart TD\n",
        "    A[\"Age (A)\"] --> B[\"HbA1c Level (B)\"]\n",
        "    T[\"Diabetes Type (T)\"] --> B\n",
        "    B --> R[\"Insulin Treatment (R)\"]\n",
        "```\n",
        "\n",
        "\n",
        "[**Tasks**]{.h4}\n",
        "\n",
        "1. Calculate the marginal probability $P(B=h)$, i.e., the probability that a patient has a high HbA1c level.\n",
        "2. Calculate the joint probabilitySo $P(A=o,T=t1,B=h,R=True)$, i.e., the probability of an older patient with Type 1 diabetes who has a high HbA1c level and requires insulin treatment.\n",
        "3. Calculate the joint probability $P(A=o,B=h,R=True)$, i.e., the probability of an older patient with a high HbA1c level who requires insulin treatment.\n",
        "4. Calculate the conditional probability $P(R=True|T=t1)$, i.e., the probability that a patient with Type 1 diabetes requires insulin treatment.\n",
        "\n",
        "::: {.callout-tip collapse=\"true\"}\n",
        "#### Hint for Task 1\n",
        "\n",
        "Remember to marginalize over all possible combinations of parent variables (Age and Diabetes Type). Start by writing out the full marginalization equation, then convert joint probabilities to conditional probabilities using the chain rule.\n",
        ":::\n",
        "\n",
        "::: {.callout-tip collapse=\"true\"}\n",
        "#### Hint for Task 4\n",
        "\n",
        "This requires application of the total probability theorem. Since $R$ and $T$ are conditionally independent given $B$, you'll need to compute:\n",
        "\n",
        "$$\n",
        "\\begin{flalign}\n",
        "P(R=True|T=t1) &= P(R=True|B=l) \\cdot P(B=l|T=t1) \\\\\n",
        "& + P(R=True|B=h) \\cdot P(B=h|T=t1)\n",
        "\\end{flalign}\n",
        "$$\n",
        "\n",
        "\n",
        "But $P(B=l|T=t1)$ and $P(B=h|T=t1)$ are not directly available in the CPTs and require marginalization over Age.\n",
        ":::\n",
        "\n",
        "\n",
        "::: {.callout-tip collapse=\"true\"}\n",
        "#### Solution notes\n",
        "\n",
        "[**Task 1: Calculate $P(B=h)$**]{.h4}\n",
        "\n",
        "The parents of variable *HbA1c Level (B)* are *Age (A)* and *Diabetes Type (T)*. Therefore we must marginalize over the values of these two parent variables:\n",
        "\n",
        "$$\\begin{align}\n",
        "P(B=h) &= P(B=h,T=t1,A=y) + P(B=h,T=t2,A=y) \\\\\n",
        "       &+ P(B=h,T=t1,A=m) + P(B=h,T=t2,A=m) \\\\\n",
        "       &+ P(B=h,T=t1,A=o) + P(B=h,T=t2,A=o)\n",
        "\\end{align}$$\n",
        "\n",
        "To simplify notation, we'll omit the variable names and keep only the values:\n",
        "\n",
        "$$\\begin{align}\n",
        "P(h) &= P(h,t1,y) + P(h,t2,y) \\\\\n",
        "     &+ P(h,t1,m) + P(h,t2,m) \\\\\n",
        "     &+ P(h,t1,o) + P(h,t2,o)\n",
        "\\end{align}$$\n",
        "\n",
        "Converting joint probabilities to conditional probabilities:\n",
        "\n",
        "$$\\begin{align}\n",
        "P(h) &= P(h \\mid t1,y) \\cdot P(t1,y) + P(h \\mid t2,y) \\cdot P(t2,y) \\\\\n",
        "     &+ P(h \\mid t1,m) \\cdot P(t1,m) + P(h \\mid t2,m) \\cdot P(t2,m) \\\\\n",
        "     &+ P(h \\mid t1,o) \\cdot P(t1,o) + P(h \\mid t2,o) \\cdot P(t2,o)\n",
        "\\end{align}$$\n",
        "\n",
        "Since Age and Diabetes Type are independent:\n",
        "\n",
        "$$\\begin{align}\n",
        "P(h) &= P(h \\mid t1,y) \\cdot P(t1) \\cdot P(y) + P(h \\mid t2,y) \\cdot P(t2) \\cdot P(y) \\\\\n",
        "     &+ P(h \\mid t1,m) \\cdot P(t1) \\cdot P(m) + P(h \\mid t2,m) \\cdot P(t2) \\cdot P(m) \\\\\n",
        "     &+ P(h \\mid t1,o) \\cdot P(t1) \\cdot P(o) + P(h \\mid t2,o) \\cdot P(t2) \\cdot P(o)\n",
        "\\end{align}$$\n",
        "\n",
        "Substituting the values from the CPTs:\n",
        "\n",
        "$$\\begin{align}\n",
        "P(h) &= 0.1 \\cdot 0.3 \\cdot 0.2 + 0.3 \\cdot 0.7 \\cdot 0.2 \\\\\n",
        "     &+ 0.6 \\cdot 0.3 \\cdot 0.4 + 0.5 \\cdot 0.7 \\cdot 0.4 \\\\\n",
        "     &+ 0.7 \\cdot 0.3 \\cdot 0.4 + 0.9 \\cdot 0.7 \\cdot 0.4 \\\\\n",
        "     &= 0.006 + 0.042 + 0.072 + 0.14 + 0.084 + 0.252 \\\\\n",
        "     &= 0.596\n",
        "\\end{align}$$\n",
        "\n",
        "Therefore, the probability of a patient having a high HbA1c level is 59.6%.\n",
        "\n",
        "[**Task 2: Calculate $P(A=o,T=t1,B=h,R=True)$**]{.h4}\n",
        "\n",
        "According to the chain rule of probability applied to the Bayesian network structure (as introduced in the section [calculating joint probabilities](#calculating-joint-probabilities)), we can calculate this joint probability as:\n",
        "\n",
        "$$\\begin{align}\n",
        "P(o,t1,h,r) &= P(r \\mid h) \\cdot P(h \\mid o,t1) \\cdot P(o) \\cdot P(t1) \\\\\n",
        "&= 0.7 \\cdot 0.7 \\cdot 0.4 \\cdot 0.3 \\\\\n",
        "&= 0.0588\n",
        "\\end{align}$$\n",
        "\n",
        "Therefore, the probability of an older patient with Type 1 diabetes who has a high HbA1c level and requires insulin treatment is 5.88%.\n",
        "\n",
        "[**Task 3: Calculate $P(A=o,B=h,R=True)$**]{.h4}\n",
        "\n",
        "In this case, we don't know the value of Diabetes Type, so we need to marginalize over it:\n",
        "\n",
        "$$\\begin{align}\n",
        "P(o,h,r) &= P(r \\mid h) \\cdot \\left[ P(h \\mid o,t1) \\cdot P(o) \\cdot P(t1) + P(h \\mid o,t2) \\cdot P(o) \\cdot P(t2)\\right] \\\\\n",
        "&= 0.7 \\cdot \\left[ 0.7 \\cdot 0.4 \\cdot 0.3 + 0.9 \\cdot 0.4 \\cdot 0.7 \\right] \\\\\n",
        "&= 0.7 \\cdot [0.084 + 0.252] \\\\\n",
        "&= 0.7 \\cdot 0.336 \\\\\n",
        "&= 0.2352\n",
        "\\end{align}$$\n",
        "\n",
        "Therefore, the probability of an older patient with a high HbA1c level who requires insulin treatment is 23.52%.\n",
        "\n",
        "[**Task 4: Calculate $P(R=True \\mid T=t1)$**]{.h4}\n",
        "\n",
        "Looking at the Bayesian network structure, we can determine that $R$ and $T$ are conditionally independent given $B$, as explained in the conditional independence section of the lecture. Therefore, we can apply the law of total probability (which follows from the marginalization process we've learned):\n",
        "\n",
        "$$P(r \\mid t1) = P(r\\mid l) \\cdot P(l \\mid t1) + P(r \\mid h) \\cdot P(h \\mid t1)$$\n",
        "\n",
        "We know $P(r\\mid l) = 0.1$ and $P(r \\mid h) = 0.7$ from the CPTs, but we need to calculate $P(l \\mid t1)$ and $P(h \\mid t1)$.\n",
        "\n",
        "Since $B$ depends on both $T$ and $A$, we need to marginalize over $A$:\n",
        "\n",
        "$$\\begin{align}\n",
        "P(h \\mid t1) &= P(h\\mid t1,y) \\cdot P(y) + P(h\\mid t1,m) \\cdot P(m) + P(h\\mid t1,o) \\cdot P(o) \\\\\n",
        "&= 0.1 \\cdot 0.2 + 0.6 \\cdot 0.4 + 0.7 \\cdot 0.4 \\\\\n",
        "&= 0.02 + 0.24 + 0.28 \\\\\n",
        "&= 0.54\n",
        "\\end{align}$$\n",
        "\n",
        "Since $P(l \\mid t1) = 1 - P(h \\mid t1) = 1 - 0.54 = 0.46$\n",
        "\n",
        "Now we can calculate $P(r \\mid t1)$:\n",
        "\n",
        "$$\\begin{align}\n",
        "P(r \\mid t1) &= 0.1 \\cdot 0.46 + 0.7 \\cdot 0.54 \\\\\n",
        "&= 0.046 + 0.378 \\\\\n",
        "&= 0.424\n",
        "\\end{align}$$\n",
        "\n",
        "Therefore, the probability that a patient with Type 1 diabetes requires insulin treatment is 42.4%.\n",
        "\n",
        ":::\n",
        "\n",
        "# Literature\n",
        "::: {#refs}\n",
        ":::"
      ],
      "id": "61b1cd65"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/awe/Library/Python/3.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}