---
title: "Probability"
subtitle: "üß† Introduction to AI ‚Äî I2AI_5"
lang: en

format: 
#  html:
#    output-file: index.html
  revealjs:
    output-file: slides.html
    include-after-body: ../assets/footer.html
---

# Top{visibility="hidden" .slide-link-hidden .unlisted}

[Slides](slides.html)

# Introduction {.vertical-center background-color=blue background-image="images/bg.jpeg"}

## Motivation

:::{.incremental}
- The agent's knowledge of the world is incomplete (not enough information) or uncertain (sensors are unreliable)
- The agent must consider every possible explanation for its percepts (no matter how unlikely)
- Leads to a large belief-state full of unlikely possibilities and arbitrarily large contingent plans
- Rules about the world are often incomplete (e.g., are all preconditions for an action known?) or even incorrect
:::

. . .

As an agent has to act in spite of this, __conclusions under uncertainty__ need to be drawn.

:::{.notes}
:::{.callout-note}
#### Qualification problem

In philosophy and AI, the qualification problem is concerned with the impossibility of listing all the preconditions required for a real-world action to have its intended effect. It might be posed as how to deal with the things that prevent me from achieving my intended result.

As we have learned, there is no complete solution within logic. System designers must use good judgment in deciding how much detail to specify in their model and what details to omit as all the conditions for an action that are necessary to achieve the intended effect often can't be known or lead to a large belief-state full of unlikely possibilities ‚Äî this is called the __qualification problem in logic__.

Probability theory allows all exceptions to be grouped together without explicitly naming them.
:::
:::

## Example

__Goal__: Be in Ulm at 8:15 to give a lecture

. . .

There are several __plans__ that achieve the goal:

:::{.incremental}
- $P_1:$ Get up at 6:00, take the bike, arrive at 7:30, take a shower, ...
- $P_2:$ Get up at 6:30, take the car at 7:00, arrive at 7:45, ...
- ...
:::
 
. . .

All these plans are correct, but they imply different __costs__ and different __probabilities__ of actually achieving the goal.

. . .

$P_2$ eventually is the plan of choice as the success rate of P1 is only 80%

## Uncertainty in rules

Take an expert dental diagnosis system as an example.

$Toothache \implies Cavity$

. . .

‚Üí This rule is incorrect ‚Äî better:

. . .

$Toothache \implies Cavity \lor GumProblem \lor Abscess ...$

. . .

‚Üí We don‚Äôt know all the causes[^1]   

. . .

Perhaps a causal rule is better?

. . .

$Cavity \implies Toothache$

. . .

‚Üí Ist still wrong and does not allow to reason from symptoms to causes.

[^1]:See qualification problem

## Learnings 

:::{.incremental}
- We cannot enumerate all possible causes (laziness)
- And even if we could, we do not know how correct the rules are (theoretical ignorance[^2]) 
- And even if we did there will always be uncertainty about the patient (practical ignorance[^3])
:::

. . .

__Without perfect knowledge, logical rules do not help much__

[^2]:Medical science has no complete theory for the domain
[^3]:For instance, the coincidence of having a toothache and a cavity that are unrelated, or the fact that not all tests have been run

## Uncertainty in facts

Let us suppose we wanted to support the localization of a robot with (constant) landmarks. With the availability of landmarks, we can narrow down on the area.

. . .

Problem: sensors can be __imprecise__.

:::{.incremental}
- From the fact that a landmark was perceived, we cannot conclude with certainty that the
robot is at that location
- The same is true when no landmark is perceived
- Only the probability increases or decreases
:::

## Probability theory

:::{.incremental}
- We (and other agents) are convinced by facts and rules only up to a certain degree
- One possibility for expressing the degree of belief is to use __probabilities__
- The agent is 90% (or 0.9) convinced by its sensor information   
  *(in 9 out of 10 cases, the information is correct (the agent believes)*
- Probabilities sum up the ‚Äúuncertainty‚Äù that stems from lack of knowledge.
- Probabilities are not to be confused with vagueness    
  *The predicate tall is vague; the statement, ‚ÄúA man is 1.75‚Äì1.80m tall‚Äù is uncertain.*
:::

# Uncertainty and rational decisions {.vertical-center background-color=blue background-image="images/bg.jpeg"}

## Decision theory

`decision theory = utility theory + probability theory`

:::{.incremental}
- We have a choice of actions (or plans)
- These can lead to different solutions with different probabilities
- The actions have different (subjective) costs
- The results have different (subjective) utilities
:::

. . .

It would be rational to choose the action with the __maximum expected utility (MEU)__[^4].

[^4]:Here "expected" means the "average", or "statistical mean" of the outcome utilities, weighted by the probability of the outcome.

## Unconditional probabilities 

$P(A)$ denotes the unconditional probability or prior probability that $A$ will appear in the absence of any other information, e.g. $P(Cavity) = 0.1$

:::{.incremental}
- Prior probabilities can be obtained from statistical analysis or general rules
- A __random variable__[^5] can take on some range---the set of possible values (e.g., Numbers, Boolean, arbitary tokens), e.g. $P(Weather=sunny=0.6)$
- Logical connectors can be used to build __propositions__, e.g. $P(Cavity \land \neg Insured) = 0.06$
- Propositions can contain equations over random variables, e.g. $P(NoonTemp=x) = Uniform(x;18C;26C)$[^6], usually called a __probability density function__
:::

[^5]:Variables in probability theory are called random variables
[^6]:Represents the belief that the temperature at noon is distributed uniformly between 18 and 26 degrees Celcius

## Conditional probabilities 

New information can change the probability, e.g. the probability of a cavity increases if we know the patient has a toothache.

:::{.incremental}
- $P(A|B)$ is the __conditional probability__ of $A$ given that all we know is $B$,   
  e.g., $P(Cavity | Toothache) = 0.8$
- $P(X|Y)$ gives the values of $P(X=x_i|Y=y_j)$ for each possible $i,j$ pair
- Conditional propabilities are defined in terms of unconditional probalities:   
   - for any propositions $a$ and $b$ (if $P(b)>0$), we have $P(a|b)=\frac{P(a \land b)}{P(b)}$,
   - or in a different form as __product rule__ $P(a \land b) = P(a|b)P(b)$,
   - which equals $P(a \land b) = P(b|a)P(a)$
:::

## Example

The __product rules__ for all possible values of $Weather$ and $Cavity$ can be written as a single equation:

|     $P(Weather,Cavity)=P(Weather|Cavity)*P(Cavity)$

. . .

which corresponds to a system of equations (using abbreviations $W$ and $C$):

. . .

|     $P(W=sun,C=true)   =  P(W=sun|C=true)*P(C=true)$

. . .

|     $P(W=rain,C=true)  =  P(W=rain|C=true)*P(C=true)$

. . .

|     ...

. . .

|     $P(W=snow,C=false) =  P(W=snow|C=false)*P(C=false)$

# Axiomatic probability theory {.vertical-center background-color=blue background-image="images/bg.jpeg"}

## Probability axioms

A function $P$ of formulae from propositional logic in the set $[0,1]$ is a probability measure if for all propositions A, B:

:::{.incremental}
- $0 \leq P(A) \leq 1$
- $P(true) = 1$
- $P(false) = 0$
- $P(A \lor B) = P(A) + P(B) - P(A \land B)$
:::

. . . 

All other properties can be derived from these axioms, e.g.

|     $P(\neg A) = 1‚ÄìP(A)$

. . .

follows from $P(A \lor \neg A) = 1$ and $P(A \land \neg A) = 0$

# ‚úèÔ∏è Exercises {.vertical-center background-color=blue background-image="images/bg.jpeg"}

## I2AI_6 E1 {.smaller}

# Literature
::: {#refs}
:::