---
title: "Search"
subtitle: "üß† Introduction to AI ‚Äî I2AI_3"

format: 
  html:
    output-file: index.html
  letterbox-revealjs:
    output-file: slides.html 
    include-before-body: ../assets/footer.html
---

# Top{visibility="hidden" .slide-link-hidden .unlisted .unnumbered}

{{< include ../assets/_top.qmd >}}

# Problem-solving {.vertical-center background-color=blue background-image="images/bg.jpeg"}

## Applications

::: {.r-stack .html-hidden}

![](images/routing.jpeg){.fragment height="550"}

![](images/vacuum.jpeg){.fragment height="400"}

![](images/robot.jpeg){.fragment height="300"}

![](images/puzzle.jpeg){.fragment height="350"}

:::

::: {.notes}
Examples for search problems are [@RusselNorvig2022AIMA, p. 87-88]:

- Route-finding problems (e.g., car navigation, airline travel-planning)
- Touring problems (e.g., the traveling salesperson problem)
- VLSI layout problems (positioning millions of components and connections on a chip)
- Robot navigation (e.g., vacuum robots)
- Automatic assembly sequencing of complex objects (e.g., protein design)
:::

## Agents

Agents that plan ahead by considering a sequence of actions that form a path to a goal state are called __problem-solving agents__ [@RusselNorvig2022AIMA, p.81]

::: {.incremental}
- The computational process it undertakes is __search__
- The representations the agents use are __atomic__ representations
- There are search algorithms for several environments
:::

::: {.fragment}
Here only __simple environments__ are considered *(episodic, single agent, fully observable, deterministic, static, discrete, and known)*.  

--> We assume that information about the environment are given (e.g., a map)
:::

:::{.notes}
There are also search algorithms for problems in *partially observable, nondeterministic, unknown, and continuous environments* (i.e., complex environments) like local search methods (e.g., hill-climbing search, local beam search, evolutionary algorithms). For details please see @RusselNorvig2022AIMA.
:::

## Process

In simple environments, agents can follow a four-phase-problem-solving process [@RusselNorvig2022AIMA, p.81-82]:

::: {.incremental}
- __Goal formulation:__ goals organize behavior by limiting the objectives and hence the actions to be considered
- __Problem formulation:__ the agents devices a description of the states and actions necessary to reach the goal‚Äîan abstract model of the relevant part of the environment
- __Search:__ the agent simulates sequences of actions in its model, searching until it finds a sequence that reaches the goal (i.e., the solution)
- __Execution:__ the agent executes the actions in the solution, one at a time 
:::

:::{.notes}
:::{.callout-note}
### Problem formulation must follow goal formulation
In goal formulation, we decide which aspects of the world we are interested in, and which can be ignored or abstracted away. Then in problem formulation we decide how to manipulate the important aspects (and ignore the others). If we did problem formulation first we would not know what to include and what to leave out.

It can happen that there is a cycle of iterations between goal formulation, problem formulation, and problem-solving until one arrives at a sufficiently useful and efficient solution.
:::
:::

# Search problem {.vertical-center background-color=blue background-image="images/bg.jpeg"} 

## Definition

A search problem can be defined formally as [@RusselNorvig2022AIMA, p.83]:

::: {.incremental}
- The __state space:__ a set of possible states[^1] the environment can be in[^2]
- The __initial state:__ the state that the agent starts
- The __goal state(s):__ a state that the agent is trying to reach[^3]
- A __transition model:__ returns the state that results from doing action $a$ in state $s$ ($RESULT(s,a)$)
- The __sucessor function:__ returns a set of (action, state) pairs for node $s$, where the state is the state reachable by taking the action $a$ ($ACTIONS(s)$)
- An __action cost function:__ gives the numeric cost of applying action $a$ in state $s$ to reach state $s'$ ($ACTION-COST(s,a,s'$)
- A __path:__ a sequence of actions
- A __solution:__ a path from the initial state to the goal state
:::

[^1]: A state is a situation that an agent can find itself in.
[^2]: Expressed by a graph whose nodes are the set of all states, and whose links are actions that transform one state into another
[^3]: Can be a single goal state, a small set of alternative goal states, or a property that applies to many states (e.g, no dirt in any location)

## Modelling

![Example: A simplified map of Romania, with road distances in miles; based on @RusselNorvig2022AIMA [p.82]](images/search-problem.svg){#fig-search-problem}

::: {.notes}
@fig-search-problem depicts the search problem as model, the state space graph [@RusselNorvig2022AIMA, p.82-84]:

- State space: cities (vertices, each state occurs only once)
- Initial state: Arad
- Goal state: Bucharest (goal test: $Is state == Bucharest?$)
- Actions: directed edges between the vertices (paths)
- Action costs: numbers on the paths

::: {.callout-note}
### Abstraction
The model is an abstract mathematical description, here a simple atomic state description. The model is an __abstraction__ as it ignores many details of the reality (e.g., weather and scenery).

A good problem formulation has the right level of detail (i.e., an appropriate __level of abstraction__).

The choice of a good abstraction involves removing as much detail as possible while retaining validity and ensuring that the abstract actions are easy to carry out. An abstraction is *valid* if any abstract solution can be elaborated into a solution in the more detailed world.
:::
:::

# Search tree {.vertical-center background-color=blue background-image="images/bg.jpeg"}

## Structure

![Example: A partial search tree for finding a route from Arad to Bucharest; based on @RusselNorvig2022AIMA [p.89]](images/search-tree.svg){#fig-search-tree}

::: {.notes}
@fig-search-tree visualizes a partial search tree: the first few steps in finding a path from Arad (initial state) to Bucharest (goal). 

- The __root node__ of the search tree is the initial state $s$ *(Arad)*
- The available actions considerd using $ACTIONS(s)$
- New nodes, called __child nodes__ or __successor nodes__ are generated using $RESULT(s,a)$ (i.e., the root node is __expanded__)
- Each child node has *Arad* as its __parent node__
- The child node to be considered next is selected by the minimum value of some __evaluation function__ $f(n)$ (__strategies__ see below)

 In @fig-search-tree, nodes that have been expanded are white with bold letters; nodes on the __frontier__ that have been generated but not yet expanded are in white and regular letters; the set of states corresponding to these two types of nodes are said to have been __reached__. Nodes that could be generated next are shown in faint dashed lines.

::: {.callout-note}

### Definition of search trees
A search tree is a ‚Äúwhat if‚Äù tree of plans and their outcomes.

- The start state is the root node,
- children correspond to successors, 
- nodes show states, but correspond to PLANS that achieve those states

There are lots of repeated structure in the search tree. Thus, for most problems, the whole tree can never be actually built. In practice, both state space graphs and search trees are constructed on demand and as little as possible [@RusselNorvig2022AIMA].
:::

Search algorithms require a data structure to keep track of the __search tree__. A node in the tree is represented by a data structure with four components [@RusselNorvig2022AIMA, p. 91]:

- $node.STATE$: the state to which the node corresponds
- $node.PARENT$: the node in the tree that generated this node
- $node.ACTION$: the action that was applied to the parent's state to generate this node
- $node.PATH-COST$: the total cost of the path from the initial state to generate this node

Following the PARENT pointers back from a node allows to revocer the states and actions along the path to that node. Doing this from a goal node generates the solution.

:::{.small}
Example data structure in pseudo code:

```{.javascript}
var searchTree = {
  node.STATE: "Bucharest"
  node.PARENT: {
    node.STATE: "Pitesti"
    node.PARENT: {
      node.STATE: "Rimmicu Vilcea"
      node.PARENT: {
        node.STATE: "Sibiu"
        node.PARENT: {
          node.STATE: "Arad"
          node.PARENT: NULL
          node.ACTION: NULL
          node.PATH-COST: 0
        }
        node.ACTION: "Arad to Sibiu"
        node.PATH-COST: 140
      }
      node.ACTION: "Sibiu to Rimmicu Vilcea"
      node.PATH-COST: 220
    }
    node.ACTION: "Rimmicu Vilcea to Pitesti"
    node.PATH-COST: 317
  }
  node.ACTION: "Pitesti to Bucharest"
  node.PATH-COST: 418
}
```

:::

In addition, a data structure to store the __fontier__ is neded. Usually this is realized using a __queue__ with following functions [@RusselNorvig2022AIMA, p. 91]:

- $IS-EMPTY(frontier)$: returns true only if there are no nodes in the frontier
- $POP(frontier)$: removes the top node from the frontier and returns it
- $TOP(frontier)$: returns (but does not remove) the top node from the frontier
- $ADD(node,frontier)$: inserts node into its proper place in the queue


:::


# Search {.vertical-center background-color=blue background-image="images/bg.jpeg"}

## Algorithms

__Search algorithms__ take a search problem as input and return a solution, or an indication of failure [@RusselNorvig2022AIMA, p.89].

::: {.incremental}
- They superimpose a search tree over the state-space graph,
- form various paths from the initial state, and 
- try to find a path that reaches a goal state 
:::

. . .

They can implement

::: {.incremental}
- __uninformed search__ methods, which only have access to the problem definition but not clue about how close a state is to the goal(s).
- __informed search__ methods, which have access to a heuristic function that gives domain-specific hints about how close a state is to the goal(s) *(e.g., using straight-line distance in route-finding problems)*
:::

## Uninformed search

![Overview of uninformed search strategies](images/strategies.svg){#fig-search-strategies}

:::{.notes}
### Breadth-first search

expands the shallowest nodes first;   
$f(n)$ is the depth of the node

  - complete, optimal for unit action costs
  - exponential space complexity
  - __FIFO queue[^4]__
  - a reached table is necessary to store the nodes already reached

### Depth-first search

expands the deepest unexpanded node first;   
$f(n)$ is the negative of the depth

  - neither complete (in infinite state spaces, it can get stock going down an infinite path) nor optimal
  - linear space complexity, a depth bound can be added
  - __LIFO queue[^5]__
  - the frontier is very small

### Uniform-cost search

expands the node with the lowest path costs;   
$f(n)$ returns the cost of the path from the root to the current node

  - complete, optimal for general action costs
  - __priority queue[^6]__ using cumulative cost  

[^6]:first pops the node with the minimum costs according to $f(n)$
[^4]:first-in-first-out queue first pops the node that was added to the queue first 
[^5]:last-in-first-out queue (also known as a stack) pops first the most recently added node

### Illustration of strategies

:::{.panel-tabset}

### Breadth-first

#### General Process

1. Add the start node to the frontier using $ADD(node,frontier)$
2. Get the top node from the frontier using $POP(frontier)$
3. Get all the actions avaible for the top node via $ACTIONS(s)$
4. Get the child nodes for the retreived actions via $RESULT(s,a)$
5. Check if one child nodes is a goal state; if yes return the search tree; if not go ahead
6. Add the child nodes to the frontier $ADD(node,frontier)$
7. Repeat steps 2 to 6 as long as no solution is found and $IS-EMPTY(frontier)$ is not true 

#### Example

Example using the tree depicted in @fig-search-tree, assuming that the goal is not included:

:::{.small}

- Add *0* to the frontier
- Get the top node from the frontier (*0*)
- Get the actions avaiable from *0* (*to 1* and *to 2*)
- Get the child nodes for the retrieved actions (*1* and *2*)
- Check if *1* is a goal state (*false*)
- Add *1* to the frontier
- Check if *2* is a goald state (*false*)
- Add *2* to the frontier
- Get the top node from the frontier, here using the FIFO (*1*)
- Get the actions avaiable from *1* (*to 3* and *to 4*)
- Get the child nodes for the retrieved actions (*3* and *4*)
- Check if *3* is a goal state (*false*)
- Add *3* to the frontier
- Check if *4* is a goal state (*false*)
- Add *4* to the frontier
- Get the top node from the frontier (*2*)
- Get the actions avaiable from *2* (*to 5* and *to 6*)
- Get the child nodes for the retrieved actions (*5* and *6*)
- Check if *5* is a goal state (*false*)
- Add *5* to the frontier
- Check if *6* is a goal state (*false*)
- Add *6* to the frontier
- Get the top node from the frontier (*3*)
- ...

:::

### Depth-first

#### General Process

The overall process is the same, only the frontier queue returns the node that was added last.

### Uniform-cost

#### General Process

The overall process is the same, only the frontier queue returns the node in the queue with the minimum : cost-paths.

#### Example

Example using the tree depicted in @fig-search-problem:

:::{.small}

- Add *Arad* to the frontier
- Get the top node from the frontier (*Arad*)
- Get the actions avaiable from *Arad* (*to Zerind*, *to Timisoara*, *to Sibiu*)
- Get the child nodes for the retrieved actions (*Zerind*, *Timisoara*, *Sibiu*)
- Add *Zerind*, *Timisoara*, and *Sibiu* to the frontier
- Get the node from the frontier with the lowest path cost (*Zerind*)
- Check if *Zerind* is a goal state (*false*)
- Get the actions avaiable from *Zerind* (*to Oradea*)
- Get the child nodes for the retrieved actions (*Oradea*)
- Add *Oradea* to the frontier
- Get the node from the frontier with the lowest path cost (*Timisoara*)
- Check if *Timisoara* is a goal state (*false*)
- Get the actions avaiable from *Timisoara* (*to Lugoj*)
- Get the child nodes for the retrieved actions (*Lugoj*)
- Add *Lugoj* the frontier
- Get the node from the frontier with the lowest path cost (*Sibiu*)
- Check if *Sibiu* is a goal state (*false*)
- Get the actions avaiable from *Sibiu* (*to Fagaras*, *to Rimmicu Vilcea*)
- Get the child nodes for the retrieved actions (*Fagaras*, *Rimmicu Vilcea*)
- Add *Fagaras* and *Rimmicu Vilcea* to the frontier
- Get the node from the frontier with the lowest path cost (*Rimmicu Vilcea*)
- Check if *Rimmicu Vilcea* is a goal state (*false*)
- Get the actions avaiable from *Rimmicu Vilcea* (*to Pitesti*, *to Craiova*)
- Get the child nodes for the retrieved actions (*Pitesti*, *Craiova*)
- Add *Pitesti* and *Craiova* to the frontier
- Get the node from the frontier with the lowest path cost (*Lugoj*)
- Check if *Lugoj* is a goal state (*false*)
- Get the actions avaiable from *Lugoj* (*to Mehadia*)
- Get the child nodes for the retrieved actions (*Mehadia*)
- Add *Mehadia* to the frontier
- Get the node from the frontier with the lowest path cost (*Fagaras*)
- Check if *Fagaras* is a goal state (*false*)
- Get the actions avaiable from *Fagaras* (*to Bucharest*)
- Get the child nodes for the retrieved actions (*Bucharest*)
- Add *Bucharest* to the frontier
- Get the node from the frontier with the lowest path cost (*Mehadia*)
- Check if *Mehadia* is a goal state (*false*)
- Get the actions avaiable from *Mehadia* (*to Drobeta*)
- Get the child nodes for the retrieved actions (*Drobeta*)
- Add *Drobeta* to the frontier
- Get the node from the frontier with the lowest path cost (*Pitesti*)
- Check if *Pitesti* is a goal state (*false*)
- Get the actions avaiable from *Pitesti* (*to Bucharest*, *to Craiova*)
- Get the child nodes for the retrieved actions (*Bucharest*, *Craiova*)
- Add *Bucharest* and *Craiova* to the frontier
- Get the node from the frontier with the lowest path cost (*Drobeta*)
- Check if *Drobeta* is a goal state (*false*)
- Get the actions avaiable from *Drobeta* (*to Craiova*)
- Get the child nodes for the retrieved actions (*Craiova*)
- Add *Craiova* to the frontier
- Get the node from the frontier with the lowest path cost (*Bucharest*)
- Check if *Drobeta* is a goal state (*true*)
- Return solution

Note if we had checked for a goald upon generating a node rather than when expanding the lowest-cost node, then we would have returned a higher-cost path (the one through Fagaras)

:::
:::

:::{.callout-note}
### Measuring problem-solving performance
The performance of problem-solving algorithms can be evaluated in four ways [@RusselNorvig2022AIMA, p.93]:

- __Completeness:__ Is the algorithm guaranteed to find a solution when there is one, and to correctly report failure when there is not?
- __Cost optimality:__ Does it find a solution with the lowest : cost-path of all solutions?
- __Time complexity:__ How long does it take to find a solution (e.g., measured in seconds or by the number of states and actions considered)?
- __Space complexity:__ How much memory is needed to perform the search?
:::
:::

## Informed search {visibility="hidden"}

Informed (heuristic) search strategies use domain-specific hints about the location of goals [@RusselNorvig2022AIMA, p. 102].

These strategies can find solutions __more efficiently__ than uninformed strategies.

The hints are generated by a __heuristic function__ $h(n)$

### Greedy best-first search [@RusselNorvig2022AIMA, p. 103ff]

The Greedy best-first search uses the evlation function $f(n) = h(n)$ 

It expands the first node with the lowest $h(n)$ value in the queue. 

In route finding problems, the straight-line distance heuristic is used as $h(n)$.

Greedy best-first graph search is complete in finite state spaces, but not in infinite ones.

### A* Search [@RusselNorvig2022AIMA, p. 103ff]

The A* search (pronounced "A-star search") uses the evaluation function $f(n) = g(n) + h(n)$

- $g(n)$ is the path cost from the initial state to node $n$ (e.g., computed by using the action-cost function)
- $h(n)$ is the *estimated cost* of the shortes path from $n$ to a goal state

A* search is complete, if it is cost-optimal depends on the heuristic

# Unknown environments {.vertical-center background-color=blue background-image="images/bg.jpeg"}

## Online search

The agents considered so far use __offline search__ algorithm. They compute a complete solution before taking their first action.

:::{.fragment}
__Online search__ agents interleaves computation and action:

:::{.incremental}
- Takes action,
- observes the environment, and
- computes the next action
:::

:::

:::{.fragment}
These agents can discover successor only for a state that is occupied or that is learned (i.e., contained in a map created online)
:::

:::{.fragment}
Online search is a good idea in dynamic or semi-dynamic environments.
:::

# Summary {.vertical-center background-color=blue background-image="images/bg.jpeg"}

Search operates over models of the world (which might be observed *online*)

:::{.incremental}
- Agents do not try all possible plans
- Planning is all "in simulation"
- Search is only as good as the models are
:::

# ‚úèÔ∏è Exercises {.vertical-center background-color=blue background-image="images/bg.jpeg"}

## I2AI_3 E1

Explain why problem formulation must follow goal formulation.

## I2AI_3 E2

Give a complete problem formulation for each of the following problems. Choose a formulation that is precise enough to be implemented.

- There are six glass boxes in a row, each with a lock. Each of the first five boxes holds a key unlocking the next box in line; the last box holds a banana. You have the key to the first box, and you want the banana.
- There is an *n x n* grid of squares, each square initially being either unpainted floor or a bottomless pit. You start standing on an unpainted floor square, and can either paint the square under you or move into an adjacent unpainted floor square. You want the whole floor painted.

## I2AI_3 E3

Your goal is to navigate a robot out of a maze. It starts in the center of the maze facing north. You can turn the robot to face north, east, south, or west; direct the robot to move forward a certain distance (it will stop before a wall). 

a. Formulate this problem. How large is the state space?
b. In navigating a maze, the only place we need to turn is at the intersection of two or more corridors. Reformulate this problem using this observation. How large is the state space now?
c. From each point in the maze, we can move in any of the four directions until we reach a turning point, and this is the only action we need to do. Reformulate the problem using these actions. Do we need to keep track of the robot‚Äôs orientation now?
d. In our initial description of the problem we already abstracted from the real world. Name three such simplifications we made.

:::{.notes}

![Solution note for I2AI_3 question b](images/maze.svg){#fig-maze}

:::

# Literature
::: {#refs}
:::